{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import MeCab\n",
    "from jamo import h2j, j2hcj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(object):\n",
    "    \"\"\"\n",
    "    Word2Vec을 이용하여 사전을 만드는 여러가지 방식을 제공\n",
    "    <Parameters>\n",
    "        - word2vec_size : 단어 하나당 vector size\n",
    "        - window : 현재 word를 계산하는데 사용하는 maximum distance\n",
    "        - min_count : 단어가 해당 개수 이하로 나오면 무시\n",
    "        - flag : 5가지 종류 제공.\n",
    "          1) W : mecab 형태소분석 결과 word 단위 word2vec 사전\n",
    "          2) WT : mecab 형태소분석 결과 word 단위 + pos tagging word2vec 사전\n",
    "          3) JM : jamo 단위 word2vec 사전\n",
    "          4) LN : N = 1,2,.. n 글자수 단위 사전 제공\n",
    "    <Function>\n",
    "        - make_input(sentence_list) : 각 flag에 알맞은 input 형태를 만들어서 return\n",
    "        - save_w2v_dic(sentence_list, path) : 각 flag에 알맞은 사전은 원하는 path에 저장\n",
    "    \"\"\"\n",
    "    def __init__(self, word2vec_size=300, window=5, min_count=2, flag='W'):\n",
    "        self.word2vec_size = word2vec_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.flag = flag\n",
    "    \n",
    "\n",
    "    def parse_sentence(self, text, param):\n",
    "        if type(text) == str:\n",
    "            text = ''.join(text.split()) #space remove\n",
    "            words_array = []\n",
    "            \n",
    "            if self.flag[0] == 'W':\n",
    "                mecab = MeCab.Tagger()\n",
    "                \n",
    "                parse_result = mecab.parse(text) # pose parse\n",
    "                info_of_words = parse_result.split('\\n')\n",
    "                \n",
    "                for info in info_of_words:\n",
    "                    if not (info == 'EOS' or info == ''):\n",
    "                        info_elems = info.split(',')\n",
    "                        posed_word = info_elems[0].split('\\t')\n",
    "\n",
    "                        if param and len(posed_word)>1:\n",
    "                            words_array.append( posed_word[0]+'/'+posed_word[1])\n",
    "                        else:\n",
    "                            words_array.append( posed_word[0])\n",
    "            elif self.flag == 'JM':\n",
    "                words_array = list(j2hcj(h2j(text)))\n",
    "            elif self.flag[0] == 'L':\n",
    "                n = param\n",
    "                words_array = [text[i:i+n] for i in range(0, len(text), n)]\n",
    "                \n",
    "            return words_array\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "\n",
    "    def make_input(self, sentence_list):\n",
    "        input_x = []\n",
    "        \n",
    "        for text in sentence_list:\n",
    "            if self.flag[0] == 'W':\n",
    "                if len(self.flag) > 1 and self.flag[1] == 'T':\n",
    "                    param = True\n",
    "                else:\n",
    "                    param = False\n",
    "            elif self.flag[0] == 'L':\n",
    "                if len(self.flag) > 1 :\n",
    "                    param = int(self.flag[1])\n",
    "            else:\n",
    "                param = ''\n",
    "            result = self.parse_sentence(text=text, param=param)\n",
    "            if type(result)==list and len(result) > 0:\n",
    "                input_x.append(result)\n",
    "        return input_x        \n",
    "    \n",
    "\n",
    "    def make_w2v_dic(self, sentence_list):\n",
    "        if type(sentence_list) == list:\n",
    "            input_x = self.make_input(sentence_list)\n",
    "            w2v_input = np.array(input_x)\n",
    "            model = gensim.models.Word2Vec(min_count=self.min_count, window=self.window, size=self.word2vec_size)\n",
    "            model.build_vocab(w2v_input)\n",
    "            model.train(w2v_input, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "            word_vectors = model.wv\n",
    "\n",
    "            return model, word_vectors\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def save_w2v_dic(self, sentence_list, path):\n",
    "\n",
    "        SAVE_PATH = path\n",
    "        SAVE_NAME = SAVE_PATH+datetime.datetime.now().strftime('%Y%m%d%H%M')+'_'+str(self.word2vec_size)+'_'+self.flag\n",
    "        \n",
    "        model, word_vectors = self.make_w2v_dic(sentence_list)\n",
    "        print(\"Dictionary is saved : \"+SAVE_NAME)\n",
    "        model.save(SAVE_NAME+'.bin')\n",
    "    \n",
    "    \n",
    "    def load_w2v_dic (self, dic_path, filetype='bin'):\n",
    "        allFileNames = os.listdir(dic_path)\n",
    "        print(allFileNames)\n",
    "        modelNames = [fn for fn in allFileNames if fn.find(self.flag+'.'+filetype) > -1 and fn.find(str(self.word2vec_size)) > -1 and fn.endswith(filetype)]\n",
    "\n",
    "        if dic_path[-1] != '/':\n",
    "            dic_path += '/'\n",
    "\n",
    "        model = gensim.models.Word2Vec.load(dic_path+modelNames[0])\n",
    "        print(modelNames[0]+' is loaded')\n",
    "        return model\n",
    "    \n",
    "    def find_vectors(sentence_list, max_seq_size, dic_path):\n",
    "        #load model\n",
    "        w2v_dic_model = self.load_w2v_dic(dic_path=dic_path, filetype='bin')\n",
    "        w2v_dic = w2v_dic_model.wv\n",
    "        \n",
    "        #make input list\n",
    "        parse_list = self.make_input(sentence_list)\n",
    "        input_x_vec = []\n",
    "        \n",
    "        for word_array in parse_list:\n",
    "            embedding_vector = []\n",
    "\n",
    "            for w in word_array:\n",
    "                if w not in w2v_dic.vocab:\n",
    "                    embedding_vector.append(np.random.normal(scale=1e-2, size=self.word2vec_size))#[np.zeros(shape=300)]\n",
    "                else:\n",
    "                    embedding_vector.append(word_vectors[w])\n",
    "\n",
    "            temp_len = max_seq_size-len(embedding_vector)\n",
    "            if temp_len > 0:\n",
    "                for i in range(0,temp_len):\n",
    "                    embedding_vec += [np.zeros(word2vector_size)]    \n",
    "            else:\n",
    "                embedding_vec = embedding_vec[:max_seq_size]\n",
    "            input_x_vec.append(embedding_vec)\n",
    "            \n",
    "        return np.array(input_x_vec)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#사용예시\n",
    "\n",
    "sentence_list = []\n",
    "\n",
    "#data 불러오기 - only sentence\n",
    "dataDir = 'data/uplusInput/train'\n",
    "allFileNames = os.listdir(dataDir)\n",
    "\n",
    "filePaths = []\n",
    "\n",
    "for fname in allFileNames:\n",
    "    if fname[0] != '.':\n",
    "        filePaths.append(os.path.join(dataDir, fname))\n",
    "\n",
    "for fpn in filePaths:\n",
    "    if fpn[-4:] == '.csv':\n",
    "        datas = pd.read_csv(fpn)\n",
    "        input_x = datas['content']\n",
    "        sentence_list.extend(list(input_x))\n",
    "\n",
    "targetDir = 'data/movieReview/formmated'\n",
    "allFileNames = os.listdir(targetDir)\n",
    "filesize = len(allFileNames)\n",
    "\n",
    "testSize = int(filesize/10)\n",
    "subNames = allFileNames[testSize+1:testSize*2]\n",
    "# print(len(subNames))\n",
    "\n",
    "for fname in subNames:\n",
    "    if fname[0] != '.':\n",
    "        data_path = os.path.join(targetDir, fname)\n",
    "\n",
    "        try:\n",
    "            corpus = pd.read_csv(data_path, quotechar=\"'\", header=None, encoding=\"utf-8\", )\n",
    "            contents = np.array(corpus[5][:])\n",
    "\n",
    "            sentence_list.extend(list(contents))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(fname + ' : ' + str(e))\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_size = 150\n",
    "window = 5  #현재 word를 계산하는데 사용하는 maximum distance ?\n",
    "min_count = 2 #단어가 해당 개수 이하로 나오면 무시\n",
    "SAVE_PATH = './data/wordDic/'\n",
    "\n",
    "nletterW2V = Word2Vec(word2vec_size=word2vec_size, window=window, min_count=min_count, flag='L2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary is saved : ./data/wordDic/201804241053_150_L2\n"
     ]
    }
   ],
   "source": [
    "nletterW2V.save_w2v_dic(sentence_list,path=SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['201804241053_150_L2.bin.trainables.syn1neg.npy', '201804241053_150_L2.bin', '201804231620_150_WT.bin', '201804231704_150_JM.bin', '201804241016_150_L1.bin', '201804231649_150_W.bin', '201804241053_150_L2.bin.wv.vectors.npy']\n",
      "./data/wordDic/\n",
      "./data/wordDic/201804241053_150_L2.bin\n",
      "201804241053_150_L2.bin is loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7f1902523d30>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nletterW2V.load_w2v_dic(dic_path=SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
