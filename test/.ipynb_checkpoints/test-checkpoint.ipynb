{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow import flags\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# loading function                                 #\n",
    "####################################################\n",
    "def loading_rdata(data_path, eng=True, num=True, punc=False):\n",
    "    # R에서 title과 contents만 csv로 저장한걸 불러와서 제목과 컨텐츠로 분리\n",
    "    # write.csv(corpus, data_path, fileEncoding='utf-8', row.names=F)\n",
    "    try:\n",
    "        corpus = pd.read_csv(data_path, quotechar=\"'\", header=None, encoding=\"utf-8\", )\n",
    "        contents = np.array(corpus[5][:])\n",
    "        points = np.array(corpus[2][:])\n",
    "    \n",
    "    except Exception as e:\n",
    "        contents = []\n",
    "        points = []\n",
    "\n",
    "#     points = []\n",
    "#     for idx,doc in enumerate(corpus):\n",
    "#         if isNumber(doc[0]) is False:\n",
    "#             content = normalize(doc[0], english=eng, number=num, punctuation=punc)\n",
    "#             contents.append(content)\n",
    "#             points.append(doc[1])\n",
    "#         if idx % 100000 is 0:\n",
    "#             print('%d docs / %d save' % (idx, len(contents)))\n",
    "    return contents, points\n",
    "\n",
    "def isNumber(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# cut words function                               #\n",
    "####################################################\n",
    "def cut(contents, cut=2):\n",
    "    results = []\n",
    "    for content in contents:\n",
    "        if type(content) == str:\n",
    "            words = content.split()\n",
    "            result = []\n",
    "            for word in words:\n",
    "                result.append(word[:cut])\n",
    "            results.append(' '.join([token for token in result]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# making input function                            # 사전 만듦\n",
    "####################################################\n",
    "def make_input(documents, max_document_length):\n",
    "    # tensorflow.contrib.learn.preprocessing 내에 VocabularyProcessor라는 클래스를 이용\n",
    "    # 모든 문서에 등장하는 단어들에 인덱스를 할당\n",
    "    # 길이가 다른 문서를 max_document_length로 맞춰주는 역할\n",
    "    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_document_length)  # 객체 선언\n",
    "    x = np.array(list(vocab_processor.fit_transform(documents)))\n",
    "    ### 텐서플로우 vocabulary processor\n",
    "    # Extract word:id mapping from the object.\n",
    "    # word to ix 와 유사\n",
    "    vocab_dict = vocab_processor.vocabulary_._mapping\n",
    "    # Sort the vocabulary dictionary on the basis of values(id).\n",
    "    sorted_vocab = sorted(vocab_dict.items(), key=lambda x: x[1])\n",
    "    # Treat the id's as index into list and create a list of words in the ascending order of id's\n",
    "    # word with id i goes at index i of the list.\n",
    "    vocabulary = list(list(zip(*sorted_vocab))[0])\n",
    "    return x, vocabulary, len(vocab_processor.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# make output function                             #\n",
    "####################################################\n",
    "def make_output(points, threshold_low, threshold_high):\n",
    "    results = np.zeros((len(points),3))\n",
    "    for idx, point in enumerate(points):\n",
    "        if point < threshold_low:\n",
    "            results[idx,0] = 1\n",
    "        elif point > threshold_high:  \n",
    "            results[idx,2] = 1\n",
    "        else:\n",
    "            results[idx,1] = 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# divide train/test set function                   #\n",
    "####################################################\n",
    "def divide(x, y, train_prop):\n",
    "    random.seed(1234)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    tmp = np.random.permutation(np.arange(len(x)))\n",
    "    x_tr = x[tmp][:round(train_prop * len(x))]\n",
    "    y_tr = y[tmp][:round(train_prop * len(x))]\n",
    "    x_te = x[tmp][-(len(x)-round(train_prop * len(x))):]\n",
    "    y_te = y[tmp][-(len(x)-round(train_prop * len(x))):]\n",
    "    return x_tr, x_te, y_tr, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# check maxlength function                         #\n",
    "####################################################\n",
    "def check_maxlength(contents):\n",
    "    max_document_length = 0\n",
    "    for document in contents:\n",
    "        document_length = len(document.split())\n",
    "        if document_length > max_document_length:\n",
    "            max_document_length = document_length\n",
    "    return max_document_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " tf.Graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    <Parameters>\n",
    "        - sequence_length: 최대 문장 길이\n",
    "        - num_classes: 클래스 개수\n",
    "        - vocab_size: 등장 단어 수\n",
    "        - embedding_size: 각 단어에 해당되는 임베디드 벡터의 차원\n",
    "        - filter_sizes: convolutional filter들의 사이즈 (= 각 filter가 몇 개의 단어를 볼 것인가?) (예: \"3, 4, 5\")\n",
    "        - num_filters: 각 filter size 별 filter 수\n",
    "        - l2_reg_lambda: 각 weights, biases에 대한 l2 regularization 정도\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, sequence_length, num_classes, vocab_size,\n",
    "            embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        \"\"\"\n",
    "        <Variable>\n",
    "            - W: 각 단어의 임베디드 벡터의 성분을 랜덤하게 할당\n",
    "        \"\"\"\n",
    "#         with tf.device('/gpu:0'), tf.name_scope(\"embedding\"):\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(axis=3, values=pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate Mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses =tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.scores)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.append?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_contents[0]) == str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/movieReview/formmated/75926-드라마.csv\n",
      "data/movieReview/formmated/91512-애니메이션.csv\n",
      "data/movieReview/formmated/97531-스릴러-범죄.csv\n",
      "data/movieReview/formmated/12008-미스터리-공포-스릴러-드라마.csv\n",
      "data/movieReview/formmated/131388-액션-SF-스릴러.csv\n",
      "data/movieReview/formmated/17657.csv\n",
      "data/movieReview/formmated/110652-SF-액션-판타지-멜로-로맨스.csv\n",
      "data/movieReview/formmated/32562-액션-공포-스릴러.csv\n",
      "data/movieReview/formmated/19337-멜로-로맨스-코미디.csv\n",
      "data/movieReview/formmated/99717-다큐멘터리.csv\n",
      "data/movieReview/formmated/115246-범죄-드라마.csv\n",
      "data/movieReview/formmated/53097-액션-드라마.csv\n",
      "data/movieReview/formmated/19315.csv\n",
      "data/movieReview/formmated/57675-드라마-멜로-로맨스-코미디.csv\n",
      "data/movieReview/formmated/115245-미스터리-멜로-로맨스.csv\n",
      "data/movieReview/formmated/149445-공포-스릴러.csv\n",
      "data/movieReview/formmated/19342.csv\n",
      "data/movieReview/formmated/23243.csv\n",
      "data/movieReview/formmated/29261-드라마.csv\n",
      "data/movieReview/formmated/154112-범죄-액션-드라마.csv\n",
      "data/movieReview/formmated/25466-SF.csv\n",
      "data/movieReview/formmated/19755.csv\n",
      "data/movieReview/formmated/118114-애니메이션-판타지-모험.csv\n",
      "data/movieReview/formmated/70318-코미디.csv\n",
      "data/movieReview/formmated/78199-드라마.csv\n",
      "data/movieReview/formmated/16885-코미디-드라마.csv\n",
      "data/movieReview/formmated/79950-드라마.csv\n",
      "data/movieReview/formmated/17537-액션-모험-코미디.csv\n",
      "data/movieReview/formmated/102861.csv\n",
      "data/movieReview/formmated/90885-멜로-로맨스-코미디.csv\n",
      "data/movieReview/formmated/12810-드라마-액션.csv\n",
      "data/movieReview/formmated/71151-드라마-멜로-로맨스-가족.csv\n",
      "data/movieReview/formmated/23653-드라마.csv\n",
      "data/movieReview/formmated/144709-다큐멘터리.csv\n",
      "data/movieReview/formmated/22779-드라마-멜로-로맨스.csv\n",
      "data/movieReview/formmated/50724-드라마.csv\n",
      "data/movieReview/formmated/92075-멜로-로맨스-코미디.csv\n",
      "data/movieReview/formmated/112427-스릴러.csv\n",
      "data/movieReview/formmated/63105-드라마-멜로-로맨스.csv\n",
      "data/movieReview/formmated/12747-코미디-멜로-로맨스-드라마.csv\n",
      "data/movieReview/formmated/142816.csv\n",
      "data/movieReview/formmated/20711.csv\n",
      "data/movieReview/formmated/138094-액션-범죄-드라마.csv\n",
      "data/movieReview/formmated/99652-드라마-서사.csv\n",
      "data/movieReview/formmated/119868-액션-SF-드라마.csv\n",
      "data/movieReview/formmated/127400-액션-드라마-스릴러.csv\n",
      "data/movieReview/formmated/44020-드라마.csv\n",
      "data/movieReview/formmated/41350-멜로-로맨스-코미디.csv\n",
      "data/movieReview/formmated/65754-멜로-로맨스-코미디-드라마.csv\n",
      "data/movieReview/formmated/136201-드라마-가족.csv\n",
      "data/movieReview/formmated/91511-드라마.csv\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "targetDir = 'data/movieReview/formmated'\n",
    "allFileNames = os.listdir(targetDir)\n",
    "\n",
    "targetPaths = []\n",
    "all_contents = []\n",
    "all_points = []\n",
    "\n",
    "for fname in allFileNames:\n",
    "    if fname[0] != '.':\n",
    "        targetPaths.append(os.path.join(targetDir, fname))\n",
    "idx = 0\n",
    "for data_path in targetPaths : \n",
    "    print(data_path)\n",
    "    contents, points = loading_rdata(data_path)\n",
    "    # contents, points = loading_rdata(data_path, eng=True, num=True, punc=False)\n",
    "    if len(contents) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        contents = cut(contents,cut=3) #cut은 몇글자를 자를지 선택하는 것\n",
    "\n",
    "        all_contents.extend(contents)\n",
    "        all_points.extend(points)\n",
    "    idx +=1\n",
    "    \n",
    "    if idx > 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사전단어수 : 51503\n"
     ]
    }
   ],
   "source": [
    "# tranform document to vector\n",
    "max_document_length = 200\n",
    "x, vocabulary, vocab_size = make_input(all_contents,max_document_length)\n",
    "print('사전단어수 : %s' % (vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = make_output(all_points,threshold_low=3.5,threshold_high=6.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide dataset into train/test set\n",
    "x_train, x_test, y_train, y_test = divide(x,y,train_prop=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40976, 40976, 40976,    90,   483,  1173,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of embedded vector (default: 128)\")\n",
    "flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\") #단어 개수 = filter의 row size\n",
    "flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\") #filter의 column size\n",
    "flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\") \n",
    "flags.DEFINE_float(\"l2_reg_lambda\", 0.1, \"L2 regularization lambda (default: 0.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "flags.DEFINE_integer(\"num_epochs\", 10, \"Number of training epochs (default: 200)\")\n",
    "flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Misc Parameters\n",
    "flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "# FLAGS._parse_flags() #현재 없어진 함수인듯??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=<absl.flags._flag.BooleanFlag object at 0x7f6d957127b8>\n",
      "BATCH_SIZE=<absl.flags._flag.Flag object at 0x7f6d9a86c2e8>\n",
      "CHECKPOINT_EVERY=<absl.flags._flag.Flag object at 0x7f6d9a86c0b8>\n",
      "DROPOUT_KEEP_PROB=<absl.flags._flag.Flag object at 0x7f6d9a86c278>\n",
      "EMBEDDING_DIM=<absl.flags._flag.Flag object at 0x7f6d9a86c860>\n",
      "EVALUATE_EVERY=<absl.flags._flag.Flag object at 0x7f6d9a86c978>\n",
      "FILTER_SIZES=<absl.flags._flag.Flag object at 0x7f6d9a86c6d8>\n",
      "L2_REG_LAMBDA=<absl.flags._flag.Flag object at 0x7f6d9a86c550>\n",
      "LOG_DEVICE_PLACEMENT=<absl.flags._flag.BooleanFlag object at 0x7f6d956e60b8>\n",
      "NUM_CHECKPOINTS=<absl.flags._flag.Flag object at 0x7f6d9a885fd0>\n",
      "NUM_EPOCHS=<absl.flags._flag.Flag object at 0x7f6d9a86c160>\n",
      "NUM_FILTERS=<absl.flags._flag.Flag object at 0x7f6d9a86cc50>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-c9a5358bf4c1>:87: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0 is illegal; using embedding/W_0 instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0 is illegal; using embedding/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0 is illegal; using conv-maxpool-3/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0 is illegal; using conv-maxpool-3/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0 is illegal; using conv-maxpool-3/b_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0 is illegal; using conv-maxpool-3/b_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0 is illegal; using conv-maxpool-4/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0 is illegal; using conv-maxpool-4/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0 is illegal; using conv-maxpool-4/b_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0 is illegal; using conv-maxpool-4/b_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0 is illegal; using conv-maxpool-5/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0 is illegal; using conv-maxpool-5/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0 is illegal; using conv-maxpool-5/b_0 instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0 is illegal; using conv-maxpool-5/b_0 instead.\n",
      "INFO:tensorflow:Summary name W:0 is illegal; using W_0 instead.\n",
      "INFO:tensorflow:Summary name W:0 is illegal; using W_0 instead.\n",
      "INFO:tensorflow:Summary name output/b:0 is illegal; using output/b_0 instead.\n",
      "INFO:tensorflow:Summary name output/b:0 is illegal; using output/b_0 instead.\n",
      "Writing to /home/yejinlee/emotionTest/runs/1523427711\n",
      "\n",
      "2018-04-11T15:21:52.180014: step 1, loss 2.37559, acc 0.609375\n",
      "2018-04-11T15:21:52.725124: step 2, loss 2.45666, acc 0.5\n",
      "2018-04-11T15:21:53.237107: step 3, loss 1.9254, acc 0.5\n",
      "2018-04-11T15:21:53.806147: step 4, loss 1.9058, acc 0.515625\n",
      "2018-04-11T15:21:54.328796: step 5, loss 1.62469, acc 0.609375\n",
      "2018-04-11T15:21:55.011542: step 6, loss 1.68576, acc 0.609375\n",
      "2018-04-11T15:21:55.604826: step 7, loss 1.50837, acc 0.703125\n",
      "2018-04-11T15:21:56.175877: step 8, loss 1.99559, acc 0.734375\n",
      "2018-04-11T15:21:56.703861: step 9, loss 1.78599, acc 0.71875\n",
      "2018-04-11T15:21:57.209795: step 10, loss 2.52381, acc 0.578125\n",
      "2018-04-11T15:21:57.735612: step 11, loss 2.20407, acc 0.65625\n",
      "2018-04-11T15:21:58.248076: step 12, loss 2.12426, acc 0.625\n",
      "2018-04-11T15:21:58.807858: step 13, loss 1.70087, acc 0.671875\n",
      "2018-04-11T15:21:59.382794: step 14, loss 1.76472, acc 0.640625\n",
      "2018-04-11T15:21:59.993484: step 15, loss 1.663, acc 0.5\n",
      "2018-04-11T15:22:00.540651: step 16, loss 2.31084, acc 0.40625\n",
      "2018-04-11T15:22:01.054972: step 17, loss 1.96346, acc 0.609375\n",
      "2018-04-11T15:22:01.564597: step 18, loss 1.47166, acc 0.578125\n",
      "2018-04-11T15:22:02.077832: step 19, loss 1.73146, acc 0.5625\n",
      "2018-04-11T15:22:02.604888: step 20, loss 1.3229, acc 0.65625\n",
      "2018-04-11T15:22:03.173996: step 21, loss 1.6727, acc 0.609375\n",
      "2018-04-11T15:22:03.687510: step 22, loss 1.56627, acc 0.65625\n",
      "2018-04-11T15:22:04.201320: step 23, loss 1.48183, acc 0.6875\n",
      "2018-04-11T15:22:04.765853: step 24, loss 1.25441, acc 0.75\n",
      "2018-04-11T15:22:05.304175: step 25, loss 2.18769, acc 0.65625\n",
      "2018-04-11T15:22:05.910356: step 26, loss 1.69529, acc 0.6875\n",
      "2018-04-11T15:22:06.482241: step 27, loss 1.47574, acc 0.609375\n",
      "2018-04-11T15:22:06.992123: step 28, loss 1.41419, acc 0.671875\n",
      "2018-04-11T15:22:07.505216: step 29, loss 1.41518, acc 0.703125\n",
      "2018-04-11T15:22:08.011158: step 30, loss 1.7, acc 0.609375\n",
      "2018-04-11T15:22:08.533402: step 31, loss 1.50362, acc 0.625\n",
      "2018-04-11T15:22:09.047508: step 32, loss 1.5995, acc 0.515625\n",
      "2018-04-11T15:22:09.594570: step 33, loss 1.70439, acc 0.609375\n",
      "2018-04-11T15:22:10.124384: step 34, loss 1.72425, acc 0.625\n",
      "2018-04-11T15:22:10.636509: step 35, loss 1.23417, acc 0.609375\n",
      "2018-04-11T15:22:11.191836: step 36, loss 1.65703, acc 0.5625\n",
      "2018-04-11T15:22:11.804756: step 37, loss 1.67005, acc 0.59375\n",
      "2018-04-11T15:22:12.359945: step 38, loss 1.75468, acc 0.515625\n",
      "2018-04-11T15:22:12.970520: step 39, loss 1.31334, acc 0.609375\n",
      "2018-04-11T15:22:13.610885: step 40, loss 1.34487, acc 0.65625\n",
      "2018-04-11T15:22:14.181262: step 41, loss 1.04911, acc 0.75\n",
      "2018-04-11T15:22:14.814262: step 42, loss 1.59923, acc 0.6875\n",
      "2018-04-11T15:22:15.347556: step 43, loss 1.58296, acc 0.671875\n",
      "2018-04-11T15:22:15.864606: step 44, loss 1.32691, acc 0.765625\n",
      "2018-04-11T15:22:16.395489: step 45, loss 2.5224, acc 0.609375\n",
      "2018-04-11T15:22:16.912561: step 46, loss 1.49564, acc 0.671875\n",
      "2018-04-11T15:22:17.437432: step 47, loss 1.57328, acc 0.6875\n",
      "2018-04-11T15:22:17.951272: step 48, loss 1.25612, acc 0.6875\n",
      "2018-04-11T15:22:18.478570: step 49, loss 1.76345, acc 0.53125\n",
      "2018-04-11T15:22:19.002782: step 50, loss 1.87664, acc 0.53125\n",
      "2018-04-11T15:22:19.523101: step 51, loss 1.76649, acc 0.59375\n",
      "2018-04-11T15:22:20.047728: step 52, loss 1.43655, acc 0.640625\n",
      "2018-04-11T15:22:20.570757: step 53, loss 1.58499, acc 0.578125\n",
      "2018-04-11T15:22:21.095739: step 54, loss 1.4942, acc 0.640625\n",
      "2018-04-11T15:22:21.620535: step 55, loss 1.38795, acc 0.65625\n",
      "2018-04-11T15:22:22.143469: step 56, loss 1.20889, acc 0.59375\n",
      "2018-04-11T15:22:22.662960: step 57, loss 1.69803, acc 0.546875\n",
      "2018-04-11T15:22:23.197473: step 58, loss 1.91398, acc 0.546875\n",
      "2018-04-11T15:22:23.736689: step 59, loss 1.25103, acc 0.71875\n",
      "2018-04-11T15:22:24.272011: step 60, loss 1.62059, acc 0.640625\n",
      "2018-04-11T15:22:24.801490: step 61, loss 1.30992, acc 0.625\n",
      "2018-04-11T15:22:25.339874: step 62, loss 1.43932, acc 0.734375\n",
      "2018-04-11T15:22:25.865633: step 63, loss 1.431, acc 0.640625\n",
      "2018-04-11T15:22:26.382445: step 64, loss 1.65655, acc 0.640625\n",
      "2018-04-11T15:22:26.909122: step 65, loss 1.37786, acc 0.65625\n",
      "2018-04-11T15:22:27.440126: step 66, loss 1.22132, acc 0.75\n",
      "2018-04-11T15:22:27.966427: step 67, loss 1.32859, acc 0.703125\n",
      "2018-04-11T15:22:28.518343: step 68, loss 1.03731, acc 0.71875\n",
      "2018-04-11T15:22:29.214318: step 69, loss 1.24143, acc 0.640625\n",
      "2018-04-11T15:22:29.863554: step 70, loss 1.21349, acc 0.671875\n",
      "2018-04-11T15:22:30.450279: step 71, loss 1.2392, acc 0.71875\n",
      "2018-04-11T15:22:31.095800: step 72, loss 1.37796, acc 0.640625\n",
      "2018-04-11T15:22:31.682656: step 73, loss 1.39743, acc 0.59375\n",
      "2018-04-11T15:22:32.276035: step 74, loss 1.24957, acc 0.578125\n",
      "2018-04-11T15:22:32.837944: step 75, loss 1.72303, acc 0.625\n",
      "2018-04-11T15:22:33.432827: step 76, loss 1.52287, acc 0.59375\n",
      "2018-04-11T15:22:34.010168: step 77, loss 1.193, acc 0.640625\n",
      "2018-04-11T15:22:34.627009: step 78, loss 1.37499, acc 0.671875\n",
      "2018-04-11T15:22:35.200765: step 79, loss 1.70125, acc 0.578125\n",
      "2018-04-11T15:22:35.747121: step 80, loss 1.59012, acc 0.59375\n",
      "2018-04-11T15:22:36.334314: step 81, loss 1.09575, acc 0.765625\n",
      "2018-04-11T15:22:36.928100: step 82, loss 1.45676, acc 0.640625\n",
      "2018-04-11T15:22:37.458283: step 83, loss 1.48612, acc 0.59375\n",
      "2018-04-11T15:22:38.046868: step 84, loss 1.56872, acc 0.671875\n",
      "2018-04-11T15:22:38.636781: step 85, loss 1.42406, acc 0.65625\n",
      "2018-04-11T15:22:39.226572: step 86, loss 1.62258, acc 0.609375\n",
      "2018-04-11T15:22:39.800939: step 87, loss 1.69405, acc 0.609375\n",
      "2018-04-11T15:22:40.400157: step 88, loss 1.42006, acc 0.59375\n",
      "2018-04-11T15:22:41.062126: step 89, loss 1.39934, acc 0.625\n",
      "2018-04-11T15:22:41.717827: step 90, loss 1.28978, acc 0.59375\n",
      "2018-04-11T15:22:42.432574: step 91, loss 1.53881, acc 0.5\n",
      "2018-04-11T15:22:43.096400: step 92, loss 1.68566, acc 0.546875\n",
      "2018-04-11T15:22:43.717125: step 93, loss 1.44072, acc 0.578125\n",
      "2018-04-11T15:22:44.328408: step 94, loss 1.0594, acc 0.71875\n",
      "2018-04-11T15:22:44.886246: step 95, loss 1.22265, acc 0.734375\n",
      "2018-04-11T15:22:45.539486: step 96, loss 1.86899, acc 0.625\n",
      "2018-04-11T15:22:46.120661: step 97, loss 1.24629, acc 0.671875\n",
      "2018-04-11T15:22:46.722191: step 98, loss 1.52336, acc 0.59375\n",
      "2018-04-11T15:22:47.380716: step 99, loss 1.43525, acc 0.59375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:22:47.964494: step 100, loss 1.48424, acc 0.625\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:22:48.155103: step 100, loss 0.762079, acc 0.85\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-100\n",
      "\n",
      "2018-04-11T15:22:48.926308: step 101, loss 1.49055, acc 0.671875\n",
      "2018-04-11T15:22:49.483283: step 102, loss 1.07738, acc 0.765625\n",
      "2018-04-11T15:22:50.032842: step 103, loss 2.03502, acc 0.46875\n",
      "2018-04-11T15:22:50.612787: step 104, loss 1.29991, acc 0.65625\n",
      "2018-04-11T15:22:51.195478: step 105, loss 1.50108, acc 0.546875\n",
      "2018-04-11T15:22:51.778363: step 106, loss 1.56746, acc 0.609375\n",
      "2018-04-11T15:22:52.417898: step 107, loss 0.910629, acc 0.71875\n",
      "2018-04-11T15:22:53.051440: step 108, loss 1.50632, acc 0.515625\n",
      "2018-04-11T15:22:53.643286: step 109, loss 1.30175, acc 0.65625\n",
      "2018-04-11T15:22:54.252897: step 110, loss 1.34655, acc 0.625\n",
      "2018-04-11T15:22:54.795485: step 111, loss 1.22625, acc 0.625\n",
      "2018-04-11T15:22:55.380796: step 112, loss 1.33778, acc 0.59375\n",
      "2018-04-11T15:22:56.010910: step 113, loss 1.26816, acc 0.59375\n",
      "2018-04-11T15:22:56.594333: step 114, loss 1.10477, acc 0.671875\n",
      "2018-04-11T15:22:57.163838: step 115, loss 1.25285, acc 0.625\n",
      "2018-04-11T15:22:57.752256: step 116, loss 0.876854, acc 0.71875\n",
      "2018-04-11T15:22:58.332833: step 117, loss 1.41337, acc 0.59375\n",
      "2018-04-11T15:22:58.891371: step 118, loss 1.12073, acc 0.671875\n",
      "2018-04-11T15:22:59.482999: step 119, loss 0.997503, acc 0.703125\n",
      "2018-04-11T15:23:00.126242: step 120, loss 1.10185, acc 0.6875\n",
      "2018-04-11T15:23:00.693532: step 121, loss 0.879353, acc 0.8125\n",
      "2018-04-11T15:23:01.311236: step 122, loss 0.970204, acc 0.703125\n",
      "2018-04-11T15:23:01.904863: step 123, loss 1.06374, acc 0.734375\n",
      "2018-04-11T15:23:02.507060: step 124, loss 1.17846, acc 0.765625\n",
      "2018-04-11T15:23:03.056029: step 125, loss 1.30613, acc 0.6875\n",
      "2018-04-11T15:23:03.676103: step 126, loss 0.882381, acc 0.71875\n",
      "2018-04-11T15:23:04.242556: step 127, loss 1.3174, acc 0.65625\n",
      "2018-04-11T15:23:04.847497: step 128, loss 1.31301, acc 0.71875\n",
      "2018-04-11T15:23:05.449762: step 129, loss 1.50696, acc 0.59375\n",
      "2018-04-11T15:23:06.054195: step 130, loss 1.15733, acc 0.625\n",
      "2018-04-11T15:23:06.621606: step 131, loss 0.950735, acc 0.765625\n",
      "2018-04-11T15:23:07.206239: step 132, loss 1.4175, acc 0.5625\n",
      "2018-04-11T15:23:07.841106: step 133, loss 1.34644, acc 0.546875\n",
      "2018-04-11T15:23:08.516951: step 134, loss 0.995995, acc 0.625\n",
      "2018-04-11T15:23:09.082762: step 135, loss 1.22535, acc 0.6875\n",
      "2018-04-11T15:23:09.712725: step 136, loss 1.65175, acc 0.5625\n",
      "2018-04-11T15:23:10.337928: step 137, loss 1.01696, acc 0.703125\n",
      "2018-04-11T15:23:10.973783: step 138, loss 1.2657, acc 0.625\n",
      "2018-04-11T15:23:11.579202: step 139, loss 1.20873, acc 0.6875\n",
      "2018-04-11T15:23:12.222258: step 140, loss 1.06899, acc 0.640625\n",
      "2018-04-11T15:23:12.857161: step 141, loss 1.605, acc 0.59375\n",
      "2018-04-11T15:23:13.460570: step 142, loss 1.11355, acc 0.734375\n",
      "2018-04-11T15:23:14.036057: step 143, loss 1.44211, acc 0.671875\n",
      "2018-04-11T15:23:14.610051: step 144, loss 1.23643, acc 0.65625\n",
      "2018-04-11T15:23:15.307099: step 145, loss 1.32219, acc 0.59375\n",
      "2018-04-11T15:23:15.942988: step 146, loss 1.26988, acc 0.578125\n",
      "2018-04-11T15:23:16.511629: step 147, loss 1.01632, acc 0.71875\n",
      "2018-04-11T15:23:17.099793: step 148, loss 1.04248, acc 0.640625\n",
      "2018-04-11T15:23:17.725436: step 149, loss 1.0587, acc 0.640625\n",
      "2018-04-11T15:23:18.371264: step 150, loss 1.20499, acc 0.546875\n",
      "2018-04-11T15:23:19.013418: step 151, loss 1.17485, acc 0.671875\n",
      "2018-04-11T15:23:19.636186: step 152, loss 1.18888, acc 0.609375\n",
      "2018-04-11T15:23:20.269377: step 153, loss 1.2549, acc 0.625\n",
      "2018-04-11T15:23:20.873264: step 154, loss 1.06505, acc 0.703125\n",
      "2018-04-11T15:23:21.515811: step 155, loss 0.965013, acc 0.71875\n",
      "2018-04-11T15:23:22.159687: step 156, loss 1.17539, acc 0.703125\n",
      "2018-04-11T15:23:22.827275: step 157, loss 0.888535, acc 0.8125\n",
      "2018-04-11T15:23:23.438099: step 158, loss 0.862989, acc 0.734375\n",
      "2018-04-11T15:23:24.062869: step 159, loss 1.28691, acc 0.671875\n",
      "2018-04-11T15:23:24.699380: step 160, loss 1.21981, acc 0.78125\n",
      "2018-04-11T15:23:25.420877: step 161, loss 0.945841, acc 0.75\n",
      "2018-04-11T15:23:26.125893: step 162, loss 1.33944, acc 0.75\n",
      "2018-04-11T15:23:26.717149: step 163, loss 1.11042, acc 0.703125\n",
      "2018-04-11T15:23:27.380285: step 164, loss 1.23681, acc 0.734375\n",
      "2018-04-11T15:23:28.062628: step 165, loss 1.36304, acc 0.671875\n",
      "2018-04-11T15:23:28.668250: step 166, loss 1.17269, acc 0.703125\n",
      "2018-04-11T15:23:29.329810: step 167, loss 0.894606, acc 0.71875\n",
      "2018-04-11T15:23:29.990685: step 168, loss 0.931038, acc 0.6875\n",
      "2018-04-11T15:23:30.594627: step 169, loss 0.896773, acc 0.6875\n",
      "2018-04-11T15:23:31.218686: step 170, loss 1.24998, acc 0.625\n",
      "2018-04-11T15:23:31.790373: step 171, loss 0.869431, acc 0.765625\n",
      "2018-04-11T15:23:32.463461: step 172, loss 1.04998, acc 0.65625\n",
      "2018-04-11T15:23:33.106568: step 173, loss 1.34878, acc 0.640625\n",
      "2018-04-11T15:23:33.756544: step 174, loss 0.912532, acc 0.71875\n",
      "2018-04-11T15:23:34.431148: step 175, loss 1.10466, acc 0.6875\n",
      "2018-04-11T15:23:35.040949: step 176, loss 1.10457, acc 0.671875\n",
      "2018-04-11T15:23:35.613448: step 177, loss 0.90786, acc 0.734375\n",
      "2018-04-11T15:23:36.188252: step 178, loss 1.30144, acc 0.671875\n",
      "2018-04-11T15:23:36.851017: step 179, loss 0.960351, acc 0.671875\n",
      "2018-04-11T15:23:37.443522: step 180, loss 0.840724, acc 0.734375\n",
      "2018-04-11T15:23:38.046392: step 181, loss 1.40579, acc 0.59375\n",
      "2018-04-11T15:23:38.750813: step 182, loss 1.11364, acc 0.703125\n",
      "2018-04-11T15:23:39.336527: step 183, loss 1.43848, acc 0.5625\n",
      "2018-04-11T15:23:39.961254: step 184, loss 1.18842, acc 0.671875\n",
      "2018-04-11T15:23:40.538535: step 185, loss 1.35178, acc 0.578125\n",
      "2018-04-11T15:23:41.123394: step 186, loss 0.855834, acc 0.703125\n",
      "2018-04-11T15:23:41.758049: step 187, loss 0.94142, acc 0.671875\n",
      "2018-04-11T15:23:42.352127: step 188, loss 0.9056, acc 0.71875\n",
      "2018-04-11T15:23:42.984348: step 189, loss 1.20739, acc 0.671875\n",
      "2018-04-11T15:23:43.680643: step 190, loss 1.09584, acc 0.5625\n",
      "2018-04-11T15:23:44.254662: step 191, loss 1.28476, acc 0.53125\n",
      "2018-04-11T15:23:44.876029: step 192, loss 0.886079, acc 0.71875\n",
      "2018-04-11T15:23:45.505943: step 193, loss 0.978168, acc 0.65625\n",
      "2018-04-11T15:23:46.090691: step 194, loss 1.04465, acc 0.703125\n",
      "2018-04-11T15:23:46.692388: step 195, loss 0.962715, acc 0.6875\n",
      "2018-04-11T15:23:47.315825: step 196, loss 0.937541, acc 0.703125\n",
      "2018-04-11T15:23:47.889918: step 197, loss 0.831852, acc 0.703125\n",
      "2018-04-11T15:23:48.452318: step 198, loss 1.14233, acc 0.703125\n",
      "2018-04-11T15:23:49.044852: step 199, loss 0.935243, acc 0.765625\n",
      "2018-04-11T15:23:49.660309: step 200, loss 0.916358, acc 0.6875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:23:49.827216: step 200, loss 0.811103, acc 0.8\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-200\n",
      "\n",
      "2018-04-11T15:23:50.503014: step 201, loss 0.692319, acc 0.765625\n",
      "2018-04-11T15:23:51.077258: step 202, loss 1.1571, acc 0.6875\n",
      "2018-04-11T15:23:51.736909: step 203, loss 1.16037, acc 0.625\n",
      "2018-04-11T15:23:52.358850: step 204, loss 0.957698, acc 0.65625\n",
      "2018-04-11T15:23:53.013712: step 205, loss 0.914226, acc 0.703125\n",
      "2018-04-11T15:23:53.582271: step 206, loss 0.896903, acc 0.796875\n",
      "2018-04-11T15:23:54.132380: step 207, loss 1.34147, acc 0.59375\n",
      "2018-04-11T15:23:54.673620: step 208, loss 1.01363, acc 0.71875\n",
      "2018-04-11T15:23:55.233937: step 209, loss 0.992157, acc 0.65625\n",
      "2018-04-11T15:23:55.794064: step 210, loss 0.937895, acc 0.671875\n",
      "2018-04-11T15:23:56.330290: step 211, loss 1.18183, acc 0.609375\n",
      "2018-04-11T15:23:56.872156: step 212, loss 0.860976, acc 0.625\n",
      "2018-04-11T15:23:57.533892: step 213, loss 1.298, acc 0.625\n",
      "2018-04-11T15:23:58.095093: step 214, loss 1.06547, acc 0.65625\n",
      "2018-04-11T15:23:58.654129: step 215, loss 1.01998, acc 0.6875\n",
      "2018-04-11T15:23:59.217439: step 216, loss 1.10537, acc 0.609375\n",
      "2018-04-11T15:23:59.783259: step 217, loss 0.873843, acc 0.71875\n",
      "2018-04-11T15:24:00.386506: step 218, loss 0.849481, acc 0.734375\n",
      "2018-04-11T15:24:00.977591: step 219, loss 0.894614, acc 0.734375\n",
      "2018-04-11T15:24:01.522113: step 220, loss 1.08284, acc 0.703125\n",
      "2018-04-11T15:24:02.071945: step 221, loss 0.832406, acc 0.703125\n",
      "2018-04-11T15:24:02.650085: step 222, loss 0.870189, acc 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:24:03.213940: step 223, loss 0.662557, acc 0.859375\n",
      "2018-04-11T15:24:03.915143: step 224, loss 1.14995, acc 0.734375\n",
      "2018-04-11T15:24:04.570248: step 225, loss 0.726319, acc 0.828125\n",
      "2018-04-11T15:24:05.201293: step 226, loss 1.02806, acc 0.703125\n",
      "2018-04-11T15:24:05.839101: step 227, loss 1.09118, acc 0.65625\n",
      "2018-04-11T15:24:06.483831: step 228, loss 0.928461, acc 0.703125\n",
      "2018-04-11T15:24:07.197208: step 229, loss 1.2593, acc 0.765625\n",
      "2018-04-11T15:24:07.825741: step 230, loss 1.07197, acc 0.671875\n",
      "2018-04-11T15:24:08.475816: step 231, loss 0.922859, acc 0.78125\n",
      "2018-04-11T15:24:09.071884: step 232, loss 1.12291, acc 0.625\n",
      "2018-04-11T15:24:09.625087: step 233, loss 1.01964, acc 0.671875\n",
      "2018-04-11T15:24:10.280808: step 234, loss 0.979407, acc 0.640625\n",
      "2018-04-11T15:24:10.843045: step 235, loss 1.15849, acc 0.609375\n",
      "2018-04-11T15:24:11.525838: step 236, loss 1.1056, acc 0.65625\n",
      "2018-04-11T15:24:12.133049: step 237, loss 0.884587, acc 0.6875\n",
      "2018-04-11T15:24:12.674519: step 238, loss 1.11517, acc 0.6875\n",
      "2018-04-11T15:24:13.224659: step 239, loss 1.07416, acc 0.71875\n",
      "2018-04-11T15:24:13.769109: step 240, loss 1.12366, acc 0.671875\n",
      "2018-04-11T15:24:14.309227: step 241, loss 1.10119, acc 0.65625\n",
      "2018-04-11T15:24:14.961887: step 242, loss 0.938768, acc 0.6875\n",
      "2018-04-11T15:24:15.664361: step 243, loss 1.00393, acc 0.703125\n",
      "2018-04-11T15:24:16.311601: step 244, loss 1.19187, acc 0.609375\n",
      "2018-04-11T15:24:16.949401: step 245, loss 0.919994, acc 0.71875\n",
      "2018-04-11T15:24:17.589175: step 246, loss 1.04115, acc 0.703125\n",
      "2018-04-11T15:24:18.247552: step 247, loss 0.995574, acc 0.65625\n",
      "2018-04-11T15:24:18.821258: step 248, loss 0.867941, acc 0.75\n",
      "2018-04-11T15:24:19.488846: step 249, loss 1.07054, acc 0.75\n",
      "2018-04-11T15:24:20.105093: step 250, loss 0.927772, acc 0.65625\n",
      "2018-04-11T15:24:20.724642: step 251, loss 1.03481, acc 0.703125\n",
      "2018-04-11T15:24:21.347807: step 252, loss 1.10935, acc 0.65625\n",
      "2018-04-11T15:24:21.919118: step 253, loss 1.04463, acc 0.625\n",
      "2018-04-11T15:24:22.589724: step 254, loss 1.07682, acc 0.671875\n",
      "2018-04-11T15:24:23.207146: step 255, loss 0.798992, acc 0.71875\n",
      "2018-04-11T15:24:23.757129: step 256, loss 1.10123, acc 0.59375\n",
      "2018-04-11T15:24:24.356322: step 257, loss 0.920961, acc 0.703125\n",
      "2018-04-11T15:24:24.891754: step 258, loss 0.927317, acc 0.734375\n",
      "2018-04-11T15:24:25.551514: step 259, loss 0.85588, acc 0.71875\n",
      "2018-04-11T15:24:26.162544: step 260, loss 0.900632, acc 0.71875\n",
      "2018-04-11T15:24:26.727879: step 261, loss 1.23235, acc 0.59375\n",
      "2018-04-11T15:24:27.345103: step 262, loss 0.840473, acc 0.75\n",
      "2018-04-11T15:24:27.944266: step 263, loss 0.897591, acc 0.703125\n",
      "2018-04-11T15:24:28.543218: step 264, loss 0.740717, acc 0.828125\n",
      "2018-04-11T15:24:29.121054: step 265, loss 1.05156, acc 0.703125\n",
      "2018-04-11T15:24:29.703274: step 266, loss 0.830054, acc 0.78125\n",
      "2018-04-11T15:24:30.365045: step 267, loss 0.76515, acc 0.765625\n",
      "2018-04-11T15:24:30.976442: step 268, loss 1.33043, acc 0.609375\n",
      "2018-04-11T15:24:31.542026: step 269, loss 0.830921, acc 0.796875\n",
      "2018-04-11T15:24:32.170116: step 270, loss 0.894034, acc 0.75\n",
      "2018-04-11T15:24:32.732488: step 271, loss 0.866344, acc 0.765625\n",
      "2018-04-11T15:24:33.388175: step 272, loss 0.91985, acc 0.703125\n",
      "2018-04-11T15:24:33.981298: step 273, loss 0.850463, acc 0.78125\n",
      "2018-04-11T15:24:34.635787: step 274, loss 1.02476, acc 0.671875\n",
      "2018-04-11T15:24:35.265333: step 275, loss 0.7195, acc 0.71875\n",
      "2018-04-11T15:24:35.914103: step 276, loss 1.05975, acc 0.671875\n",
      "2018-04-11T15:24:36.567983: step 277, loss 0.962521, acc 0.578125\n",
      "2018-04-11T15:24:37.182152: step 278, loss 0.920801, acc 0.625\n",
      "2018-04-11T15:24:37.832208: step 279, loss 0.766038, acc 0.703125\n",
      "2018-04-11T15:24:38.555993: step 280, loss 0.977537, acc 0.6875\n",
      "2018-04-11T15:24:39.227951: step 281, loss 0.900497, acc 0.6875\n",
      "2018-04-11T15:24:39.978088: step 282, loss 0.702258, acc 0.796875\n",
      "2018-04-11T15:24:40.685490: step 283, loss 0.88755, acc 0.703125\n",
      "2018-04-11T15:24:41.418229: step 284, loss 0.997158, acc 0.703125\n",
      "2018-04-11T15:24:42.093715: step 285, loss 0.924338, acc 0.640625\n",
      "2018-04-11T15:24:42.728749: step 286, loss 0.96539, acc 0.75\n",
      "2018-04-11T15:24:43.363054: step 287, loss 0.719603, acc 0.8125\n",
      "2018-04-11T15:24:43.992983: step 288, loss 0.957038, acc 0.65625\n",
      "2018-04-11T15:24:44.604901: step 289, loss 0.966045, acc 0.671875\n",
      "2018-04-11T15:24:45.284074: step 290, loss 0.935756, acc 0.640625\n",
      "2018-04-11T15:24:45.980863: step 291, loss 1.19343, acc 0.5625\n",
      "2018-04-11T15:24:46.576650: step 292, loss 1.04307, acc 0.65625\n",
      "2018-04-11T15:24:47.192906: step 293, loss 0.832016, acc 0.734375\n",
      "2018-04-11T15:24:47.849595: step 294, loss 0.744528, acc 0.75\n",
      "2018-04-11T15:24:48.487210: step 295, loss 0.79354, acc 0.734375\n",
      "2018-04-11T15:24:49.145794: step 296, loss 0.856655, acc 0.703125\n",
      "2018-04-11T15:24:49.760682: step 297, loss 0.807001, acc 0.703125\n",
      "2018-04-11T15:24:50.378202: step 298, loss 0.901792, acc 0.71875\n",
      "2018-04-11T15:24:50.993722: step 299, loss 0.703253, acc 0.75\n",
      "2018-04-11T15:24:51.614027: step 300, loss 0.848158, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:24:51.833180: step 300, loss 0.902689, acc 0.68\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-300\n",
      "\n",
      "2018-04-11T15:24:52.634036: step 301, loss 0.719763, acc 0.796875\n",
      "2018-04-11T15:24:53.235711: step 302, loss 0.858793, acc 0.765625\n",
      "2018-04-11T15:24:53.835222: step 303, loss 0.712635, acc 0.828125\n",
      "2018-04-11T15:24:54.431445: step 304, loss 0.887134, acc 0.78125\n",
      "2018-04-11T15:24:55.030136: step 305, loss 1.04413, acc 0.640625\n",
      "2018-04-11T15:24:55.672794: step 306, loss 1.00315, acc 0.71875\n",
      "2018-04-11T15:24:56.261319: step 307, loss 0.79435, acc 0.8125\n",
      "2018-04-11T15:24:56.929738: step 308, loss 0.821708, acc 0.765625\n",
      "2018-04-11T15:24:57.523309: step 309, loss 0.998179, acc 0.703125\n",
      "2018-04-11T15:24:58.113405: step 310, loss 0.927819, acc 0.78125\n",
      "2018-04-11T15:24:58.819076: step 311, loss 1.40876, acc 0.546875\n",
      "2018-04-11T15:24:59.555579: step 312, loss 0.83667, acc 0.6875\n",
      "2018-04-11T15:25:00.273791: step 313, loss 0.880522, acc 0.6875\n",
      "2018-04-11T15:25:01.018997: step 314, loss 0.929359, acc 0.609375\n",
      "2018-04-11T15:25:01.674102: step 315, loss 0.8917, acc 0.609375\n",
      "2018-04-11T15:25:02.424227: step 316, loss 0.894351, acc 0.734375\n",
      "2018-04-11T15:25:03.031193: step 317, loss 0.884546, acc 0.71875\n",
      "2018-04-11T15:25:03.614834: step 318, loss 1.12455, acc 0.65625\n",
      "2018-04-11T15:25:04.198019: step 319, loss 0.977549, acc 0.71875\n",
      "2018-04-11T15:25:04.867637: step 320, loss 0.779237, acc 0.71875\n",
      "2018-04-11T15:25:05.554018: step 321, loss 0.787942, acc 0.78125\n",
      "2018-04-11T15:25:06.190869: step 322, loss 0.801416, acc 0.703125\n",
      "2018-04-11T15:25:06.881749: step 323, loss 0.812981, acc 0.796875\n",
      "2018-04-11T15:25:07.551951: step 324, loss 0.749315, acc 0.796875\n",
      "2018-04-11T15:25:08.208758: step 325, loss 1.0999, acc 0.6875\n",
      "2018-04-11T15:25:08.821871: step 326, loss 0.81684, acc 0.765625\n",
      "2018-04-11T15:25:09.520974: step 327, loss 0.826439, acc 0.703125\n",
      "2018-04-11T15:25:10.185050: step 328, loss 0.706303, acc 0.765625\n",
      "2018-04-11T15:25:10.856369: step 329, loss 0.82666, acc 0.75\n",
      "2018-04-11T15:25:11.506335: step 330, loss 0.725319, acc 0.78125\n",
      "2018-04-11T15:25:12.167334: step 331, loss 0.817141, acc 0.734375\n",
      "2018-04-11T15:25:12.800597: step 332, loss 0.642546, acc 0.8125\n",
      "2018-04-11T15:25:13.446991: step 333, loss 0.768485, acc 0.8125\n",
      "2018-04-11T15:25:14.128832: step 334, loss 0.876797, acc 0.71875\n",
      "2018-04-11T15:25:14.764453: step 335, loss 0.925543, acc 0.75\n",
      "2018-04-11T15:25:15.392274: step 336, loss 0.544793, acc 0.84375\n",
      "2018-04-11T15:25:16.053566: step 337, loss 0.933672, acc 0.703125\n",
      "2018-04-11T15:25:16.713820: step 338, loss 0.573895, acc 0.84375\n",
      "2018-04-11T15:25:17.388526: step 339, loss 0.964168, acc 0.75\n",
      "2018-04-11T15:25:18.095214: step 340, loss 0.682371, acc 0.78125\n",
      "2018-04-11T15:25:18.690911: step 341, loss 0.893122, acc 0.703125\n",
      "2018-04-11T15:25:19.304215: step 342, loss 1.14743, acc 0.65625\n",
      "2018-04-11T15:25:19.993804: step 343, loss 0.827658, acc 0.703125\n",
      "2018-04-11T15:25:20.591745: step 344, loss 0.967361, acc 0.65625\n",
      "2018-04-11T15:25:21.234037: step 345, loss 0.799858, acc 0.71875\n",
      "2018-04-11T15:25:21.863969: step 346, loss 0.941707, acc 0.734375\n",
      "2018-04-11T15:25:22.442950: step 347, loss 0.857133, acc 0.765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:25:23.074006: step 348, loss 0.620807, acc 0.765625\n",
      "2018-04-11T15:25:23.659377: step 349, loss 0.840888, acc 0.71875\n",
      "2018-04-11T15:25:24.327719: step 350, loss 0.701648, acc 0.8125\n",
      "2018-04-11T15:25:25.010146: step 351, loss 0.906279, acc 0.6875\n",
      "2018-04-11T15:25:25.591913: step 352, loss 0.86846, acc 0.703125\n",
      "2018-04-11T15:25:26.280845: step 353, loss 0.968682, acc 0.640625\n",
      "2018-04-11T15:25:26.946210: step 354, loss 0.939699, acc 0.6875\n",
      "2018-04-11T15:25:27.573687: step 355, loss 0.722293, acc 0.78125\n",
      "2018-04-11T15:25:28.227988: step 356, loss 0.845342, acc 0.703125\n",
      "2018-04-11T15:25:28.860127: step 357, loss 0.990941, acc 0.71875\n",
      "2018-04-11T15:25:29.551258: step 358, loss 0.807961, acc 0.671875\n",
      "2018-04-11T15:25:30.137042: step 359, loss 0.646639, acc 0.78125\n",
      "2018-04-11T15:25:30.803347: step 360, loss 0.650534, acc 0.8125\n",
      "2018-04-11T15:25:31.510100: step 361, loss 0.693117, acc 0.75\n",
      "2018-04-11T15:25:32.148347: step 362, loss 0.933657, acc 0.703125\n",
      "2018-04-11T15:25:32.812433: step 363, loss 0.966077, acc 0.71875\n",
      "2018-04-11T15:25:33.470249: step 364, loss 0.885646, acc 0.640625\n",
      "2018-04-11T15:25:34.106148: step 365, loss 0.777272, acc 0.734375\n",
      "2018-04-11T15:25:34.750286: step 366, loss 0.923691, acc 0.65625\n",
      "2018-04-11T15:25:35.413315: step 367, loss 0.896752, acc 0.65625\n",
      "2018-04-11T15:25:36.118974: step 368, loss 0.710517, acc 0.734375\n",
      "2018-04-11T15:25:36.695155: step 369, loss 1.07128, acc 0.640625\n",
      "2018-04-11T15:25:37.320035: step 370, loss 0.851516, acc 0.6875\n",
      "2018-04-11T15:25:37.990703: step 371, loss 0.901114, acc 0.640625\n",
      "2018-04-11T15:25:38.565458: step 372, loss 0.706922, acc 0.71875\n",
      "2018-04-11T15:25:39.234022: step 373, loss 0.686965, acc 0.78125\n",
      "2018-04-11T15:25:39.849628: step 374, loss 0.756009, acc 0.6875\n",
      "2018-04-11T15:25:40.478571: step 375, loss 0.814335, acc 0.703125\n",
      "2018-04-11T15:25:41.120080: step 376, loss 0.906875, acc 0.6875\n",
      "2018-04-11T15:25:41.760640: step 377, loss 0.880957, acc 0.71875\n",
      "2018-04-11T15:25:42.375220: step 378, loss 0.822046, acc 0.765625\n",
      "2018-04-11T15:25:42.950028: step 379, loss 0.778088, acc 0.75\n",
      "2018-04-11T15:25:43.589106: step 380, loss 0.82311, acc 0.71875\n",
      "2018-04-11T15:25:44.237338: step 381, loss 0.711621, acc 0.78125\n",
      "2018-04-11T15:25:44.852581: step 382, loss 0.741092, acc 0.765625\n",
      "2018-04-11T15:25:45.494429: step 383, loss 0.862775, acc 0.671875\n",
      "2018-04-11T15:25:46.134059: step 384, loss 0.885274, acc 0.8125\n",
      "2018-04-11T15:25:46.802842: step 385, loss 0.728745, acc 0.78125\n",
      "2018-04-11T15:25:47.465341: step 386, loss 0.987082, acc 0.671875\n",
      "2018-04-11T15:25:48.109090: step 387, loss 0.926238, acc 0.703125\n",
      "2018-04-11T15:25:48.756734: step 388, loss 0.668341, acc 0.796875\n",
      "2018-04-11T15:25:49.355112: step 389, loss 0.696396, acc 0.78125\n",
      "2018-04-11T15:25:49.921874: step 390, loss 0.939224, acc 0.640625\n",
      "2018-04-11T15:25:50.500907: step 391, loss 0.925378, acc 0.640625\n",
      "2018-04-11T15:25:51.102986: step 392, loss 0.714789, acc 0.734375\n",
      "2018-04-11T15:25:51.708706: step 393, loss 0.847291, acc 0.6875\n",
      "2018-04-11T15:25:52.302836: step 394, loss 0.88794, acc 0.703125\n",
      "2018-04-11T15:25:52.881030: step 395, loss 0.767792, acc 0.6875\n",
      "2018-04-11T15:25:53.463786: step 396, loss 0.757081, acc 0.78125\n",
      "2018-04-11T15:25:54.054102: step 397, loss 0.774668, acc 0.765625\n",
      "2018-04-11T15:25:54.629301: step 398, loss 0.866877, acc 0.671875\n",
      "2018-04-11T15:25:55.210233: step 399, loss 0.806052, acc 0.640625\n",
      "2018-04-11T15:25:55.796287: step 400, loss 0.682745, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:25:55.977526: step 400, loss 0.681385, acc 0.73\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-400\n",
      "\n",
      "2018-04-11T15:25:56.673385: step 401, loss 0.699184, acc 0.703125\n",
      "2018-04-11T15:25:57.275156: step 402, loss 0.886573, acc 0.65625\n",
      "2018-04-11T15:25:57.854659: step 403, loss 1.01394, acc 0.703125\n",
      "2018-04-11T15:25:58.429750: step 404, loss 0.830033, acc 0.71875\n",
      "2018-04-11T15:25:59.000985: step 405, loss 0.861118, acc 0.734375\n",
      "2018-04-11T15:25:59.569833: step 406, loss 0.753389, acc 0.75\n",
      "2018-04-11T15:26:00.147443: step 407, loss 0.560178, acc 0.875\n",
      "2018-04-11T15:26:00.733815: step 408, loss 0.665594, acc 0.78125\n",
      "2018-04-11T15:26:01.326183: step 409, loss 0.835224, acc 0.734375\n",
      "2018-04-11T15:26:01.901235: step 410, loss 0.819614, acc 0.71875\n",
      "2018-04-11T15:26:02.471272: step 411, loss 0.65879, acc 0.796875\n",
      "2018-04-11T15:26:03.065310: step 412, loss 0.736346, acc 0.734375\n",
      "2018-04-11T15:26:03.637491: step 413, loss 0.612146, acc 0.828125\n",
      "2018-04-11T15:26:04.212500: step 414, loss 0.738287, acc 0.8125\n",
      "2018-04-11T15:26:04.771911: step 415, loss 0.543113, acc 0.796875\n",
      "2018-04-11T15:26:05.325200: step 416, loss 0.870215, acc 0.6875\n",
      "2018-04-11T15:26:05.895151: step 417, loss 0.877485, acc 0.703125\n",
      "2018-04-11T15:26:06.459249: step 418, loss 0.775177, acc 0.78125\n",
      "2018-04-11T15:26:07.014895: step 419, loss 0.789937, acc 0.734375\n",
      "2018-04-11T15:26:07.555783: step 420, loss 0.724162, acc 0.78125\n",
      "2018-04-11T15:26:08.119375: step 421, loss 1.01954, acc 0.671875\n",
      "2018-04-11T15:26:08.693292: step 422, loss 0.881382, acc 0.671875\n",
      "2018-04-11T15:26:09.251782: step 423, loss 0.715374, acc 0.75\n",
      "2018-04-11T15:26:09.821020: step 424, loss 0.62366, acc 0.796875\n",
      "2018-04-11T15:26:10.400055: step 425, loss 0.628177, acc 0.796875\n",
      "2018-04-11T15:26:10.968289: step 426, loss 0.757382, acc 0.65625\n",
      "2018-04-11T15:26:11.531627: step 427, loss 0.73997, acc 0.78125\n",
      "2018-04-11T15:26:12.106333: step 428, loss 0.960725, acc 0.609375\n",
      "2018-04-11T15:26:12.650409: step 429, loss 0.671156, acc 0.78125\n",
      "2018-04-11T15:26:13.228785: step 430, loss 0.891222, acc 0.765625\n",
      "2018-04-11T15:26:13.784454: step 431, loss 0.762922, acc 0.71875\n",
      "2018-04-11T15:26:14.346074: step 432, loss 0.754241, acc 0.71875\n",
      "2018-04-11T15:26:14.923371: step 433, loss 0.790074, acc 0.671875\n",
      "2018-04-11T15:26:15.537968: step 434, loss 0.759964, acc 0.78125\n",
      "2018-04-11T15:26:16.163968: step 435, loss 0.794182, acc 0.71875\n",
      "2018-04-11T15:26:16.723935: step 436, loss 0.75438, acc 0.765625\n",
      "2018-04-11T15:26:17.273219: step 437, loss 1.03238, acc 0.625\n",
      "2018-04-11T15:26:17.842800: step 438, loss 0.74259, acc 0.796875\n",
      "2018-04-11T15:26:18.405348: step 439, loss 0.676555, acc 0.8125\n",
      "2018-04-11T15:26:18.949806: step 440, loss 0.896188, acc 0.609375\n",
      "2018-04-11T15:26:19.512717: step 441, loss 0.911744, acc 0.59375\n",
      "2018-04-11T15:26:20.066302: step 442, loss 0.704448, acc 0.75\n",
      "2018-04-11T15:26:20.629909: step 443, loss 0.810903, acc 0.75\n",
      "2018-04-11T15:26:21.217002: step 444, loss 0.83325, acc 0.78125\n",
      "2018-04-11T15:26:21.806433: step 445, loss 1.00643, acc 0.640625\n",
      "2018-04-11T15:26:22.377794: step 446, loss 0.610541, acc 0.796875\n",
      "2018-04-11T15:26:23.026841: step 447, loss 0.635146, acc 0.765625\n",
      "2018-04-11T15:26:23.711981: step 448, loss 0.730343, acc 0.796875\n",
      "2018-04-11T15:26:24.384427: step 449, loss 0.74795, acc 0.75\n",
      "2018-04-11T15:26:24.989149: step 450, loss 0.803274, acc 0.71875\n",
      "2018-04-11T15:26:25.659879: step 451, loss 0.725032, acc 0.75\n",
      "2018-04-11T15:26:26.284833: step 452, loss 0.792618, acc 0.78125\n",
      "2018-04-11T15:26:26.899820: step 453, loss 0.853101, acc 0.765625\n",
      "2018-04-11T15:26:27.526647: step 454, loss 0.923756, acc 0.671875\n",
      "2018-04-11T15:26:28.131264: step 455, loss 0.698803, acc 0.765625\n",
      "2018-04-11T15:26:28.730439: step 456, loss 0.645501, acc 0.78125\n",
      "2018-04-11T15:26:29.335190: step 457, loss 1.09744, acc 0.625\n",
      "2018-04-11T15:26:29.898195: step 458, loss 0.756162, acc 0.703125\n",
      "2018-04-11T15:26:30.475295: step 459, loss 0.687802, acc 0.765625\n",
      "2018-04-11T15:26:31.041885: step 460, loss 0.882425, acc 0.6875\n",
      "2018-04-11T15:26:31.630610: step 461, loss 0.825122, acc 0.671875\n",
      "2018-04-11T15:26:32.229990: step 462, loss 0.901424, acc 0.65625\n",
      "2018-04-11T15:26:32.869916: step 463, loss 0.795452, acc 0.71875\n",
      "2018-04-11T15:26:33.559457: step 464, loss 0.741809, acc 0.78125\n",
      "2018-04-11T15:26:34.206966: step 465, loss 0.681899, acc 0.796875\n",
      "2018-04-11T15:26:34.898534: step 466, loss 0.814286, acc 0.703125\n",
      "2018-04-11T15:26:35.574391: step 467, loss 0.749782, acc 0.765625\n",
      "2018-04-11T15:26:36.194777: step 468, loss 0.645016, acc 0.796875\n",
      "2018-04-11T15:26:36.816386: step 469, loss 0.808876, acc 0.734375\n",
      "2018-04-11T15:26:37.416255: step 470, loss 0.713215, acc 0.78125\n",
      "2018-04-11T15:26:38.023466: step 471, loss 0.777246, acc 0.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:26:38.607050: step 472, loss 1.00358, acc 0.625\n",
      "2018-04-11T15:26:39.213641: step 473, loss 1.0732, acc 0.546875\n",
      "2018-04-11T15:26:39.819255: step 474, loss 0.863418, acc 0.734375\n",
      "2018-04-11T15:26:40.393887: step 475, loss 0.686836, acc 0.734375\n",
      "2018-04-11T15:26:40.987257: step 476, loss 0.95332, acc 0.703125\n",
      "2018-04-11T15:26:41.610320: step 477, loss 0.821025, acc 0.71875\n",
      "2018-04-11T15:26:42.219667: step 478, loss 0.923979, acc 0.625\n",
      "2018-04-11T15:26:42.855926: step 479, loss 0.842804, acc 0.71875\n",
      "2018-04-11T15:26:43.495435: step 480, loss 0.837562, acc 0.6875\n",
      "2018-04-11T15:26:44.107424: step 481, loss 0.968202, acc 0.671875\n",
      "2018-04-11T15:26:44.733297: step 482, loss 0.754273, acc 0.734375\n",
      "2018-04-11T15:26:45.334227: step 483, loss 0.756101, acc 0.75\n",
      "2018-04-11T15:26:45.953043: step 484, loss 0.69846, acc 0.734375\n",
      "2018-04-11T15:26:46.613656: step 485, loss 0.774158, acc 0.734375\n",
      "2018-04-11T15:26:47.260877: step 486, loss 0.729893, acc 0.765625\n",
      "2018-04-11T15:26:47.866385: step 487, loss 0.595084, acc 0.8125\n",
      "2018-04-11T15:26:48.431867: step 488, loss 0.798084, acc 0.703125\n",
      "2018-04-11T15:26:49.059231: step 489, loss 0.76246, acc 0.734375\n",
      "2018-04-11T15:26:49.692516: step 490, loss 0.945381, acc 0.65625\n",
      "2018-04-11T15:26:50.301675: step 491, loss 0.767946, acc 0.765625\n",
      "2018-04-11T15:26:50.910548: step 492, loss 0.603507, acc 0.78125\n",
      "2018-04-11T15:26:51.520323: step 493, loss 0.744388, acc 0.75\n",
      "2018-04-11T15:26:52.115875: step 494, loss 0.71869, acc 0.796875\n",
      "2018-04-11T15:26:52.728352: step 495, loss 0.756468, acc 0.71875\n",
      "2018-04-11T15:26:53.338994: step 496, loss 0.868829, acc 0.703125\n",
      "2018-04-11T15:26:53.955821: step 497, loss 0.909813, acc 0.671875\n",
      "2018-04-11T15:26:54.553244: step 498, loss 0.871238, acc 0.640625\n",
      "2018-04-11T15:26:55.143391: step 499, loss 0.719038, acc 0.75\n",
      "2018-04-11T15:26:55.746264: step 500, loss 0.582469, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:26:55.932936: step 500, loss 0.880434, acc 0.72\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-500\n",
      "\n",
      "2018-04-11T15:26:56.636958: step 501, loss 0.792181, acc 0.734375\n",
      "2018-04-11T15:26:57.201054: step 502, loss 0.643098, acc 0.828125\n",
      "2018-04-11T15:26:57.779434: step 503, loss 0.526941, acc 0.84375\n",
      "2018-04-11T15:26:58.354139: step 504, loss 0.597721, acc 0.796875\n",
      "2018-04-11T15:26:58.927099: step 505, loss 0.638476, acc 0.765625\n",
      "2018-04-11T15:26:59.529803: step 506, loss 0.654101, acc 0.765625\n",
      "2018-04-11T15:27:00.104155: step 507, loss 0.61037, acc 0.78125\n",
      "2018-04-11T15:27:00.670839: step 508, loss 0.676953, acc 0.78125\n",
      "2018-04-11T15:27:01.218506: step 509, loss 1.03942, acc 0.671875\n",
      "2018-04-11T15:27:01.800821: step 510, loss 0.804277, acc 0.75\n",
      "2018-04-11T15:27:02.374812: step 511, loss 0.893013, acc 0.78125\n",
      "2018-04-11T15:27:02.946917: step 512, loss 0.779127, acc 0.71875\n",
      "2018-04-11T15:27:03.182378: step 513, loss 0.725905, acc 0.75\n",
      "2018-04-11T15:27:03.749251: step 514, loss 0.649161, acc 0.765625\n",
      "2018-04-11T15:27:04.331765: step 515, loss 0.73383, acc 0.6875\n",
      "2018-04-11T15:27:04.889578: step 516, loss 0.815792, acc 0.671875\n",
      "2018-04-11T15:27:05.459009: step 517, loss 0.610899, acc 0.796875\n",
      "2018-04-11T15:27:06.067184: step 518, loss 0.777268, acc 0.65625\n",
      "2018-04-11T15:27:06.638214: step 519, loss 0.807613, acc 0.703125\n",
      "2018-04-11T15:27:07.249549: step 520, loss 0.673869, acc 0.765625\n",
      "2018-04-11T15:27:07.845766: step 521, loss 0.894919, acc 0.625\n",
      "2018-04-11T15:27:08.466292: step 522, loss 0.672963, acc 0.78125\n",
      "2018-04-11T15:27:09.043685: step 523, loss 0.600574, acc 0.78125\n",
      "2018-04-11T15:27:09.595855: step 524, loss 0.633764, acc 0.8125\n",
      "2018-04-11T15:27:10.159137: step 525, loss 0.690525, acc 0.75\n",
      "2018-04-11T15:27:10.740340: step 526, loss 0.712593, acc 0.75\n",
      "2018-04-11T15:27:11.303049: step 527, loss 0.625775, acc 0.78125\n",
      "2018-04-11T15:27:11.855002: step 528, loss 0.793542, acc 0.6875\n",
      "2018-04-11T15:27:12.410179: step 529, loss 0.716445, acc 0.765625\n",
      "2018-04-11T15:27:12.980893: step 530, loss 0.680599, acc 0.75\n",
      "2018-04-11T15:27:13.552751: step 531, loss 0.760932, acc 0.6875\n",
      "2018-04-11T15:27:14.146929: step 532, loss 0.900329, acc 0.671875\n",
      "2018-04-11T15:27:14.716877: step 533, loss 0.554072, acc 0.84375\n",
      "2018-04-11T15:27:15.264162: step 534, loss 0.620994, acc 0.828125\n",
      "2018-04-11T15:27:15.808039: step 535, loss 0.542736, acc 0.84375\n",
      "2018-04-11T15:27:16.376198: step 536, loss 0.522949, acc 0.78125\n",
      "2018-04-11T15:27:16.937046: step 537, loss 0.770261, acc 0.671875\n",
      "2018-04-11T15:27:17.482093: step 538, loss 0.697012, acc 0.75\n",
      "2018-04-11T15:27:18.049199: step 539, loss 0.755009, acc 0.671875\n",
      "2018-04-11T15:27:18.620560: step 540, loss 0.728169, acc 0.703125\n",
      "2018-04-11T15:27:19.170927: step 541, loss 0.485868, acc 0.796875\n",
      "2018-04-11T15:27:19.735314: step 542, loss 0.695135, acc 0.734375\n",
      "2018-04-11T15:27:20.285031: step 543, loss 0.550996, acc 0.78125\n",
      "2018-04-11T15:27:20.836777: step 544, loss 0.765495, acc 0.75\n",
      "2018-04-11T15:27:21.393026: step 545, loss 0.527738, acc 0.78125\n",
      "2018-04-11T15:27:21.986632: step 546, loss 0.536406, acc 0.828125\n",
      "2018-04-11T15:27:22.596356: step 547, loss 0.62821, acc 0.765625\n",
      "2018-04-11T15:27:23.207524: step 548, loss 0.789542, acc 0.71875\n",
      "2018-04-11T15:27:23.824267: step 549, loss 0.662841, acc 0.765625\n",
      "2018-04-11T15:27:24.412179: step 550, loss 0.826035, acc 0.71875\n",
      "2018-04-11T15:27:25.024699: step 551, loss 0.637563, acc 0.765625\n",
      "2018-04-11T15:27:25.688570: step 552, loss 0.869588, acc 0.6875\n",
      "2018-04-11T15:27:26.375271: step 553, loss 0.749014, acc 0.75\n",
      "2018-04-11T15:27:27.007018: step 554, loss 0.596834, acc 0.84375\n",
      "2018-04-11T15:27:27.647904: step 555, loss 0.702917, acc 0.71875\n",
      "2018-04-11T15:27:28.261733: step 556, loss 0.62248, acc 0.796875\n",
      "2018-04-11T15:27:28.864936: step 557, loss 0.724514, acc 0.78125\n",
      "2018-04-11T15:27:29.469866: step 558, loss 0.72133, acc 0.78125\n",
      "2018-04-11T15:27:30.143625: step 559, loss 0.633155, acc 0.75\n",
      "2018-04-11T15:27:30.716938: step 560, loss 0.658537, acc 0.765625\n",
      "2018-04-11T15:27:31.350504: step 561, loss 0.647283, acc 0.828125\n",
      "2018-04-11T15:27:31.919898: step 562, loss 0.476438, acc 0.890625\n",
      "2018-04-11T15:27:32.526551: step 563, loss 0.748332, acc 0.71875\n",
      "2018-04-11T15:27:33.101847: step 564, loss 0.738828, acc 0.71875\n",
      "2018-04-11T15:27:33.702249: step 565, loss 0.618644, acc 0.796875\n",
      "2018-04-11T15:27:34.287184: step 566, loss 0.664953, acc 0.75\n",
      "2018-04-11T15:27:34.883458: step 567, loss 0.62076, acc 0.796875\n",
      "2018-04-11T15:27:35.479121: step 568, loss 0.815124, acc 0.65625\n",
      "2018-04-11T15:27:36.118295: step 569, loss 0.650283, acc 0.734375\n",
      "2018-04-11T15:27:36.708983: step 570, loss 0.707258, acc 0.703125\n",
      "2018-04-11T15:27:37.313204: step 571, loss 0.745539, acc 0.734375\n",
      "2018-04-11T15:27:37.942747: step 572, loss 0.713545, acc 0.78125\n",
      "2018-04-11T15:27:38.518380: step 573, loss 0.627039, acc 0.765625\n",
      "2018-04-11T15:27:39.163203: step 574, loss 0.703707, acc 0.734375\n",
      "2018-04-11T15:27:39.775167: step 575, loss 0.750871, acc 0.71875\n",
      "2018-04-11T15:27:40.438269: step 576, loss 0.552217, acc 0.8125\n",
      "2018-04-11T15:27:41.128075: step 577, loss 0.708311, acc 0.75\n",
      "2018-04-11T15:27:41.882340: step 578, loss 0.76349, acc 0.6875\n",
      "2018-04-11T15:27:42.543393: step 579, loss 0.636477, acc 0.828125\n",
      "2018-04-11T15:27:43.140832: step 580, loss 0.655366, acc 0.75\n",
      "2018-04-11T15:27:43.831562: step 581, loss 0.61686, acc 0.796875\n",
      "2018-04-11T15:27:44.446984: step 582, loss 0.718524, acc 0.6875\n",
      "2018-04-11T15:27:45.052306: step 583, loss 0.739597, acc 0.765625\n",
      "2018-04-11T15:27:45.626533: step 584, loss 0.643761, acc 0.765625\n",
      "2018-04-11T15:27:46.200803: step 585, loss 0.857206, acc 0.71875\n",
      "2018-04-11T15:27:46.843140: step 586, loss 0.477404, acc 0.84375\n",
      "2018-04-11T15:27:47.536752: step 587, loss 0.626841, acc 0.765625\n",
      "2018-04-11T15:27:48.197529: step 588, loss 0.700869, acc 0.765625\n",
      "2018-04-11T15:27:48.920619: step 589, loss 0.651504, acc 0.796875\n",
      "2018-04-11T15:27:49.562942: step 590, loss 0.785546, acc 0.6875\n",
      "2018-04-11T15:27:50.259038: step 591, loss 0.736073, acc 0.703125\n",
      "2018-04-11T15:27:50.949891: step 592, loss 0.574418, acc 0.71875\n",
      "2018-04-11T15:27:51.595308: step 593, loss 0.924274, acc 0.6875\n",
      "2018-04-11T15:27:52.244317: step 594, loss 0.825593, acc 0.6875\n",
      "2018-04-11T15:27:52.833371: step 595, loss 0.44999, acc 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:27:53.434881: step 596, loss 0.66269, acc 0.78125\n",
      "2018-04-11T15:27:54.021882: step 597, loss 0.686378, acc 0.765625\n",
      "2018-04-11T15:27:54.608504: step 598, loss 0.679467, acc 0.78125\n",
      "2018-04-11T15:27:55.205575: step 599, loss 0.693538, acc 0.734375\n",
      "2018-04-11T15:27:55.773130: step 600, loss 0.694987, acc 0.671875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:27:55.951902: step 600, loss 0.748582, acc 0.74\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-600\n",
      "\n",
      "2018-04-11T15:27:56.685540: step 601, loss 0.848516, acc 0.671875\n",
      "2018-04-11T15:27:57.299956: step 602, loss 0.580109, acc 0.796875\n",
      "2018-04-11T15:27:57.872689: step 603, loss 0.560976, acc 0.796875\n",
      "2018-04-11T15:27:58.468174: step 604, loss 0.697063, acc 0.765625\n",
      "2018-04-11T15:27:59.043772: step 605, loss 0.56572, acc 0.8125\n",
      "2018-04-11T15:27:59.616117: step 606, loss 0.539416, acc 0.8125\n",
      "2018-04-11T15:28:00.201042: step 607, loss 0.486797, acc 0.828125\n",
      "2018-04-11T15:28:00.776018: step 608, loss 0.53906, acc 0.84375\n",
      "2018-04-11T15:28:01.354557: step 609, loss 0.575661, acc 0.78125\n",
      "2018-04-11T15:28:01.909433: step 610, loss 0.590875, acc 0.8125\n",
      "2018-04-11T15:28:02.506099: step 611, loss 0.552731, acc 0.828125\n",
      "2018-04-11T15:28:03.081284: step 612, loss 0.624765, acc 0.796875\n",
      "2018-04-11T15:28:03.643985: step 613, loss 0.590683, acc 0.828125\n",
      "2018-04-11T15:28:04.207491: step 614, loss 0.603871, acc 0.796875\n",
      "2018-04-11T15:28:04.766019: step 615, loss 0.543492, acc 0.8125\n",
      "2018-04-11T15:28:05.316744: step 616, loss 0.661255, acc 0.8125\n",
      "2018-04-11T15:28:05.902366: step 617, loss 0.629883, acc 0.84375\n",
      "2018-04-11T15:28:06.445634: step 618, loss 0.634503, acc 0.796875\n",
      "2018-04-11T15:28:07.008515: step 619, loss 0.817731, acc 0.671875\n",
      "2018-04-11T15:28:07.570930: step 620, loss 0.635376, acc 0.8125\n",
      "2018-04-11T15:28:08.110627: step 621, loss 0.571363, acc 0.828125\n",
      "2018-04-11T15:28:08.656427: step 622, loss 0.60022, acc 0.765625\n",
      "2018-04-11T15:28:09.208480: step 623, loss 0.517314, acc 0.828125\n",
      "2018-04-11T15:28:09.758855: step 624, loss 0.616339, acc 0.8125\n",
      "2018-04-11T15:28:10.307363: step 625, loss 0.548713, acc 0.828125\n",
      "2018-04-11T15:28:10.876885: step 626, loss 0.733308, acc 0.71875\n",
      "2018-04-11T15:28:11.451386: step 627, loss 0.579382, acc 0.78125\n",
      "2018-04-11T15:28:12.001737: step 628, loss 0.50002, acc 0.828125\n",
      "2018-04-11T15:28:12.546506: step 629, loss 0.520032, acc 0.84375\n",
      "2018-04-11T15:28:13.104992: step 630, loss 0.858752, acc 0.671875\n",
      "2018-04-11T15:28:13.655783: step 631, loss 0.583115, acc 0.796875\n",
      "2018-04-11T15:28:14.211076: step 632, loss 0.715038, acc 0.703125\n",
      "2018-04-11T15:28:14.778003: step 633, loss 0.502346, acc 0.859375\n",
      "2018-04-11T15:28:15.315820: step 634, loss 0.647267, acc 0.75\n",
      "2018-04-11T15:28:15.860358: step 635, loss 0.693725, acc 0.765625\n",
      "2018-04-11T15:28:16.402428: step 636, loss 0.79021, acc 0.765625\n",
      "2018-04-11T15:28:16.953760: step 637, loss 0.667302, acc 0.796875\n",
      "2018-04-11T15:28:17.510356: step 638, loss 0.662654, acc 0.796875\n",
      "2018-04-11T15:28:18.084294: step 639, loss 0.637212, acc 0.765625\n",
      "2018-04-11T15:28:18.627944: step 640, loss 0.58804, acc 0.828125\n",
      "2018-04-11T15:28:19.173166: step 641, loss 0.806387, acc 0.734375\n",
      "2018-04-11T15:28:19.729686: step 642, loss 0.757228, acc 0.765625\n",
      "2018-04-11T15:28:20.294219: step 643, loss 0.608666, acc 0.8125\n",
      "2018-04-11T15:28:20.843076: step 644, loss 0.68854, acc 0.734375\n",
      "2018-04-11T15:28:21.398493: step 645, loss 0.427893, acc 0.953125\n",
      "2018-04-11T15:28:21.956472: step 646, loss 0.530211, acc 0.78125\n",
      "2018-04-11T15:28:22.522638: step 647, loss 0.578196, acc 0.828125\n",
      "2018-04-11T15:28:23.075165: step 648, loss 0.584356, acc 0.8125\n",
      "2018-04-11T15:28:23.635127: step 649, loss 0.481107, acc 0.859375\n",
      "2018-04-11T15:28:24.170011: step 650, loss 0.516825, acc 0.890625\n",
      "2018-04-11T15:28:24.721553: step 651, loss 0.577862, acc 0.84375\n",
      "2018-04-11T15:28:25.260165: step 652, loss 0.656676, acc 0.8125\n",
      "2018-04-11T15:28:25.821143: step 653, loss 0.66341, acc 0.765625\n",
      "2018-04-11T15:28:26.376900: step 654, loss 0.622039, acc 0.828125\n",
      "2018-04-11T15:28:26.915153: step 655, loss 0.834881, acc 0.734375\n",
      "2018-04-11T15:28:27.468873: step 656, loss 0.574875, acc 0.796875\n",
      "2018-04-11T15:28:28.007268: step 657, loss 0.740805, acc 0.71875\n",
      "2018-04-11T15:28:28.567369: step 658, loss 0.600608, acc 0.78125\n",
      "2018-04-11T15:28:29.126021: step 659, loss 0.645686, acc 0.828125\n",
      "2018-04-11T15:28:29.672926: step 660, loss 0.660036, acc 0.75\n",
      "2018-04-11T15:28:30.224636: step 661, loss 0.391824, acc 0.890625\n",
      "2018-04-11T15:28:30.768399: step 662, loss 0.667668, acc 0.75\n",
      "2018-04-11T15:28:31.326477: step 663, loss 0.670241, acc 0.734375\n",
      "2018-04-11T15:28:31.880571: step 664, loss 0.645446, acc 0.796875\n",
      "2018-04-11T15:28:32.428488: step 665, loss 0.630778, acc 0.75\n",
      "2018-04-11T15:28:32.981320: step 666, loss 0.604496, acc 0.8125\n",
      "2018-04-11T15:28:33.535893: step 667, loss 0.919188, acc 0.6875\n",
      "2018-04-11T15:28:34.098337: step 668, loss 0.65425, acc 0.765625\n",
      "2018-04-11T15:28:34.646438: step 669, loss 0.504739, acc 0.84375\n",
      "2018-04-11T15:28:35.174895: step 670, loss 0.615886, acc 0.765625\n",
      "2018-04-11T15:28:35.736534: step 671, loss 0.608238, acc 0.84375\n",
      "2018-04-11T15:28:36.299279: step 672, loss 0.586888, acc 0.8125\n",
      "2018-04-11T15:28:36.840938: step 673, loss 0.777301, acc 0.765625\n",
      "2018-04-11T15:28:37.388213: step 674, loss 0.697319, acc 0.78125\n",
      "2018-04-11T15:28:37.930892: step 675, loss 0.729757, acc 0.71875\n",
      "2018-04-11T15:28:38.477152: step 676, loss 0.814661, acc 0.703125\n",
      "2018-04-11T15:28:39.026737: step 677, loss 0.528946, acc 0.8125\n",
      "2018-04-11T15:28:39.601255: step 678, loss 0.536231, acc 0.84375\n",
      "2018-04-11T15:28:40.151931: step 679, loss 0.627073, acc 0.796875\n",
      "2018-04-11T15:28:40.705628: step 680, loss 0.572723, acc 0.859375\n",
      "2018-04-11T15:28:41.275241: step 681, loss 0.619315, acc 0.734375\n",
      "2018-04-11T15:28:41.806738: step 682, loss 0.571267, acc 0.78125\n",
      "2018-04-11T15:28:42.354977: step 683, loss 0.673469, acc 0.78125\n",
      "2018-04-11T15:28:42.891500: step 684, loss 0.731551, acc 0.796875\n",
      "2018-04-11T15:28:43.465419: step 685, loss 0.891843, acc 0.609375\n",
      "2018-04-11T15:28:44.004842: step 686, loss 0.792396, acc 0.71875\n",
      "2018-04-11T15:28:44.555679: step 687, loss 0.624008, acc 0.890625\n",
      "2018-04-11T15:28:45.138613: step 688, loss 0.649908, acc 0.75\n",
      "2018-04-11T15:28:45.686692: step 689, loss 0.612277, acc 0.75\n",
      "2018-04-11T15:28:46.247080: step 690, loss 0.556108, acc 0.8125\n",
      "2018-04-11T15:28:46.805627: step 691, loss 0.665033, acc 0.765625\n",
      "2018-04-11T15:28:47.356320: step 692, loss 0.614633, acc 0.75\n",
      "2018-04-11T15:28:47.901842: step 693, loss 0.618619, acc 0.71875\n",
      "2018-04-11T15:28:48.463415: step 694, loss 0.756617, acc 0.765625\n",
      "2018-04-11T15:28:49.009901: step 695, loss 0.626609, acc 0.8125\n",
      "2018-04-11T15:28:49.551181: step 696, loss 0.833864, acc 0.6875\n",
      "2018-04-11T15:28:50.085018: step 697, loss 0.432438, acc 0.890625\n",
      "2018-04-11T15:28:50.638771: step 698, loss 0.858403, acc 0.671875\n",
      "2018-04-11T15:28:51.170234: step 699, loss 0.582504, acc 0.765625\n",
      "2018-04-11T15:28:51.735965: step 700, loss 0.735726, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:28:51.904781: step 700, loss 0.638386, acc 0.75\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-700\n",
      "\n",
      "2018-04-11T15:28:52.592661: step 701, loss 0.549657, acc 0.828125\n",
      "2018-04-11T15:28:53.117482: step 702, loss 0.733187, acc 0.796875\n",
      "2018-04-11T15:28:53.679928: step 703, loss 0.709493, acc 0.75\n",
      "2018-04-11T15:28:54.229933: step 704, loss 0.66301, acc 0.71875\n",
      "2018-04-11T15:28:54.767423: step 705, loss 0.686788, acc 0.8125\n",
      "2018-04-11T15:28:55.312402: step 706, loss 0.665948, acc 0.765625\n",
      "2018-04-11T15:28:55.856072: step 707, loss 0.67073, acc 0.71875\n",
      "2018-04-11T15:28:56.396090: step 708, loss 0.650109, acc 0.765625\n",
      "2018-04-11T15:28:56.944027: step 709, loss 0.470383, acc 0.796875\n",
      "2018-04-11T15:28:57.493211: step 710, loss 0.54546, acc 0.796875\n",
      "2018-04-11T15:28:58.025149: step 711, loss 0.698417, acc 0.734375\n",
      "2018-04-11T15:28:58.577225: step 712, loss 0.585804, acc 0.75\n",
      "2018-04-11T15:28:59.120662: step 713, loss 0.552482, acc 0.828125\n",
      "2018-04-11T15:28:59.659258: step 714, loss 0.872547, acc 0.671875\n",
      "2018-04-11T15:29:00.212472: step 715, loss 0.530523, acc 0.8125\n",
      "2018-04-11T15:29:00.757837: step 716, loss 0.683713, acc 0.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:29:01.315828: step 717, loss 0.688887, acc 0.75\n",
      "2018-04-11T15:29:01.855241: step 718, loss 0.74586, acc 0.78125\n",
      "2018-04-11T15:29:02.403063: step 719, loss 0.60818, acc 0.78125\n",
      "2018-04-11T15:29:02.951298: step 720, loss 0.692736, acc 0.71875\n",
      "2018-04-11T15:29:03.481457: step 721, loss 0.594864, acc 0.84375\n",
      "2018-04-11T15:29:04.024822: step 722, loss 0.530456, acc 0.84375\n",
      "2018-04-11T15:29:04.563372: step 723, loss 0.711704, acc 0.71875\n",
      "2018-04-11T15:29:05.111131: step 724, loss 0.727721, acc 0.71875\n",
      "2018-04-11T15:29:05.662999: step 725, loss 0.548711, acc 0.78125\n",
      "2018-04-11T15:29:06.191180: step 726, loss 0.569989, acc 0.765625\n",
      "2018-04-11T15:29:06.745614: step 727, loss 0.800414, acc 0.71875\n",
      "2018-04-11T15:29:07.281714: step 728, loss 0.753188, acc 0.78125\n",
      "2018-04-11T15:29:07.817994: step 729, loss 0.615199, acc 0.734375\n",
      "2018-04-11T15:29:08.395617: step 730, loss 0.593966, acc 0.78125\n",
      "2018-04-11T15:29:08.945889: step 731, loss 0.59694, acc 0.78125\n",
      "2018-04-11T15:29:09.497436: step 732, loss 0.587039, acc 0.75\n",
      "2018-04-11T15:29:10.041904: step 733, loss 0.600131, acc 0.765625\n",
      "2018-04-11T15:29:10.635955: step 734, loss 0.656224, acc 0.75\n",
      "2018-04-11T15:29:11.166587: step 735, loss 0.5624, acc 0.78125\n",
      "2018-04-11T15:29:11.695131: step 736, loss 0.512218, acc 0.84375\n",
      "2018-04-11T15:29:12.233832: step 737, loss 0.497785, acc 0.828125\n",
      "2018-04-11T15:29:12.766064: step 738, loss 0.699221, acc 0.796875\n",
      "2018-04-11T15:29:13.299246: step 739, loss 0.645996, acc 0.796875\n",
      "2018-04-11T15:29:13.849054: step 740, loss 0.530635, acc 0.828125\n",
      "2018-04-11T15:29:14.400148: step 741, loss 0.541504, acc 0.84375\n",
      "2018-04-11T15:29:14.931002: step 742, loss 0.614623, acc 0.765625\n",
      "2018-04-11T15:29:15.477346: step 743, loss 0.575092, acc 0.8125\n",
      "2018-04-11T15:29:16.029098: step 744, loss 0.628888, acc 0.78125\n",
      "2018-04-11T15:29:16.568692: step 745, loss 0.678852, acc 0.796875\n",
      "2018-04-11T15:29:17.111293: step 746, loss 0.833693, acc 0.65625\n",
      "2018-04-11T15:29:17.660668: step 747, loss 0.53156, acc 0.8125\n",
      "2018-04-11T15:29:18.223789: step 748, loss 0.803146, acc 0.640625\n",
      "2018-04-11T15:29:18.775484: step 749, loss 0.577411, acc 0.78125\n",
      "2018-04-11T15:29:19.305979: step 750, loss 0.759557, acc 0.703125\n",
      "2018-04-11T15:29:19.865588: step 751, loss 0.628833, acc 0.75\n",
      "2018-04-11T15:29:20.416399: step 752, loss 0.660855, acc 0.765625\n",
      "2018-04-11T15:29:20.952023: step 753, loss 0.710448, acc 0.734375\n",
      "2018-04-11T15:29:21.511135: step 754, loss 0.679316, acc 0.75\n",
      "2018-04-11T15:29:22.067941: step 755, loss 0.583369, acc 0.8125\n",
      "2018-04-11T15:29:22.612925: step 756, loss 0.668147, acc 0.765625\n",
      "2018-04-11T15:29:23.160126: step 757, loss 0.528166, acc 0.8125\n",
      "2018-04-11T15:29:23.688135: step 758, loss 0.499597, acc 0.84375\n",
      "2018-04-11T15:29:24.222428: step 759, loss 0.531074, acc 0.84375\n",
      "2018-04-11T15:29:24.793331: step 760, loss 0.573175, acc 0.8125\n",
      "2018-04-11T15:29:25.451576: step 761, loss 0.545179, acc 0.8125\n",
      "2018-04-11T15:29:26.102654: step 762, loss 0.796375, acc 0.75\n",
      "2018-04-11T15:29:26.755133: step 763, loss 0.629894, acc 0.8125\n",
      "2018-04-11T15:29:27.344195: step 764, loss 0.715157, acc 0.75\n",
      "2018-04-11T15:29:27.955008: step 765, loss 0.534014, acc 0.84375\n",
      "2018-04-11T15:29:28.532685: step 766, loss 0.786641, acc 0.6875\n",
      "2018-04-11T15:29:29.095948: step 767, loss 0.407425, acc 0.859375\n",
      "2018-04-11T15:29:29.710203: step 768, loss 0.67409, acc 0.8125\n",
      "2018-04-11T15:29:30.401944: step 769, loss 0.668021, acc 0.796875\n",
      "2018-04-11T15:29:30.941518: step 770, loss 0.627176, acc 0.78125\n",
      "2018-04-11T15:29:31.565584: step 771, loss 0.577613, acc 0.84375\n",
      "2018-04-11T15:29:32.172978: step 772, loss 0.686822, acc 0.765625\n",
      "2018-04-11T15:29:32.836546: step 773, loss 0.564856, acc 0.8125\n",
      "2018-04-11T15:29:33.525017: step 774, loss 0.593432, acc 0.78125\n",
      "2018-04-11T15:29:34.102395: step 775, loss 0.618023, acc 0.796875\n",
      "2018-04-11T15:29:34.716153: step 776, loss 0.445042, acc 0.828125\n",
      "2018-04-11T15:29:35.335801: step 777, loss 0.715853, acc 0.75\n",
      "2018-04-11T15:29:35.922095: step 778, loss 0.482054, acc 0.84375\n",
      "2018-04-11T15:29:36.535137: step 779, loss 0.544589, acc 0.765625\n",
      "2018-04-11T15:29:37.117593: step 780, loss 0.738012, acc 0.71875\n",
      "2018-04-11T15:29:37.697296: step 781, loss 0.562565, acc 0.765625\n",
      "2018-04-11T15:29:38.289634: step 782, loss 0.593608, acc 0.8125\n",
      "2018-04-11T15:29:38.959930: step 783, loss 0.686272, acc 0.734375\n",
      "2018-04-11T15:29:39.566077: step 784, loss 0.588331, acc 0.78125\n",
      "2018-04-11T15:29:40.149387: step 785, loss 0.573753, acc 0.765625\n",
      "2018-04-11T15:29:40.775500: step 786, loss 0.586852, acc 0.78125\n",
      "2018-04-11T15:29:41.364101: step 787, loss 0.726851, acc 0.71875\n",
      "2018-04-11T15:29:41.952691: step 788, loss 0.513408, acc 0.796875\n",
      "2018-04-11T15:29:42.511968: step 789, loss 0.575522, acc 0.765625\n",
      "2018-04-11T15:29:43.069655: step 790, loss 0.561521, acc 0.859375\n",
      "2018-04-11T15:29:43.689201: step 791, loss 0.542639, acc 0.8125\n",
      "2018-04-11T15:29:44.273037: step 792, loss 0.634302, acc 0.78125\n",
      "2018-04-11T15:29:44.838028: step 793, loss 0.641036, acc 0.78125\n",
      "2018-04-11T15:29:45.416661: step 794, loss 0.372765, acc 0.90625\n",
      "2018-04-11T15:29:45.973163: step 795, loss 0.686525, acc 0.734375\n",
      "2018-04-11T15:29:46.533402: step 796, loss 0.517475, acc 0.78125\n",
      "2018-04-11T15:29:47.124127: step 797, loss 0.553592, acc 0.796875\n",
      "2018-04-11T15:29:47.715232: step 798, loss 0.500567, acc 0.84375\n",
      "2018-04-11T15:29:48.294775: step 799, loss 0.485483, acc 0.8125\n",
      "2018-04-11T15:29:48.869847: step 800, loss 0.730096, acc 0.734375\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:29:49.049634: step 800, loss 0.555713, acc 0.8\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-800\n",
      "\n",
      "2018-04-11T15:29:49.765577: step 801, loss 0.595947, acc 0.828125\n",
      "2018-04-11T15:29:50.320512: step 802, loss 0.542138, acc 0.8125\n",
      "2018-04-11T15:29:50.887642: step 803, loss 0.660238, acc 0.765625\n",
      "2018-04-11T15:29:51.478547: step 804, loss 0.448206, acc 0.890625\n",
      "2018-04-11T15:29:52.107871: step 805, loss 0.474742, acc 0.8125\n",
      "2018-04-11T15:29:52.697426: step 806, loss 0.587017, acc 0.84375\n",
      "2018-04-11T15:29:53.315458: step 807, loss 0.497127, acc 0.8125\n",
      "2018-04-11T15:29:53.932085: step 808, loss 0.403436, acc 0.859375\n",
      "2018-04-11T15:29:54.510036: step 809, loss 0.7988, acc 0.75\n",
      "2018-04-11T15:29:55.100009: step 810, loss 0.62698, acc 0.75\n",
      "2018-04-11T15:29:55.720448: step 811, loss 0.652024, acc 0.796875\n",
      "2018-04-11T15:29:56.323056: step 812, loss 0.586017, acc 0.75\n",
      "2018-04-11T15:29:56.893055: step 813, loss 0.548509, acc 0.84375\n",
      "2018-04-11T15:29:57.494498: step 814, loss 0.745758, acc 0.734375\n",
      "2018-04-11T15:29:58.066305: step 815, loss 0.461237, acc 0.890625\n",
      "2018-04-11T15:29:58.626801: step 816, loss 0.443626, acc 0.84375\n",
      "2018-04-11T15:29:59.189832: step 817, loss 0.816967, acc 0.640625\n",
      "2018-04-11T15:29:59.779026: step 818, loss 0.57236, acc 0.8125\n",
      "2018-04-11T15:30:00.374248: step 819, loss 0.736931, acc 0.78125\n",
      "2018-04-11T15:30:00.944123: step 820, loss 0.756025, acc 0.75\n",
      "2018-04-11T15:30:01.496363: step 821, loss 0.619588, acc 0.796875\n",
      "2018-04-11T15:30:02.071412: step 822, loss 0.808664, acc 0.6875\n",
      "2018-04-11T15:30:02.641462: step 823, loss 0.764949, acc 0.71875\n",
      "2018-04-11T15:30:03.222084: step 824, loss 0.675109, acc 0.75\n",
      "2018-04-11T15:30:03.796801: step 825, loss 0.482439, acc 0.828125\n",
      "2018-04-11T15:30:04.385231: step 826, loss 0.731521, acc 0.71875\n",
      "2018-04-11T15:30:04.951454: step 827, loss 0.540818, acc 0.8125\n",
      "2018-04-11T15:30:05.524033: step 828, loss 0.624699, acc 0.78125\n",
      "2018-04-11T15:30:06.097369: step 829, loss 0.606544, acc 0.78125\n",
      "2018-04-11T15:30:06.679124: step 830, loss 0.665742, acc 0.8125\n",
      "2018-04-11T15:30:07.261271: step 831, loss 0.827148, acc 0.734375\n",
      "2018-04-11T15:30:07.820285: step 832, loss 0.467552, acc 0.875\n",
      "2018-04-11T15:30:08.374204: step 833, loss 0.653605, acc 0.78125\n",
      "2018-04-11T15:30:08.950239: step 834, loss 0.465846, acc 0.859375\n",
      "2018-04-11T15:30:09.495618: step 835, loss 0.446882, acc 0.859375\n",
      "2018-04-11T15:30:10.042085: step 836, loss 0.529267, acc 0.84375\n",
      "2018-04-11T15:30:10.614642: step 837, loss 0.58727, acc 0.8125\n",
      "2018-04-11T15:30:11.188633: step 838, loss 0.550265, acc 0.796875\n",
      "2018-04-11T15:30:11.762724: step 839, loss 0.58937, acc 0.765625\n",
      "2018-04-11T15:30:12.323110: step 840, loss 0.683911, acc 0.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:30:12.895949: step 841, loss 0.442888, acc 0.84375\n",
      "2018-04-11T15:30:13.452678: step 842, loss 0.654502, acc 0.796875\n",
      "2018-04-11T15:30:14.022856: step 843, loss 0.556035, acc 0.8125\n",
      "2018-04-11T15:30:14.573455: step 844, loss 0.466021, acc 0.859375\n",
      "2018-04-11T15:30:15.123507: step 845, loss 0.453372, acc 0.828125\n",
      "2018-04-11T15:30:15.670627: step 846, loss 0.661128, acc 0.796875\n",
      "2018-04-11T15:30:16.224806: step 847, loss 0.679195, acc 0.75\n",
      "2018-04-11T15:30:16.787414: step 848, loss 0.73517, acc 0.71875\n",
      "2018-04-11T15:30:17.342838: step 849, loss 0.543537, acc 0.78125\n",
      "2018-04-11T15:30:17.882364: step 850, loss 0.58575, acc 0.765625\n",
      "2018-04-11T15:30:18.432266: step 851, loss 0.632807, acc 0.84375\n",
      "2018-04-11T15:30:18.986215: step 852, loss 0.611132, acc 0.75\n",
      "2018-04-11T15:30:19.529422: step 853, loss 0.7499, acc 0.6875\n",
      "2018-04-11T15:30:20.095237: step 854, loss 0.592719, acc 0.796875\n",
      "2018-04-11T15:30:20.640598: step 855, loss 0.456262, acc 0.859375\n",
      "2018-04-11T15:30:21.184360: step 856, loss 0.437159, acc 0.859375\n",
      "2018-04-11T15:30:21.730253: step 857, loss 0.862515, acc 0.71875\n",
      "2018-04-11T15:30:22.295282: step 858, loss 0.602651, acc 0.796875\n",
      "2018-04-11T15:30:22.832762: step 859, loss 0.384484, acc 0.890625\n",
      "2018-04-11T15:30:23.377856: step 860, loss 0.556305, acc 0.84375\n",
      "2018-04-11T15:30:23.918639: step 861, loss 0.529453, acc 0.828125\n",
      "2018-04-11T15:30:24.462660: step 862, loss 0.522485, acc 0.796875\n",
      "2018-04-11T15:30:25.004631: step 863, loss 0.776637, acc 0.75\n",
      "2018-04-11T15:30:25.568545: step 864, loss 0.778052, acc 0.703125\n",
      "2018-04-11T15:30:26.113981: step 865, loss 0.653048, acc 0.6875\n",
      "2018-04-11T15:30:26.677098: step 866, loss 0.594222, acc 0.8125\n",
      "2018-04-11T15:30:27.223454: step 867, loss 0.677479, acc 0.75\n",
      "2018-04-11T15:30:27.758986: step 868, loss 0.449298, acc 0.84375\n",
      "2018-04-11T15:30:28.316766: step 869, loss 0.512114, acc 0.828125\n",
      "2018-04-11T15:30:28.862360: step 870, loss 0.571977, acc 0.84375\n",
      "2018-04-11T15:30:29.404021: step 871, loss 0.361461, acc 0.90625\n",
      "2018-04-11T15:30:29.932431: step 872, loss 0.451115, acc 0.859375\n",
      "2018-04-11T15:30:30.486449: step 873, loss 0.792069, acc 0.734375\n",
      "2018-04-11T15:30:31.024278: step 874, loss 0.586408, acc 0.78125\n",
      "2018-04-11T15:30:31.580579: step 875, loss 0.788819, acc 0.703125\n",
      "2018-04-11T15:30:32.140039: step 876, loss 0.646582, acc 0.734375\n",
      "2018-04-11T15:30:32.700797: step 877, loss 0.616244, acc 0.78125\n",
      "2018-04-11T15:30:33.255547: step 878, loss 0.591738, acc 0.78125\n",
      "2018-04-11T15:30:33.824921: step 879, loss 0.602815, acc 0.765625\n",
      "2018-04-11T15:30:34.378704: step 880, loss 0.491158, acc 0.859375\n",
      "2018-04-11T15:30:34.922228: step 881, loss 0.503412, acc 0.828125\n",
      "2018-04-11T15:30:35.458238: step 882, loss 0.629635, acc 0.765625\n",
      "2018-04-11T15:30:35.997343: step 883, loss 0.494184, acc 0.796875\n",
      "2018-04-11T15:30:36.540495: step 884, loss 0.436853, acc 0.84375\n",
      "2018-04-11T15:30:37.088742: step 885, loss 0.574174, acc 0.765625\n",
      "2018-04-11T15:30:37.634402: step 886, loss 0.705168, acc 0.765625\n",
      "2018-04-11T15:30:38.173696: step 887, loss 0.697014, acc 0.703125\n",
      "2018-04-11T15:30:38.727994: step 888, loss 0.701418, acc 0.765625\n",
      "2018-04-11T15:30:39.280733: step 889, loss 0.626516, acc 0.8125\n",
      "2018-04-11T15:30:39.840265: step 890, loss 0.562998, acc 0.765625\n",
      "2018-04-11T15:30:40.389688: step 891, loss 0.759753, acc 0.71875\n",
      "2018-04-11T15:30:40.934744: step 892, loss 0.644806, acc 0.75\n",
      "2018-04-11T15:30:41.466012: step 893, loss 0.686021, acc 0.796875\n",
      "2018-04-11T15:30:42.006413: step 894, loss 0.557075, acc 0.78125\n",
      "2018-04-11T15:30:42.543497: step 895, loss 0.608228, acc 0.78125\n",
      "2018-04-11T15:30:43.102853: step 896, loss 0.636489, acc 0.8125\n",
      "2018-04-11T15:30:43.657600: step 897, loss 0.70194, acc 0.78125\n",
      "2018-04-11T15:30:44.200822: step 898, loss 0.53079, acc 0.765625\n",
      "2018-04-11T15:30:44.739538: step 899, loss 0.586184, acc 0.859375\n",
      "2018-04-11T15:30:45.310181: step 900, loss 0.588806, acc 0.796875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:30:45.480453: step 900, loss 0.505203, acc 0.85\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-900\n",
      "\n",
      "2018-04-11T15:30:46.157556: step 901, loss 0.545523, acc 0.796875\n",
      "2018-04-11T15:30:46.686728: step 902, loss 0.659448, acc 0.6875\n",
      "2018-04-11T15:30:47.235049: step 903, loss 0.621616, acc 0.765625\n",
      "2018-04-11T15:30:47.779794: step 904, loss 0.681723, acc 0.78125\n",
      "2018-04-11T15:30:48.322878: step 905, loss 0.455397, acc 0.828125\n",
      "2018-04-11T15:30:48.872536: step 906, loss 0.702507, acc 0.765625\n",
      "2018-04-11T15:30:49.419301: step 907, loss 0.556228, acc 0.78125\n",
      "2018-04-11T15:30:49.958846: step 908, loss 0.602696, acc 0.84375\n",
      "2018-04-11T15:30:50.503270: step 909, loss 0.401837, acc 0.859375\n",
      "2018-04-11T15:30:51.034211: step 910, loss 0.472638, acc 0.875\n",
      "2018-04-11T15:30:51.573331: step 911, loss 0.531051, acc 0.828125\n",
      "2018-04-11T15:30:52.110660: step 912, loss 0.539419, acc 0.765625\n",
      "2018-04-11T15:30:52.640587: step 913, loss 0.964543, acc 0.59375\n",
      "2018-04-11T15:30:53.182505: step 914, loss 0.666076, acc 0.734375\n",
      "2018-04-11T15:30:53.713083: step 915, loss 0.612841, acc 0.765625\n",
      "2018-04-11T15:30:54.260648: step 916, loss 0.701732, acc 0.734375\n",
      "2018-04-11T15:30:54.807385: step 917, loss 0.536765, acc 0.859375\n",
      "2018-04-11T15:30:55.362202: step 918, loss 0.668427, acc 0.765625\n",
      "2018-04-11T15:30:55.902394: step 919, loss 0.534458, acc 0.84375\n",
      "2018-04-11T15:30:56.431970: step 920, loss 0.645742, acc 0.796875\n",
      "2018-04-11T15:30:56.965410: step 921, loss 0.706915, acc 0.734375\n",
      "2018-04-11T15:30:57.500176: step 922, loss 0.537124, acc 0.8125\n",
      "2018-04-11T15:30:58.023249: step 923, loss 0.680846, acc 0.75\n",
      "2018-04-11T15:30:58.552871: step 924, loss 0.573653, acc 0.671875\n",
      "2018-04-11T15:30:59.103605: step 925, loss 0.570562, acc 0.859375\n",
      "2018-04-11T15:30:59.646836: step 926, loss 0.84965, acc 0.75\n",
      "2018-04-11T15:31:00.185794: step 927, loss 0.46316, acc 0.875\n",
      "2018-04-11T15:31:00.739979: step 928, loss 0.487044, acc 0.875\n",
      "2018-04-11T15:31:01.269940: step 929, loss 0.518294, acc 0.78125\n",
      "2018-04-11T15:31:01.819542: step 930, loss 0.681455, acc 0.71875\n",
      "2018-04-11T15:31:02.369367: step 931, loss 0.630276, acc 0.78125\n",
      "2018-04-11T15:31:02.939981: step 932, loss 0.749817, acc 0.671875\n",
      "2018-04-11T15:31:03.493620: step 933, loss 0.560451, acc 0.796875\n",
      "2018-04-11T15:31:04.019719: step 934, loss 0.507576, acc 0.796875\n",
      "2018-04-11T15:31:04.583937: step 935, loss 0.715389, acc 0.71875\n",
      "2018-04-11T15:31:05.134248: step 936, loss 0.707962, acc 0.75\n",
      "2018-04-11T15:31:05.682409: step 937, loss 0.543915, acc 0.8125\n",
      "2018-04-11T15:31:06.229590: step 938, loss 0.506162, acc 0.84375\n",
      "2018-04-11T15:31:06.757194: step 939, loss 0.652949, acc 0.765625\n",
      "2018-04-11T15:31:07.287865: step 940, loss 0.637784, acc 0.75\n",
      "2018-04-11T15:31:07.832265: step 941, loss 0.486609, acc 0.828125\n",
      "2018-04-11T15:31:08.404840: step 942, loss 0.482481, acc 0.8125\n",
      "2018-04-11T15:31:08.963746: step 943, loss 0.626111, acc 0.75\n",
      "2018-04-11T15:31:09.515828: step 944, loss 0.535151, acc 0.859375\n",
      "2018-04-11T15:31:10.061514: step 945, loss 0.522887, acc 0.828125\n",
      "2018-04-11T15:31:10.641366: step 946, loss 0.48178, acc 0.8125\n",
      "2018-04-11T15:31:11.205241: step 947, loss 0.700242, acc 0.765625\n",
      "2018-04-11T15:31:11.738338: step 948, loss 0.524214, acc 0.8125\n",
      "2018-04-11T15:31:12.294559: step 949, loss 0.616114, acc 0.734375\n",
      "2018-04-11T15:31:12.840369: step 950, loss 0.507858, acc 0.8125\n",
      "2018-04-11T15:31:13.389042: step 951, loss 0.498208, acc 0.8125\n",
      "2018-04-11T15:31:13.922301: step 952, loss 0.76997, acc 0.75\n",
      "2018-04-11T15:31:14.465154: step 953, loss 0.642084, acc 0.765625\n",
      "2018-04-11T15:31:14.995428: step 954, loss 0.459644, acc 0.828125\n",
      "2018-04-11T15:31:15.520938: step 955, loss 0.678982, acc 0.71875\n",
      "2018-04-11T15:31:16.077063: step 956, loss 0.678219, acc 0.796875\n",
      "2018-04-11T15:31:16.610640: step 957, loss 0.687177, acc 0.75\n",
      "2018-04-11T15:31:17.153870: step 958, loss 0.737888, acc 0.734375\n",
      "2018-04-11T15:31:17.698170: step 959, loss 0.578559, acc 0.796875\n",
      "2018-04-11T15:31:18.303833: step 960, loss 0.660727, acc 0.75\n",
      "2018-04-11T15:31:18.921398: step 961, loss 0.72148, acc 0.734375\n",
      "2018-04-11T15:31:19.462076: step 962, loss 0.521809, acc 0.796875\n",
      "2018-04-11T15:31:19.990475: step 963, loss 0.59234, acc 0.78125\n",
      "2018-04-11T15:31:20.542652: step 964, loss 0.674331, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:31:21.105244: step 965, loss 0.599828, acc 0.78125\n",
      "2018-04-11T15:31:21.651558: step 966, loss 0.43776, acc 0.84375\n",
      "2018-04-11T15:31:22.176517: step 967, loss 0.618488, acc 0.78125\n",
      "2018-04-11T15:31:22.733563: step 968, loss 0.727793, acc 0.703125\n",
      "2018-04-11T15:31:23.304146: step 969, loss 0.440804, acc 0.859375\n",
      "2018-04-11T15:31:23.864120: step 970, loss 0.712115, acc 0.78125\n",
      "2018-04-11T15:31:24.416978: step 971, loss 0.672859, acc 0.78125\n",
      "2018-04-11T15:31:24.960526: step 972, loss 0.696268, acc 0.734375\n",
      "2018-04-11T15:31:25.510517: step 973, loss 0.619685, acc 0.78125\n",
      "2018-04-11T15:31:26.076575: step 974, loss 0.55163, acc 0.765625\n",
      "2018-04-11T15:31:26.642039: step 975, loss 0.730549, acc 0.71875\n",
      "2018-04-11T15:31:27.184851: step 976, loss 0.537685, acc 0.8125\n",
      "2018-04-11T15:31:27.746567: step 977, loss 0.599856, acc 0.765625\n",
      "2018-04-11T15:31:28.304074: step 978, loss 0.690414, acc 0.765625\n",
      "2018-04-11T15:31:28.850714: step 979, loss 0.646651, acc 0.796875\n",
      "2018-04-11T15:31:29.397360: step 980, loss 0.357934, acc 0.90625\n",
      "2018-04-11T15:31:29.941507: step 981, loss 0.517067, acc 0.859375\n",
      "2018-04-11T15:31:30.475701: step 982, loss 0.430386, acc 0.875\n",
      "2018-04-11T15:31:31.032689: step 983, loss 0.427233, acc 0.859375\n",
      "2018-04-11T15:31:31.603433: step 984, loss 0.663016, acc 0.75\n",
      "2018-04-11T15:31:32.151590: step 985, loss 0.617262, acc 0.796875\n",
      "2018-04-11T15:31:32.701787: step 986, loss 0.540627, acc 0.828125\n",
      "2018-04-11T15:31:33.256044: step 987, loss 0.585554, acc 0.78125\n",
      "2018-04-11T15:31:33.814140: step 988, loss 0.509583, acc 0.828125\n",
      "2018-04-11T15:31:34.368672: step 989, loss 0.67878, acc 0.78125\n",
      "2018-04-11T15:31:34.939451: step 990, loss 0.535356, acc 0.8125\n",
      "2018-04-11T15:31:35.496929: step 991, loss 0.461021, acc 0.84375\n",
      "2018-04-11T15:31:36.063865: step 992, loss 0.660323, acc 0.734375\n",
      "2018-04-11T15:31:36.636456: step 993, loss 0.640893, acc 0.75\n",
      "2018-04-11T15:31:37.248221: step 994, loss 0.503432, acc 0.828125\n",
      "2018-04-11T15:31:37.844544: step 995, loss 0.505718, acc 0.765625\n",
      "2018-04-11T15:31:38.395816: step 996, loss 0.774202, acc 0.71875\n",
      "2018-04-11T15:31:38.952776: step 997, loss 0.522623, acc 0.875\n",
      "2018-04-11T15:31:39.518067: step 998, loss 0.513168, acc 0.84375\n",
      "2018-04-11T15:31:40.100691: step 999, loss 0.590868, acc 0.78125\n",
      "2018-04-11T15:31:40.675099: step 1000, loss 0.426618, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:31:40.849499: step 1000, loss 0.586182, acc 0.82\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-1000\n",
      "\n",
      "2018-04-11T15:31:41.553220: step 1001, loss 0.71745, acc 0.765625\n",
      "2018-04-11T15:31:42.157892: step 1002, loss 0.634695, acc 0.796875\n",
      "2018-04-11T15:31:42.840494: step 1003, loss 0.654048, acc 0.765625\n",
      "2018-04-11T15:31:43.436481: step 1004, loss 0.490083, acc 0.84375\n",
      "2018-04-11T15:31:44.015824: step 1005, loss 0.655637, acc 0.75\n",
      "2018-04-11T15:31:44.566784: step 1006, loss 0.605075, acc 0.75\n",
      "2018-04-11T15:31:45.157404: step 1007, loss 0.678488, acc 0.765625\n",
      "2018-04-11T15:31:45.730722: step 1008, loss 0.797189, acc 0.71875\n",
      "2018-04-11T15:31:46.315341: step 1009, loss 0.540582, acc 0.796875\n",
      "2018-04-11T15:31:46.878599: step 1010, loss 0.623833, acc 0.75\n",
      "2018-04-11T15:31:47.456492: step 1011, loss 0.55839, acc 0.765625\n",
      "2018-04-11T15:31:48.023025: step 1012, loss 0.552756, acc 0.78125\n",
      "2018-04-11T15:31:48.618448: step 1013, loss 0.637533, acc 0.734375\n",
      "2018-04-11T15:31:49.234133: step 1014, loss 0.45003, acc 0.859375\n",
      "2018-04-11T15:31:49.816603: step 1015, loss 0.478703, acc 0.8125\n",
      "2018-04-11T15:31:50.383882: step 1016, loss 0.636486, acc 0.765625\n",
      "2018-04-11T15:31:50.966725: step 1017, loss 0.767707, acc 0.71875\n",
      "2018-04-11T15:31:51.595777: step 1018, loss 0.396088, acc 0.875\n",
      "2018-04-11T15:31:52.213289: step 1019, loss 0.701735, acc 0.78125\n",
      "2018-04-11T15:31:52.826826: step 1020, loss 0.443413, acc 0.84375\n",
      "2018-04-11T15:31:53.449157: step 1021, loss 0.643454, acc 0.75\n",
      "2018-04-11T15:31:54.065382: step 1022, loss 0.63093, acc 0.765625\n",
      "2018-04-11T15:31:54.697252: step 1023, loss 0.450754, acc 0.8125\n",
      "2018-04-11T15:31:55.293993: step 1024, loss 0.521787, acc 0.78125\n",
      "2018-04-11T15:31:55.918259: step 1025, loss 0.687276, acc 0.75\n",
      "2018-04-11T15:31:56.205094: step 1026, loss 0.48557, acc 0.75\n",
      "2018-04-11T15:31:56.816964: step 1027, loss 0.338558, acc 0.90625\n",
      "2018-04-11T15:31:57.412520: step 1028, loss 0.450622, acc 0.828125\n",
      "2018-04-11T15:31:57.994800: step 1029, loss 0.528582, acc 0.796875\n",
      "2018-04-11T15:31:58.590170: step 1030, loss 0.531442, acc 0.828125\n",
      "2018-04-11T15:31:59.180190: step 1031, loss 0.493387, acc 0.78125\n",
      "2018-04-11T15:31:59.755489: step 1032, loss 0.453899, acc 0.859375\n",
      "2018-04-11T15:32:00.351340: step 1033, loss 0.388055, acc 0.875\n",
      "2018-04-11T15:32:00.965338: step 1034, loss 0.634695, acc 0.765625\n",
      "2018-04-11T15:32:01.625549: step 1035, loss 0.377463, acc 0.890625\n",
      "2018-04-11T15:32:02.233780: step 1036, loss 0.447061, acc 0.875\n",
      "2018-04-11T15:32:02.834548: step 1037, loss 0.553188, acc 0.828125\n",
      "2018-04-11T15:32:03.460542: step 1038, loss 0.393483, acc 0.859375\n",
      "2018-04-11T15:32:04.054773: step 1039, loss 0.568392, acc 0.78125\n",
      "2018-04-11T15:32:04.652113: step 1040, loss 0.702057, acc 0.75\n",
      "2018-04-11T15:32:05.236794: step 1041, loss 0.470848, acc 0.875\n",
      "2018-04-11T15:32:05.790300: step 1042, loss 0.414598, acc 0.890625\n",
      "2018-04-11T15:32:06.364247: step 1043, loss 0.487375, acc 0.875\n",
      "2018-04-11T15:32:06.963752: step 1044, loss 0.474165, acc 0.859375\n",
      "2018-04-11T15:32:07.550512: step 1045, loss 0.66648, acc 0.78125\n",
      "2018-04-11T15:32:08.174677: step 1046, loss 0.493312, acc 0.84375\n",
      "2018-04-11T15:32:08.812405: step 1047, loss 0.557874, acc 0.8125\n",
      "2018-04-11T15:32:09.443578: step 1048, loss 0.604396, acc 0.78125\n",
      "2018-04-11T15:32:10.141905: step 1049, loss 0.47238, acc 0.828125\n",
      "2018-04-11T15:32:10.798316: step 1050, loss 0.504213, acc 0.796875\n",
      "2018-04-11T15:32:11.424862: step 1051, loss 0.500834, acc 0.84375\n",
      "2018-04-11T15:32:12.059141: step 1052, loss 0.507288, acc 0.875\n",
      "2018-04-11T15:32:12.747022: step 1053, loss 0.498771, acc 0.890625\n",
      "2018-04-11T15:32:13.409431: step 1054, loss 0.492943, acc 0.84375\n",
      "2018-04-11T15:32:14.110433: step 1055, loss 0.248709, acc 0.9375\n",
      "2018-04-11T15:32:14.758789: step 1056, loss 0.414162, acc 0.84375\n",
      "2018-04-11T15:32:15.423438: step 1057, loss 0.76031, acc 0.6875\n",
      "2018-04-11T15:32:16.051191: step 1058, loss 0.62277, acc 0.8125\n",
      "2018-04-11T15:32:16.786833: step 1059, loss 0.729376, acc 0.75\n",
      "2018-04-11T15:32:17.443996: step 1060, loss 0.491655, acc 0.8125\n",
      "2018-04-11T15:32:18.169265: step 1061, loss 0.400423, acc 0.890625\n",
      "2018-04-11T15:32:18.784492: step 1062, loss 0.544277, acc 0.84375\n",
      "2018-04-11T15:32:19.431357: step 1063, loss 0.651056, acc 0.734375\n",
      "2018-04-11T15:32:20.113911: step 1064, loss 0.505367, acc 0.796875\n",
      "2018-04-11T15:32:20.795723: step 1065, loss 0.68899, acc 0.765625\n",
      "2018-04-11T15:32:21.404146: step 1066, loss 0.607772, acc 0.8125\n",
      "2018-04-11T15:32:21.987892: step 1067, loss 0.338681, acc 0.890625\n",
      "2018-04-11T15:32:22.577599: step 1068, loss 0.551105, acc 0.78125\n",
      "2018-04-11T15:32:23.177129: step 1069, loss 0.467585, acc 0.859375\n",
      "2018-04-11T15:32:23.784423: step 1070, loss 0.442656, acc 0.828125\n",
      "2018-04-11T15:32:24.389689: step 1071, loss 0.46644, acc 0.8125\n",
      "2018-04-11T15:32:24.980058: step 1072, loss 0.353289, acc 0.859375\n",
      "2018-04-11T15:32:25.561262: step 1073, loss 0.401915, acc 0.890625\n",
      "2018-04-11T15:32:26.158930: step 1074, loss 0.480944, acc 0.796875\n",
      "2018-04-11T15:32:26.784289: step 1075, loss 0.393059, acc 0.875\n",
      "2018-04-11T15:32:27.386343: step 1076, loss 0.477521, acc 0.84375\n",
      "2018-04-11T15:32:27.955219: step 1077, loss 0.416216, acc 0.828125\n",
      "2018-04-11T15:32:28.525047: step 1078, loss 0.302373, acc 0.953125\n",
      "2018-04-11T15:32:29.106088: step 1079, loss 0.606743, acc 0.78125\n",
      "2018-04-11T15:32:29.761766: step 1080, loss 0.579187, acc 0.796875\n",
      "2018-04-11T15:32:30.467258: step 1081, loss 0.417729, acc 0.875\n",
      "2018-04-11T15:32:31.195505: step 1082, loss 0.380878, acc 0.890625\n",
      "2018-04-11T15:32:31.991506: step 1083, loss 0.604327, acc 0.78125\n",
      "2018-04-11T15:32:32.731959: step 1084, loss 0.493554, acc 0.796875\n",
      "2018-04-11T15:32:33.490787: step 1085, loss 0.475204, acc 0.8125\n",
      "2018-04-11T15:32:34.225109: step 1086, loss 0.36526, acc 0.90625\n",
      "2018-04-11T15:32:34.919365: step 1087, loss 0.604318, acc 0.765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:32:35.578004: step 1088, loss 0.369462, acc 0.890625\n",
      "2018-04-11T15:32:36.231902: step 1089, loss 0.583365, acc 0.78125\n",
      "2018-04-11T15:32:36.869974: step 1090, loss 0.263674, acc 0.9375\n",
      "2018-04-11T15:32:37.535574: step 1091, loss 0.446796, acc 0.78125\n",
      "2018-04-11T15:32:38.171755: step 1092, loss 0.474907, acc 0.828125\n",
      "2018-04-11T15:32:38.814006: step 1093, loss 0.464421, acc 0.828125\n",
      "2018-04-11T15:32:39.396329: step 1094, loss 0.458548, acc 0.859375\n",
      "2018-04-11T15:32:39.989134: step 1095, loss 0.470484, acc 0.859375\n",
      "2018-04-11T15:32:40.585031: step 1096, loss 0.426934, acc 0.84375\n",
      "2018-04-11T15:32:41.164876: step 1097, loss 0.466208, acc 0.875\n",
      "2018-04-11T15:32:41.726204: step 1098, loss 0.658738, acc 0.796875\n",
      "2018-04-11T15:32:42.308661: step 1099, loss 0.538005, acc 0.765625\n",
      "2018-04-11T15:32:42.902846: step 1100, loss 0.531419, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:32:43.085846: step 1100, loss 0.717038, acc 0.74\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-1100\n",
      "\n",
      "2018-04-11T15:32:43.802145: step 1101, loss 0.354984, acc 0.890625\n",
      "2018-04-11T15:32:44.388501: step 1102, loss 0.408136, acc 0.84375\n",
      "2018-04-11T15:32:44.988546: step 1103, loss 0.465863, acc 0.84375\n",
      "2018-04-11T15:32:45.578088: step 1104, loss 0.449246, acc 0.8125\n",
      "2018-04-11T15:32:46.166957: step 1105, loss 0.592809, acc 0.8125\n",
      "2018-04-11T15:32:46.819292: step 1106, loss 0.449191, acc 0.84375\n",
      "2018-04-11T15:32:47.496607: step 1107, loss 0.623249, acc 0.765625\n",
      "2018-04-11T15:32:48.242840: step 1108, loss 0.479286, acc 0.859375\n",
      "2018-04-11T15:32:48.965272: step 1109, loss 0.455987, acc 0.8125\n",
      "2018-04-11T15:32:49.722660: step 1110, loss 0.58016, acc 0.78125\n",
      "2018-04-11T15:32:50.451335: step 1111, loss 0.64383, acc 0.765625\n",
      "2018-04-11T15:32:51.186529: step 1112, loss 0.436971, acc 0.828125\n",
      "2018-04-11T15:32:51.800928: step 1113, loss 0.586481, acc 0.8125\n",
      "2018-04-11T15:32:52.432736: step 1114, loss 0.577406, acc 0.78125\n",
      "2018-04-11T15:32:53.109004: step 1115, loss 0.456587, acc 0.828125\n",
      "2018-04-11T15:32:53.797368: step 1116, loss 0.486204, acc 0.828125\n",
      "2018-04-11T15:32:54.432521: step 1117, loss 0.438848, acc 0.84375\n",
      "2018-04-11T15:32:55.092720: step 1118, loss 0.429281, acc 0.84375\n",
      "2018-04-11T15:32:55.740963: step 1119, loss 0.445979, acc 0.84375\n",
      "2018-04-11T15:32:56.405886: step 1120, loss 0.518628, acc 0.8125\n",
      "2018-04-11T15:32:57.093845: step 1121, loss 0.488104, acc 0.859375\n",
      "2018-04-11T15:32:57.767909: step 1122, loss 0.511365, acc 0.796875\n",
      "2018-04-11T15:32:58.445965: step 1123, loss 0.575244, acc 0.828125\n",
      "2018-04-11T15:32:59.103530: step 1124, loss 0.440146, acc 0.828125\n",
      "2018-04-11T15:32:59.712465: step 1125, loss 0.475553, acc 0.84375\n",
      "2018-04-11T15:33:00.345064: step 1126, loss 0.547059, acc 0.828125\n",
      "2018-04-11T15:33:00.944069: step 1127, loss 0.54756, acc 0.75\n",
      "2018-04-11T15:33:01.556757: step 1128, loss 0.290276, acc 0.921875\n",
      "2018-04-11T15:33:02.208196: step 1129, loss 0.361024, acc 0.875\n",
      "2018-04-11T15:33:02.887449: step 1130, loss 0.552796, acc 0.84375\n",
      "2018-04-11T15:33:03.542029: step 1131, loss 0.47985, acc 0.8125\n",
      "2018-04-11T15:33:04.186764: step 1132, loss 0.388148, acc 0.875\n",
      "2018-04-11T15:33:04.814451: step 1133, loss 0.418897, acc 0.828125\n",
      "2018-04-11T15:33:05.450161: step 1134, loss 0.542885, acc 0.8125\n",
      "2018-04-11T15:33:06.104579: step 1135, loss 0.396873, acc 0.859375\n",
      "2018-04-11T15:33:06.791655: step 1136, loss 0.465035, acc 0.828125\n",
      "2018-04-11T15:33:07.488008: step 1137, loss 0.58611, acc 0.765625\n",
      "2018-04-11T15:33:08.156049: step 1138, loss 0.441767, acc 0.859375\n",
      "2018-04-11T15:33:08.824025: step 1139, loss 0.662382, acc 0.78125\n",
      "2018-04-11T15:33:09.447371: step 1140, loss 0.534746, acc 0.84375\n",
      "2018-04-11T15:33:10.035880: step 1141, loss 0.295985, acc 0.9375\n",
      "2018-04-11T15:33:10.646201: step 1142, loss 0.597851, acc 0.734375\n",
      "2018-04-11T15:33:11.224442: step 1143, loss 0.499571, acc 0.84375\n",
      "2018-04-11T15:33:11.843594: step 1144, loss 0.395773, acc 0.875\n",
      "2018-04-11T15:33:12.453029: step 1145, loss 0.539882, acc 0.765625\n",
      "2018-04-11T15:33:13.046932: step 1146, loss 0.478272, acc 0.828125\n",
      "2018-04-11T15:33:13.687573: step 1147, loss 0.468238, acc 0.84375\n",
      "2018-04-11T15:33:14.331104: step 1148, loss 0.394338, acc 0.875\n",
      "2018-04-11T15:33:14.937322: step 1149, loss 0.514698, acc 0.828125\n",
      "2018-04-11T15:33:15.520864: step 1150, loss 0.315136, acc 0.890625\n",
      "2018-04-11T15:33:16.108503: step 1151, loss 0.625406, acc 0.765625\n",
      "2018-04-11T15:33:16.752470: step 1152, loss 0.444048, acc 0.828125\n",
      "2018-04-11T15:33:17.345904: step 1153, loss 0.386161, acc 0.90625\n",
      "2018-04-11T15:33:17.956317: step 1154, loss 0.49456, acc 0.796875\n",
      "2018-04-11T15:33:18.551174: step 1155, loss 0.679252, acc 0.71875\n",
      "2018-04-11T15:33:19.139294: step 1156, loss 0.797224, acc 0.609375\n",
      "2018-04-11T15:33:19.739282: step 1157, loss 0.359106, acc 0.890625\n",
      "2018-04-11T15:33:20.386428: step 1158, loss 0.347962, acc 0.90625\n",
      "2018-04-11T15:33:20.983200: step 1159, loss 0.553404, acc 0.796875\n",
      "2018-04-11T15:33:21.559435: step 1160, loss 0.512271, acc 0.8125\n",
      "2018-04-11T15:33:22.149754: step 1161, loss 0.471051, acc 0.875\n",
      "2018-04-11T15:33:22.730271: step 1162, loss 0.441492, acc 0.828125\n",
      "2018-04-11T15:33:23.318264: step 1163, loss 0.457387, acc 0.84375\n",
      "2018-04-11T15:33:23.890144: step 1164, loss 0.421199, acc 0.875\n",
      "2018-04-11T15:33:24.484929: step 1165, loss 0.488467, acc 0.828125\n",
      "2018-04-11T15:33:25.098366: step 1166, loss 0.493288, acc 0.78125\n",
      "2018-04-11T15:33:25.691010: step 1167, loss 0.557486, acc 0.828125\n",
      "2018-04-11T15:33:26.279306: step 1168, loss 0.410247, acc 0.859375\n",
      "2018-04-11T15:33:26.916554: step 1169, loss 0.380284, acc 0.890625\n",
      "2018-04-11T15:33:27.513642: step 1170, loss 0.387889, acc 0.875\n",
      "2018-04-11T15:33:28.127362: step 1171, loss 0.594797, acc 0.796875\n",
      "2018-04-11T15:33:28.722704: step 1172, loss 0.412225, acc 0.859375\n",
      "2018-04-11T15:33:29.305072: step 1173, loss 0.489343, acc 0.875\n",
      "2018-04-11T15:33:29.909097: step 1174, loss 0.448432, acc 0.8125\n",
      "2018-04-11T15:33:30.486427: step 1175, loss 0.484108, acc 0.859375\n",
      "2018-04-11T15:33:31.082022: step 1176, loss 0.450045, acc 0.859375\n",
      "2018-04-11T15:33:31.671625: step 1177, loss 0.447333, acc 0.84375\n",
      "2018-04-11T15:33:32.343796: step 1178, loss 0.38395, acc 0.875\n",
      "2018-04-11T15:33:32.953228: step 1179, loss 0.389993, acc 0.875\n",
      "2018-04-11T15:33:33.524503: step 1180, loss 0.409144, acc 0.90625\n",
      "2018-04-11T15:33:34.095744: step 1181, loss 0.49929, acc 0.875\n",
      "2018-04-11T15:33:34.688877: step 1182, loss 0.483325, acc 0.828125\n",
      "2018-04-11T15:33:35.267347: step 1183, loss 0.568275, acc 0.796875\n",
      "2018-04-11T15:33:35.855423: step 1184, loss 0.475463, acc 0.8125\n",
      "2018-04-11T15:33:36.454086: step 1185, loss 0.412283, acc 0.890625\n",
      "2018-04-11T15:33:37.029883: step 1186, loss 0.535024, acc 0.828125\n",
      "2018-04-11T15:33:37.605689: step 1187, loss 0.566628, acc 0.84375\n",
      "2018-04-11T15:33:38.186772: step 1188, loss 0.433317, acc 0.84375\n",
      "2018-04-11T15:33:38.778427: step 1189, loss 0.434261, acc 0.84375\n",
      "2018-04-11T15:33:39.368164: step 1190, loss 0.472343, acc 0.875\n",
      "2018-04-11T15:33:39.956768: step 1191, loss 0.476517, acc 0.859375\n",
      "2018-04-11T15:33:40.533190: step 1192, loss 0.459315, acc 0.796875\n",
      "2018-04-11T15:33:41.109064: step 1193, loss 0.352627, acc 0.859375\n",
      "2018-04-11T15:33:41.687702: step 1194, loss 0.431082, acc 0.859375\n",
      "2018-04-11T15:33:42.266920: step 1195, loss 0.309992, acc 0.921875\n",
      "2018-04-11T15:33:42.861258: step 1196, loss 0.442953, acc 0.859375\n",
      "2018-04-11T15:33:43.551930: step 1197, loss 0.515916, acc 0.796875\n",
      "2018-04-11T15:33:44.143330: step 1198, loss 0.585516, acc 0.78125\n",
      "2018-04-11T15:33:44.800306: step 1199, loss 0.454473, acc 0.84375\n",
      "2018-04-11T15:33:45.480313: step 1200, loss 0.443523, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:33:45.661760: step 1200, loss 0.742897, acc 0.76\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-1200\n",
      "\n",
      "2018-04-11T15:33:46.478568: step 1201, loss 0.400107, acc 0.84375\n",
      "2018-04-11T15:33:47.087072: step 1202, loss 0.422234, acc 0.84375\n",
      "2018-04-11T15:33:47.719534: step 1203, loss 0.417009, acc 0.828125\n",
      "2018-04-11T15:33:48.333597: step 1204, loss 0.51175, acc 0.796875\n",
      "2018-04-11T15:33:48.934080: step 1205, loss 0.51074, acc 0.796875\n",
      "2018-04-11T15:33:49.528749: step 1206, loss 0.403914, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:33:50.101491: step 1207, loss 0.31353, acc 0.953125\n",
      "2018-04-11T15:33:50.687153: step 1208, loss 0.501148, acc 0.84375\n",
      "2018-04-11T15:33:51.296390: step 1209, loss 0.574893, acc 0.8125\n",
      "2018-04-11T15:33:51.870838: step 1210, loss 0.520237, acc 0.796875\n",
      "2018-04-11T15:33:52.451550: step 1211, loss 0.437816, acc 0.8125\n",
      "2018-04-11T15:33:53.014044: step 1212, loss 0.289706, acc 0.90625\n",
      "2018-04-11T15:33:53.578515: step 1213, loss 0.430637, acc 0.84375\n",
      "2018-04-11T15:33:54.173268: step 1214, loss 0.413452, acc 0.84375\n",
      "2018-04-11T15:33:54.744519: step 1215, loss 0.492307, acc 0.828125\n",
      "2018-04-11T15:33:55.316850: step 1216, loss 0.717211, acc 0.75\n",
      "2018-04-11T15:33:55.888333: step 1217, loss 0.449466, acc 0.8125\n",
      "2018-04-11T15:33:56.457121: step 1218, loss 0.595684, acc 0.78125\n",
      "2018-04-11T15:33:57.090581: step 1219, loss 0.637371, acc 0.71875\n",
      "2018-04-11T15:33:57.688785: step 1220, loss 0.531901, acc 0.859375\n",
      "2018-04-11T15:33:58.274324: step 1221, loss 0.455337, acc 0.890625\n",
      "2018-04-11T15:33:58.875788: step 1222, loss 0.447368, acc 0.828125\n",
      "2018-04-11T15:33:59.497932: step 1223, loss 0.666623, acc 0.71875\n",
      "2018-04-11T15:34:00.137254: step 1224, loss 0.583642, acc 0.765625\n",
      "2018-04-11T15:34:00.732073: step 1225, loss 0.34435, acc 0.890625\n",
      "2018-04-11T15:34:01.321512: step 1226, loss 0.514273, acc 0.84375\n",
      "2018-04-11T15:34:01.914781: step 1227, loss 0.421591, acc 0.828125\n",
      "2018-04-11T15:34:02.532137: step 1228, loss 0.549541, acc 0.796875\n",
      "2018-04-11T15:34:03.155190: step 1229, loss 0.463221, acc 0.859375\n",
      "2018-04-11T15:34:03.786104: step 1230, loss 0.404315, acc 0.875\n",
      "2018-04-11T15:34:04.430770: step 1231, loss 0.454589, acc 0.890625\n",
      "2018-04-11T15:34:05.145186: step 1232, loss 0.549477, acc 0.796875\n",
      "2018-04-11T15:34:05.910203: step 1233, loss 0.664894, acc 0.859375\n",
      "2018-04-11T15:34:06.640747: step 1234, loss 0.466094, acc 0.8125\n",
      "2018-04-11T15:34:07.372015: step 1235, loss 0.495152, acc 0.828125\n",
      "2018-04-11T15:34:08.016679: step 1236, loss 0.396881, acc 0.84375\n",
      "2018-04-11T15:34:08.627732: step 1237, loss 0.776462, acc 0.703125\n",
      "2018-04-11T15:34:09.293077: step 1238, loss 0.420669, acc 0.8125\n",
      "2018-04-11T15:34:09.902939: step 1239, loss 0.583106, acc 0.734375\n",
      "2018-04-11T15:34:10.541055: step 1240, loss 0.371355, acc 0.875\n",
      "2018-04-11T15:34:11.150758: step 1241, loss 0.488796, acc 0.8125\n",
      "2018-04-11T15:34:11.784194: step 1242, loss 0.426388, acc 0.859375\n",
      "2018-04-11T15:34:12.378446: step 1243, loss 0.510017, acc 0.828125\n",
      "2018-04-11T15:34:12.964915: step 1244, loss 0.610556, acc 0.796875\n",
      "2018-04-11T15:34:13.565765: step 1245, loss 0.469082, acc 0.828125\n",
      "2018-04-11T15:34:14.216031: step 1246, loss 0.312409, acc 0.90625\n",
      "2018-04-11T15:34:14.797340: step 1247, loss 0.433045, acc 0.890625\n",
      "2018-04-11T15:34:15.446108: step 1248, loss 0.626323, acc 0.828125\n",
      "2018-04-11T15:34:16.037943: step 1249, loss 0.53406, acc 0.796875\n",
      "2018-04-11T15:34:16.618726: step 1250, loss 0.737037, acc 0.765625\n",
      "2018-04-11T15:34:17.214330: step 1251, loss 0.398433, acc 0.90625\n",
      "2018-04-11T15:34:17.785967: step 1252, loss 0.557933, acc 0.78125\n",
      "2018-04-11T15:34:18.383021: step 1253, loss 0.50723, acc 0.8125\n",
      "2018-04-11T15:34:18.971316: step 1254, loss 0.53088, acc 0.8125\n",
      "2018-04-11T15:34:19.600383: step 1255, loss 0.604573, acc 0.796875\n",
      "2018-04-11T15:34:20.197148: step 1256, loss 0.472085, acc 0.84375\n",
      "2018-04-11T15:34:20.785112: step 1257, loss 0.469901, acc 0.859375\n",
      "2018-04-11T15:34:21.426146: step 1258, loss 0.455062, acc 0.859375\n",
      "2018-04-11T15:34:22.032697: step 1259, loss 0.410569, acc 0.90625\n",
      "2018-04-11T15:34:22.654947: step 1260, loss 0.475269, acc 0.828125\n",
      "2018-04-11T15:34:23.261464: step 1261, loss 0.523682, acc 0.8125\n",
      "2018-04-11T15:34:23.869963: step 1262, loss 0.484926, acc 0.859375\n",
      "2018-04-11T15:34:24.538203: step 1263, loss 0.455453, acc 0.859375\n",
      "2018-04-11T15:34:25.168331: step 1264, loss 0.562577, acc 0.75\n",
      "2018-04-11T15:34:25.791037: step 1265, loss 0.65587, acc 0.828125\n",
      "2018-04-11T15:34:26.439528: step 1266, loss 0.584766, acc 0.796875\n",
      "2018-04-11T15:34:27.116016: step 1267, loss 0.485211, acc 0.828125\n",
      "2018-04-11T15:34:27.824381: step 1268, loss 0.428167, acc 0.84375\n",
      "2018-04-11T15:34:28.454352: step 1269, loss 0.539918, acc 0.84375\n",
      "2018-04-11T15:34:29.078765: step 1270, loss 0.43216, acc 0.84375\n",
      "2018-04-11T15:34:29.719092: step 1271, loss 0.222329, acc 0.96875\n",
      "2018-04-11T15:34:30.396663: step 1272, loss 0.645877, acc 0.796875\n",
      "2018-04-11T15:34:31.005658: step 1273, loss 0.420312, acc 0.875\n",
      "2018-04-11T15:34:31.624216: step 1274, loss 0.342326, acc 0.890625\n",
      "2018-04-11T15:34:32.213093: step 1275, loss 0.511581, acc 0.828125\n",
      "2018-04-11T15:34:32.793740: step 1276, loss 0.416676, acc 0.828125\n",
      "2018-04-11T15:34:33.377284: step 1277, loss 0.468371, acc 0.84375\n",
      "2018-04-11T15:34:34.017207: step 1278, loss 0.466547, acc 0.828125\n",
      "2018-04-11T15:34:34.646214: step 1279, loss 0.585861, acc 0.765625\n",
      "2018-04-11T15:34:35.338978: step 1280, loss 0.576123, acc 0.78125\n",
      "2018-04-11T15:34:36.004871: step 1281, loss 0.483389, acc 0.84375\n",
      "2018-04-11T15:34:36.659628: step 1282, loss 0.617199, acc 0.8125\n",
      "2018-04-11T15:34:37.282169: step 1283, loss 0.546589, acc 0.78125\n",
      "2018-04-11T15:34:37.977166: step 1284, loss 0.606131, acc 0.734375\n",
      "2018-04-11T15:34:38.683860: step 1285, loss 0.287527, acc 0.890625\n",
      "2018-04-11T15:34:39.321678: step 1286, loss 0.480483, acc 0.828125\n",
      "2018-04-11T15:34:39.998634: step 1287, loss 0.376638, acc 0.859375\n",
      "2018-04-11T15:34:40.721014: step 1288, loss 0.49173, acc 0.8125\n",
      "2018-04-11T15:34:41.329167: step 1289, loss 0.461264, acc 0.84375\n",
      "2018-04-11T15:34:41.982945: step 1290, loss 0.376416, acc 0.859375\n",
      "2018-04-11T15:34:42.616348: step 1291, loss 0.793681, acc 0.765625\n",
      "2018-04-11T15:34:43.226769: step 1292, loss 0.378622, acc 0.84375\n",
      "2018-04-11T15:34:43.856760: step 1293, loss 0.458171, acc 0.828125\n",
      "2018-04-11T15:34:44.576760: step 1294, loss 0.405517, acc 0.875\n",
      "2018-04-11T15:34:45.328546: step 1295, loss 0.396798, acc 0.859375\n",
      "2018-04-11T15:34:46.054420: step 1296, loss 0.608007, acc 0.796875\n",
      "2018-04-11T15:34:46.722417: step 1297, loss 0.440093, acc 0.859375\n",
      "2018-04-11T15:34:47.430811: step 1298, loss 0.38043, acc 0.875\n",
      "2018-04-11T15:34:48.105989: step 1299, loss 0.468192, acc 0.828125\n",
      "2018-04-11T15:34:48.903077: step 1300, loss 0.377359, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:34:49.264696: step 1300, loss 0.497685, acc 0.81\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-1300\n",
      "\n",
      "2018-04-11T15:34:50.167755: step 1301, loss 0.632324, acc 0.765625\n",
      "2018-04-11T15:34:50.886279: step 1302, loss 0.440275, acc 0.859375\n",
      "2018-04-11T15:34:51.588190: step 1303, loss 0.501744, acc 0.8125\n",
      "2018-04-11T15:34:52.289007: step 1304, loss 0.396011, acc 0.875\n",
      "2018-04-11T15:34:52.934603: step 1305, loss 0.592493, acc 0.828125\n",
      "2018-04-11T15:34:53.633956: step 1306, loss 0.434934, acc 0.84375\n",
      "2018-04-11T15:34:54.303154: step 1307, loss 0.483318, acc 0.859375\n",
      "2018-04-11T15:34:54.912219: step 1308, loss 0.538895, acc 0.828125\n",
      "2018-04-11T15:34:55.606091: step 1309, loss 0.424205, acc 0.859375\n",
      "2018-04-11T15:34:56.286579: step 1310, loss 0.400984, acc 0.859375\n",
      "2018-04-11T15:34:56.928565: step 1311, loss 0.463835, acc 0.796875\n",
      "2018-04-11T15:34:57.525840: step 1312, loss 0.456232, acc 0.8125\n",
      "2018-04-11T15:34:58.139304: step 1313, loss 0.341621, acc 0.859375\n",
      "2018-04-11T15:34:58.828896: step 1314, loss 0.524327, acc 0.78125\n",
      "2018-04-11T15:34:59.474862: step 1315, loss 0.472641, acc 0.84375\n",
      "2018-04-11T15:35:00.101358: step 1316, loss 0.426079, acc 0.875\n",
      "2018-04-11T15:35:00.716774: step 1317, loss 0.537472, acc 0.828125\n",
      "2018-04-11T15:35:01.310251: step 1318, loss 0.649526, acc 0.78125\n",
      "2018-04-11T15:35:01.916431: step 1319, loss 0.368586, acc 0.890625\n",
      "2018-04-11T15:35:02.541572: step 1320, loss 0.329482, acc 0.90625\n",
      "2018-04-11T15:35:03.173181: step 1321, loss 0.512948, acc 0.84375\n",
      "2018-04-11T15:35:03.814060: step 1322, loss 0.46396, acc 0.859375\n",
      "2018-04-11T15:35:04.403282: step 1323, loss 0.5114, acc 0.828125\n",
      "2018-04-11T15:35:04.993863: step 1324, loss 0.518456, acc 0.796875\n",
      "2018-04-11T15:35:05.607646: step 1325, loss 0.540082, acc 0.84375\n",
      "2018-04-11T15:35:06.232218: step 1326, loss 0.461148, acc 0.84375\n",
      "2018-04-11T15:35:06.884075: step 1327, loss 0.347028, acc 0.890625\n",
      "2018-04-11T15:35:07.590034: step 1328, loss 0.657489, acc 0.765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:35:08.300677: step 1329, loss 0.406073, acc 0.84375\n",
      "2018-04-11T15:35:08.941383: step 1330, loss 0.490022, acc 0.8125\n",
      "2018-04-11T15:35:09.661160: step 1331, loss 0.467919, acc 0.828125\n",
      "2018-04-11T15:35:10.281803: step 1332, loss 0.383727, acc 0.84375\n",
      "2018-04-11T15:35:10.899040: step 1333, loss 0.574248, acc 0.828125\n",
      "2018-04-11T15:35:11.625396: step 1334, loss 0.377188, acc 0.84375\n",
      "2018-04-11T15:35:12.267246: step 1335, loss 0.457526, acc 0.8125\n",
      "2018-04-11T15:35:12.853201: step 1336, loss 0.427367, acc 0.796875\n",
      "2018-04-11T15:35:13.466383: step 1337, loss 0.40804, acc 0.84375\n",
      "2018-04-11T15:35:14.088905: step 1338, loss 0.513499, acc 0.828125\n",
      "2018-04-11T15:35:14.709712: step 1339, loss 0.46678, acc 0.859375\n",
      "2018-04-11T15:35:15.342438: step 1340, loss 0.529371, acc 0.828125\n",
      "2018-04-11T15:35:15.953356: step 1341, loss 0.290162, acc 0.90625\n",
      "2018-04-11T15:35:16.626427: step 1342, loss 0.453046, acc 0.84375\n",
      "2018-04-11T15:35:17.227494: step 1343, loss 0.335869, acc 0.890625\n",
      "2018-04-11T15:35:17.830093: step 1344, loss 0.368986, acc 0.875\n",
      "2018-04-11T15:35:18.432370: step 1345, loss 0.361925, acc 0.859375\n",
      "2018-04-11T15:35:19.039050: step 1346, loss 0.42084, acc 0.859375\n",
      "2018-04-11T15:35:19.643528: step 1347, loss 0.605788, acc 0.796875\n",
      "2018-04-11T15:35:20.244158: step 1348, loss 0.410642, acc 0.90625\n",
      "2018-04-11T15:35:20.872963: step 1349, loss 0.5719, acc 0.8125\n",
      "2018-04-11T15:35:21.441807: step 1350, loss 0.494065, acc 0.765625\n",
      "2018-04-11T15:35:22.023433: step 1351, loss 0.421472, acc 0.84375\n",
      "2018-04-11T15:35:22.607572: step 1352, loss 0.601215, acc 0.796875\n",
      "2018-04-11T15:35:23.204515: step 1353, loss 0.514384, acc 0.796875\n",
      "2018-04-11T15:35:23.822214: step 1354, loss 0.624647, acc 0.8125\n",
      "2018-04-11T15:35:24.427052: step 1355, loss 0.581147, acc 0.796875\n",
      "2018-04-11T15:35:25.002322: step 1356, loss 0.620367, acc 0.765625\n",
      "2018-04-11T15:35:25.627071: step 1357, loss 0.646403, acc 0.75\n",
      "2018-04-11T15:35:26.260326: step 1358, loss 0.325787, acc 0.921875\n",
      "2018-04-11T15:35:26.980244: step 1359, loss 0.338599, acc 0.859375\n",
      "2018-04-11T15:35:27.687974: step 1360, loss 0.504856, acc 0.796875\n",
      "2018-04-11T15:35:28.335520: step 1361, loss 0.511569, acc 0.828125\n",
      "2018-04-11T15:35:29.024447: step 1362, loss 0.443783, acc 0.828125\n",
      "2018-04-11T15:35:29.648879: step 1363, loss 0.425092, acc 0.859375\n",
      "2018-04-11T15:35:30.273346: step 1364, loss 0.33207, acc 0.890625\n",
      "2018-04-11T15:35:30.933824: step 1365, loss 0.534404, acc 0.828125\n",
      "2018-04-11T15:35:31.600323: step 1366, loss 0.461585, acc 0.84375\n",
      "2018-04-11T15:35:32.253261: step 1367, loss 0.666173, acc 0.765625\n",
      "2018-04-11T15:35:32.887368: step 1368, loss 0.685871, acc 0.75\n",
      "2018-04-11T15:35:33.622468: step 1369, loss 0.55754, acc 0.796875\n",
      "2018-04-11T15:35:34.291746: step 1370, loss 0.491293, acc 0.859375\n",
      "2018-04-11T15:35:34.909980: step 1371, loss 0.710276, acc 0.703125\n",
      "2018-04-11T15:35:35.514548: step 1372, loss 0.400055, acc 0.875\n",
      "2018-04-11T15:35:36.101584: step 1373, loss 0.477412, acc 0.78125\n",
      "2018-04-11T15:35:36.676911: step 1374, loss 0.442071, acc 0.875\n",
      "2018-04-11T15:35:37.261189: step 1375, loss 0.62639, acc 0.75\n",
      "2018-04-11T15:35:37.875959: step 1376, loss 0.450637, acc 0.875\n",
      "2018-04-11T15:35:38.549721: step 1377, loss 0.554547, acc 0.796875\n",
      "2018-04-11T15:35:39.190393: step 1378, loss 0.583801, acc 0.78125\n",
      "2018-04-11T15:35:39.786322: step 1379, loss 0.585647, acc 0.828125\n",
      "2018-04-11T15:35:40.443429: step 1380, loss 0.486189, acc 0.859375\n",
      "2018-04-11T15:35:41.030858: step 1381, loss 0.496608, acc 0.796875\n",
      "2018-04-11T15:35:41.655532: step 1382, loss 0.39212, acc 0.828125\n",
      "2018-04-11T15:35:42.297146: step 1383, loss 0.324912, acc 0.9375\n",
      "2018-04-11T15:35:42.929544: step 1384, loss 0.380011, acc 0.859375\n",
      "2018-04-11T15:35:43.568785: step 1385, loss 0.541402, acc 0.765625\n",
      "2018-04-11T15:35:44.158125: step 1386, loss 0.394196, acc 0.8125\n",
      "2018-04-11T15:35:44.780429: step 1387, loss 0.460618, acc 0.84375\n",
      "2018-04-11T15:35:45.457647: step 1388, loss 0.504025, acc 0.859375\n",
      "2018-04-11T15:35:46.060967: step 1389, loss 0.619773, acc 0.765625\n",
      "2018-04-11T15:35:46.679097: step 1390, loss 0.377007, acc 0.859375\n",
      "2018-04-11T15:35:47.394486: step 1391, loss 0.599448, acc 0.78125\n",
      "2018-04-11T15:35:47.972855: step 1392, loss 0.582303, acc 0.765625\n",
      "2018-04-11T15:35:48.649090: step 1393, loss 0.595677, acc 0.8125\n",
      "2018-04-11T15:35:49.248887: step 1394, loss 0.511779, acc 0.8125\n",
      "2018-04-11T15:35:49.850102: step 1395, loss 0.452173, acc 0.890625\n",
      "2018-04-11T15:35:50.443485: step 1396, loss 0.624458, acc 0.78125\n",
      "2018-04-11T15:35:51.020413: step 1397, loss 0.435728, acc 0.84375\n",
      "2018-04-11T15:35:51.604320: step 1398, loss 0.320498, acc 0.90625\n",
      "2018-04-11T15:35:52.244847: step 1399, loss 0.484796, acc 0.796875\n",
      "2018-04-11T15:35:52.886357: step 1400, loss 0.483374, acc 0.796875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:35:53.074658: step 1400, loss 0.509401, acc 0.84\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-1400\n",
      "\n",
      "2018-04-11T15:35:53.881525: step 1401, loss 0.389399, acc 0.890625\n",
      "2018-04-11T15:35:54.478059: step 1402, loss 0.35691, acc 0.90625\n",
      "2018-04-11T15:35:55.070979: step 1403, loss 0.510967, acc 0.828125\n",
      "2018-04-11T15:35:55.662407: step 1404, loss 0.37257, acc 0.890625\n",
      "2018-04-11T15:35:56.273605: step 1405, loss 0.525107, acc 0.8125\n",
      "2018-04-11T15:35:56.949083: step 1406, loss 0.508728, acc 0.796875\n",
      "2018-04-11T15:35:57.570487: step 1407, loss 0.391101, acc 0.84375\n",
      "2018-04-11T15:35:58.169238: step 1408, loss 0.419758, acc 0.859375\n",
      "2018-04-11T15:35:58.732273: step 1409, loss 0.339827, acc 0.921875\n",
      "2018-04-11T15:35:59.315435: step 1410, loss 0.416057, acc 0.84375\n",
      "2018-04-11T15:35:59.889056: step 1411, loss 0.420547, acc 0.90625\n",
      "2018-04-11T15:36:00.614903: step 1412, loss 0.348766, acc 0.859375\n",
      "2018-04-11T15:36:01.278490: step 1413, loss 0.373885, acc 0.859375\n",
      "2018-04-11T15:36:01.878360: step 1414, loss 0.536668, acc 0.8125\n",
      "2018-04-11T15:36:02.528500: step 1415, loss 0.433554, acc 0.859375\n",
      "2018-04-11T15:36:03.137544: step 1416, loss 0.552448, acc 0.78125\n",
      "2018-04-11T15:36:03.810139: step 1417, loss 0.26307, acc 0.921875\n",
      "2018-04-11T15:36:04.589629: step 1418, loss 0.454107, acc 0.859375\n",
      "2018-04-11T15:36:05.289110: step 1419, loss 0.465049, acc 0.8125\n",
      "2018-04-11T15:36:06.009956: step 1420, loss 0.390521, acc 0.859375\n",
      "2018-04-11T15:36:06.683164: step 1421, loss 0.623922, acc 0.796875\n",
      "2018-04-11T15:36:07.377344: step 1422, loss 0.696171, acc 0.796875\n",
      "2018-04-11T15:36:07.987810: step 1423, loss 0.695074, acc 0.765625\n",
      "2018-04-11T15:36:08.713028: step 1424, loss 0.501758, acc 0.84375\n",
      "2018-04-11T15:36:09.319749: step 1425, loss 0.433698, acc 0.84375\n",
      "2018-04-11T15:36:09.930182: step 1426, loss 0.633335, acc 0.75\n",
      "2018-04-11T15:36:10.666181: step 1427, loss 0.496139, acc 0.78125\n",
      "2018-04-11T15:36:11.292669: step 1428, loss 0.487111, acc 0.875\n",
      "2018-04-11T15:36:11.895626: step 1429, loss 0.477233, acc 0.828125\n",
      "2018-04-11T15:36:12.525693: step 1430, loss 0.385433, acc 0.890625\n",
      "2018-04-11T15:36:13.108584: step 1431, loss 0.327579, acc 0.921875\n",
      "2018-04-11T15:36:13.747718: step 1432, loss 0.414141, acc 0.828125\n",
      "2018-04-11T15:36:14.470450: step 1433, loss 0.396491, acc 0.875\n",
      "2018-04-11T15:36:15.244088: step 1434, loss 0.46715, acc 0.796875\n",
      "2018-04-11T15:36:16.038396: step 1435, loss 0.384848, acc 0.875\n",
      "2018-04-11T15:36:16.805689: step 1436, loss 0.523991, acc 0.84375\n",
      "2018-04-11T15:36:17.824489: step 1437, loss 0.439851, acc 0.875\n",
      "2018-04-11T15:36:18.708952: step 1438, loss 0.358918, acc 0.90625\n",
      "2018-04-11T15:36:19.332939: step 1439, loss 0.355966, acc 0.875\n",
      "2018-04-11T15:36:19.905524: step 1440, loss 0.615493, acc 0.8125\n",
      "2018-04-11T15:36:20.519401: step 1441, loss 0.303399, acc 0.90625\n",
      "2018-04-11T15:36:21.093130: step 1442, loss 0.716671, acc 0.78125\n",
      "2018-04-11T15:36:21.676011: step 1443, loss 0.371006, acc 0.84375\n",
      "2018-04-11T15:36:22.244754: step 1444, loss 0.598544, acc 0.796875\n",
      "2018-04-11T15:36:22.806985: step 1445, loss 0.420175, acc 0.84375\n",
      "2018-04-11T15:36:23.388502: step 1446, loss 0.43499, acc 0.859375\n",
      "2018-04-11T15:36:23.949035: step 1447, loss 0.45669, acc 0.8125\n",
      "2018-04-11T15:36:24.500778: step 1448, loss 0.509513, acc 0.8125\n",
      "2018-04-11T15:36:25.091276: step 1449, loss 0.580667, acc 0.8125\n",
      "2018-04-11T15:36:25.676589: step 1450, loss 0.394833, acc 0.859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:36:26.234644: step 1451, loss 0.376036, acc 0.875\n",
      "2018-04-11T15:36:26.816656: step 1452, loss 0.451788, acc 0.84375\n",
      "2018-04-11T15:36:27.400237: step 1453, loss 0.430159, acc 0.875\n",
      "2018-04-11T15:36:27.982871: step 1454, loss 0.334637, acc 0.921875\n",
      "2018-04-11T15:36:28.565931: step 1455, loss 0.514702, acc 0.8125\n",
      "2018-04-11T15:36:29.155271: step 1456, loss 0.361634, acc 0.890625\n",
      "2018-04-11T15:36:29.710602: step 1457, loss 0.465909, acc 0.84375\n",
      "2018-04-11T15:36:30.275366: step 1458, loss 0.612214, acc 0.796875\n",
      "2018-04-11T15:36:30.828378: step 1459, loss 0.346346, acc 0.859375\n",
      "2018-04-11T15:36:31.403375: step 1460, loss 0.594207, acc 0.78125\n",
      "2018-04-11T15:36:31.967034: step 1461, loss 0.285518, acc 0.9375\n",
      "2018-04-11T15:36:32.527088: step 1462, loss 0.516166, acc 0.8125\n",
      "2018-04-11T15:36:33.114162: step 1463, loss 0.49378, acc 0.828125\n",
      "2018-04-11T15:36:33.703170: step 1464, loss 0.545479, acc 0.8125\n",
      "2018-04-11T15:36:34.272539: step 1465, loss 0.343999, acc 0.859375\n",
      "2018-04-11T15:36:34.851832: step 1466, loss 0.5147, acc 0.859375\n",
      "2018-04-11T15:36:35.413617: step 1467, loss 0.25932, acc 0.921875\n",
      "2018-04-11T15:36:35.994615: step 1468, loss 0.507376, acc 0.796875\n",
      "2018-04-11T15:36:36.573098: step 1469, loss 0.397043, acc 0.890625\n",
      "2018-04-11T15:36:37.127575: step 1470, loss 0.435659, acc 0.890625\n",
      "2018-04-11T15:36:37.702864: step 1471, loss 0.483193, acc 0.78125\n",
      "2018-04-11T15:36:38.255221: step 1472, loss 0.474221, acc 0.84375\n",
      "2018-04-11T15:36:38.813621: step 1473, loss 0.577784, acc 0.8125\n",
      "2018-04-11T15:36:39.359870: step 1474, loss 0.325172, acc 0.890625\n",
      "2018-04-11T15:36:39.924155: step 1475, loss 0.469879, acc 0.859375\n",
      "2018-04-11T15:36:40.509653: step 1476, loss 0.341472, acc 0.890625\n",
      "2018-04-11T15:36:41.072591: step 1477, loss 0.552155, acc 0.8125\n",
      "2018-04-11T15:36:41.620091: step 1478, loss 0.407564, acc 0.90625\n",
      "2018-04-11T15:36:42.196990: step 1479, loss 0.323557, acc 0.84375\n",
      "2018-04-11T15:36:42.788346: step 1480, loss 0.579037, acc 0.78125\n",
      "2018-04-11T15:36:43.385542: step 1481, loss 0.570409, acc 0.8125\n",
      "2018-04-11T15:36:43.939805: step 1482, loss 0.538427, acc 0.78125\n",
      "2018-04-11T15:36:44.494736: step 1483, loss 0.554002, acc 0.828125\n",
      "2018-04-11T15:36:45.065723: step 1484, loss 0.545444, acc 0.765625\n",
      "2018-04-11T15:36:45.612972: step 1485, loss 0.394121, acc 0.84375\n",
      "2018-04-11T15:36:46.189277: step 1486, loss 0.378641, acc 0.859375\n",
      "2018-04-11T15:36:46.771509: step 1487, loss 0.421798, acc 0.859375\n",
      "2018-04-11T15:36:47.320004: step 1488, loss 0.52623, acc 0.828125\n",
      "2018-04-11T15:36:47.855608: step 1489, loss 0.404092, acc 0.859375\n",
      "2018-04-11T15:36:48.406654: step 1490, loss 0.519644, acc 0.859375\n",
      "2018-04-11T15:36:48.966868: step 1491, loss 0.530288, acc 0.78125\n",
      "2018-04-11T15:36:49.518936: step 1492, loss 0.294086, acc 0.890625\n",
      "2018-04-11T15:36:50.057151: step 1493, loss 0.457911, acc 0.84375\n",
      "2018-04-11T15:36:50.597421: step 1494, loss 0.337139, acc 0.875\n",
      "2018-04-11T15:36:51.151314: step 1495, loss 0.775865, acc 0.734375\n",
      "2018-04-11T15:36:51.684709: step 1496, loss 0.490268, acc 0.875\n",
      "2018-04-11T15:36:52.246230: step 1497, loss 0.568812, acc 0.796875\n",
      "2018-04-11T15:36:52.794787: step 1498, loss 0.604067, acc 0.796875\n",
      "2018-04-11T15:36:53.338541: step 1499, loss 0.369879, acc 0.859375\n",
      "2018-04-11T15:36:53.880842: step 1500, loss 0.582777, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:36:54.050795: step 1500, loss 0.580202, acc 0.81\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-1500\n",
      "\n",
      "2018-04-11T15:36:54.737200: step 1501, loss 0.279178, acc 0.953125\n",
      "2018-04-11T15:36:55.288174: step 1502, loss 0.48513, acc 0.84375\n",
      "2018-04-11T15:36:55.868434: step 1503, loss 0.705223, acc 0.734375\n",
      "2018-04-11T15:36:56.421748: step 1504, loss 0.533113, acc 0.8125\n",
      "2018-04-11T15:36:56.964426: step 1505, loss 0.516034, acc 0.75\n",
      "2018-04-11T15:36:57.532512: step 1506, loss 0.537218, acc 0.8125\n",
      "2018-04-11T15:36:58.060984: step 1507, loss 0.416011, acc 0.859375\n",
      "2018-04-11T15:36:58.594630: step 1508, loss 0.644841, acc 0.78125\n",
      "2018-04-11T15:36:59.132089: step 1509, loss 0.467012, acc 0.8125\n",
      "2018-04-11T15:36:59.691149: step 1510, loss 0.308378, acc 0.890625\n",
      "2018-04-11T15:37:00.233571: step 1511, loss 0.354841, acc 0.921875\n",
      "2018-04-11T15:37:00.791310: step 1512, loss 0.660571, acc 0.6875\n",
      "2018-04-11T15:37:01.332701: step 1513, loss 0.598975, acc 0.796875\n",
      "2018-04-11T15:37:01.889509: step 1514, loss 0.566849, acc 0.828125\n",
      "2018-04-11T15:37:02.449423: step 1515, loss 0.605272, acc 0.765625\n",
      "2018-04-11T15:37:02.998852: step 1516, loss 0.430322, acc 0.828125\n",
      "2018-04-11T15:37:03.545200: step 1517, loss 0.454648, acc 0.84375\n",
      "2018-04-11T15:37:04.078622: step 1518, loss 0.407208, acc 0.859375\n",
      "2018-04-11T15:37:04.621592: step 1519, loss 0.332185, acc 0.859375\n",
      "2018-04-11T15:37:05.183346: step 1520, loss 0.437764, acc 0.84375\n",
      "2018-04-11T15:37:05.733197: step 1521, loss 0.332146, acc 0.890625\n",
      "2018-04-11T15:37:06.302363: step 1522, loss 0.470561, acc 0.859375\n",
      "2018-04-11T15:37:06.853350: step 1523, loss 0.484603, acc 0.875\n",
      "2018-04-11T15:37:07.396337: step 1524, loss 0.692127, acc 0.796875\n",
      "2018-04-11T15:37:07.931332: step 1525, loss 0.518753, acc 0.828125\n",
      "2018-04-11T15:37:08.505154: step 1526, loss 0.418985, acc 0.859375\n",
      "2018-04-11T15:37:09.060560: step 1527, loss 0.378984, acc 0.859375\n",
      "2018-04-11T15:37:09.617887: step 1528, loss 0.693461, acc 0.8125\n",
      "2018-04-11T15:37:10.153657: step 1529, loss 0.458544, acc 0.828125\n",
      "2018-04-11T15:37:10.692311: step 1530, loss 0.572068, acc 0.8125\n",
      "2018-04-11T15:37:11.249656: step 1531, loss 0.331847, acc 0.90625\n",
      "2018-04-11T15:37:11.807852: step 1532, loss 0.362529, acc 0.890625\n",
      "2018-04-11T15:37:12.334013: step 1533, loss 0.525785, acc 0.84375\n",
      "2018-04-11T15:37:12.881436: step 1534, loss 0.544479, acc 0.78125\n",
      "2018-04-11T15:37:13.415781: step 1535, loss 0.597565, acc 0.828125\n",
      "2018-04-11T15:37:13.959168: step 1536, loss 0.335938, acc 0.890625\n",
      "2018-04-11T15:37:14.488120: step 1537, loss 0.23277, acc 0.953125\n",
      "2018-04-11T15:37:15.016712: step 1538, loss 0.480324, acc 0.828125\n",
      "2018-04-11T15:37:15.230953: step 1539, loss 0.480588, acc 0.833333\n",
      "2018-04-11T15:37:15.787717: step 1540, loss 0.383034, acc 0.84375\n",
      "2018-04-11T15:37:16.343881: step 1541, loss 0.471926, acc 0.796875\n",
      "2018-04-11T15:37:16.879743: step 1542, loss 0.472122, acc 0.859375\n",
      "2018-04-11T15:37:17.440701: step 1543, loss 0.443011, acc 0.859375\n",
      "2018-04-11T15:37:17.996771: step 1544, loss 0.400286, acc 0.859375\n",
      "2018-04-11T15:37:18.544259: step 1545, loss 0.441167, acc 0.859375\n",
      "2018-04-11T15:37:19.084835: step 1546, loss 0.311169, acc 0.90625\n",
      "2018-04-11T15:37:19.624661: step 1547, loss 0.422584, acc 0.859375\n",
      "2018-04-11T15:37:20.163494: step 1548, loss 0.348101, acc 0.890625\n",
      "2018-04-11T15:37:20.706269: step 1549, loss 0.454253, acc 0.8125\n",
      "2018-04-11T15:37:21.231049: step 1550, loss 0.432326, acc 0.828125\n",
      "2018-04-11T15:37:21.745965: step 1551, loss 0.446033, acc 0.828125\n",
      "2018-04-11T15:37:22.281495: step 1552, loss 0.421495, acc 0.875\n",
      "2018-04-11T15:37:22.806177: step 1553, loss 0.397586, acc 0.875\n",
      "2018-04-11T15:37:23.337922: step 1554, loss 0.391334, acc 0.890625\n",
      "2018-04-11T15:37:23.870294: step 1555, loss 0.335195, acc 0.90625\n",
      "2018-04-11T15:37:24.412438: step 1556, loss 0.357306, acc 0.859375\n",
      "2018-04-11T15:37:24.946821: step 1557, loss 0.397695, acc 0.859375\n",
      "2018-04-11T15:37:25.496269: step 1558, loss 0.269298, acc 0.9375\n",
      "2018-04-11T15:37:26.044178: step 1559, loss 0.35307, acc 0.890625\n",
      "2018-04-11T15:37:26.580893: step 1560, loss 0.566309, acc 0.796875\n",
      "2018-04-11T15:37:27.118336: step 1561, loss 0.267112, acc 0.953125\n",
      "2018-04-11T15:37:27.681171: step 1562, loss 0.314396, acc 0.890625\n",
      "2018-04-11T15:37:28.362718: step 1563, loss 0.43251, acc 0.78125\n",
      "2018-04-11T15:37:29.082136: step 1564, loss 0.478207, acc 0.828125\n",
      "2018-04-11T15:37:29.616345: step 1565, loss 0.240449, acc 0.90625\n",
      "2018-04-11T15:37:30.147383: step 1566, loss 0.447782, acc 0.84375\n",
      "2018-04-11T15:37:30.692408: step 1567, loss 0.339609, acc 0.859375\n",
      "2018-04-11T15:37:31.248126: step 1568, loss 0.352516, acc 0.9375\n",
      "2018-04-11T15:37:31.774353: step 1569, loss 0.399943, acc 0.875\n",
      "2018-04-11T15:37:32.305782: step 1570, loss 0.204309, acc 0.96875\n",
      "2018-04-11T15:37:32.859482: step 1571, loss 0.459172, acc 0.828125\n",
      "2018-04-11T15:37:33.413132: step 1572, loss 0.255698, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:37:33.968498: step 1573, loss 0.294419, acc 0.90625\n",
      "2018-04-11T15:37:34.500492: step 1574, loss 0.333649, acc 0.921875\n",
      "2018-04-11T15:37:35.054589: step 1575, loss 0.457532, acc 0.859375\n",
      "2018-04-11T15:37:35.597786: step 1576, loss 0.423929, acc 0.875\n",
      "2018-04-11T15:37:36.139333: step 1577, loss 0.339955, acc 0.921875\n",
      "2018-04-11T15:37:36.686999: step 1578, loss 0.458001, acc 0.84375\n",
      "2018-04-11T15:37:37.244392: step 1579, loss 0.472581, acc 0.859375\n",
      "2018-04-11T15:37:37.781309: step 1580, loss 0.32472, acc 0.890625\n",
      "2018-04-11T15:37:38.337640: step 1581, loss 0.335889, acc 0.90625\n",
      "2018-04-11T15:37:38.890508: step 1582, loss 0.418171, acc 0.828125\n",
      "2018-04-11T15:37:39.446964: step 1583, loss 0.383438, acc 0.890625\n",
      "2018-04-11T15:37:39.999498: step 1584, loss 0.421117, acc 0.859375\n",
      "2018-04-11T15:37:40.541275: step 1585, loss 0.370818, acc 0.90625\n",
      "2018-04-11T15:37:41.098570: step 1586, loss 0.43137, acc 0.84375\n",
      "2018-04-11T15:37:41.651399: step 1587, loss 0.44836, acc 0.8125\n",
      "2018-04-11T15:37:42.194529: step 1588, loss 0.385237, acc 0.875\n",
      "2018-04-11T15:37:42.748476: step 1589, loss 0.329737, acc 0.890625\n",
      "2018-04-11T15:37:43.313486: step 1590, loss 0.312345, acc 0.921875\n",
      "2018-04-11T15:37:43.877540: step 1591, loss 0.283599, acc 0.875\n",
      "2018-04-11T15:37:44.423352: step 1592, loss 0.375324, acc 0.875\n",
      "2018-04-11T15:37:44.988285: step 1593, loss 0.506195, acc 0.84375\n",
      "2018-04-11T15:37:45.552343: step 1594, loss 0.417071, acc 0.8125\n",
      "2018-04-11T15:37:46.096368: step 1595, loss 0.483331, acc 0.859375\n",
      "2018-04-11T15:37:46.646917: step 1596, loss 0.2987, acc 0.90625\n",
      "2018-04-11T15:37:47.192461: step 1597, loss 0.311345, acc 0.890625\n",
      "2018-04-11T15:37:47.732376: step 1598, loss 0.365437, acc 0.84375\n",
      "2018-04-11T15:37:48.278596: step 1599, loss 0.433825, acc 0.875\n",
      "2018-04-11T15:37:48.839386: step 1600, loss 0.450881, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:37:49.015017: step 1600, loss 0.413591, acc 0.83\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-1600\n",
      "\n",
      "2018-04-11T15:37:49.690735: step 1601, loss 0.368028, acc 0.890625\n",
      "2018-04-11T15:37:50.235777: step 1602, loss 0.358113, acc 0.90625\n",
      "2018-04-11T15:37:50.793912: step 1603, loss 0.402084, acc 0.859375\n",
      "2018-04-11T15:37:51.337713: step 1604, loss 0.215189, acc 0.921875\n",
      "2018-04-11T15:37:51.893737: step 1605, loss 0.341316, acc 0.875\n",
      "2018-04-11T15:37:52.438340: step 1606, loss 0.327532, acc 0.953125\n",
      "2018-04-11T15:37:52.997716: step 1607, loss 0.331767, acc 0.90625\n",
      "2018-04-11T15:37:53.544149: step 1608, loss 0.432984, acc 0.84375\n",
      "2018-04-11T15:37:54.088027: step 1609, loss 0.302874, acc 0.9375\n",
      "2018-04-11T15:37:54.633187: step 1610, loss 0.327302, acc 0.90625\n",
      "2018-04-11T15:37:55.185152: step 1611, loss 0.375819, acc 0.8125\n",
      "2018-04-11T15:37:55.748651: step 1612, loss 0.4315, acc 0.78125\n",
      "2018-04-11T15:37:56.293370: step 1613, loss 0.382896, acc 0.859375\n",
      "2018-04-11T15:37:56.859432: step 1614, loss 0.428267, acc 0.875\n",
      "2018-04-11T15:37:57.405517: step 1615, loss 0.355011, acc 0.90625\n",
      "2018-04-11T15:37:57.957164: step 1616, loss 0.485069, acc 0.859375\n",
      "2018-04-11T15:37:58.500927: step 1617, loss 0.39953, acc 0.828125\n",
      "2018-04-11T15:37:59.074146: step 1618, loss 0.429053, acc 0.78125\n",
      "2018-04-11T15:37:59.644265: step 1619, loss 0.292616, acc 0.890625\n",
      "2018-04-11T15:38:00.202597: step 1620, loss 0.36124, acc 0.859375\n",
      "2018-04-11T15:38:00.766214: step 1621, loss 0.366859, acc 0.875\n",
      "2018-04-11T15:38:01.330758: step 1622, loss 0.34392, acc 0.890625\n",
      "2018-04-11T15:38:01.897058: step 1623, loss 0.456134, acc 0.859375\n",
      "2018-04-11T15:38:02.486406: step 1624, loss 0.392172, acc 0.875\n",
      "2018-04-11T15:38:03.104969: step 1625, loss 0.328264, acc 0.890625\n",
      "2018-04-11T15:38:03.705623: step 1626, loss 0.295817, acc 0.9375\n",
      "2018-04-11T15:38:04.259212: step 1627, loss 0.402312, acc 0.875\n",
      "2018-04-11T15:38:04.827704: step 1628, loss 0.394501, acc 0.84375\n",
      "2018-04-11T15:38:05.413254: step 1629, loss 0.394104, acc 0.90625\n",
      "2018-04-11T15:38:05.991217: step 1630, loss 0.352462, acc 0.890625\n",
      "2018-04-11T15:38:06.545523: step 1631, loss 0.422865, acc 0.84375\n",
      "2018-04-11T15:38:07.141223: step 1632, loss 0.399025, acc 0.859375\n",
      "2018-04-11T15:38:07.723313: step 1633, loss 0.472744, acc 0.890625\n",
      "2018-04-11T15:38:08.314083: step 1634, loss 0.249917, acc 0.90625\n",
      "2018-04-11T15:38:08.874237: step 1635, loss 0.243495, acc 0.921875\n",
      "2018-04-11T15:38:09.432390: step 1636, loss 0.379849, acc 0.84375\n",
      "2018-04-11T15:38:09.988893: step 1637, loss 0.342762, acc 0.890625\n",
      "2018-04-11T15:38:10.561348: step 1638, loss 0.350047, acc 0.890625\n",
      "2018-04-11T15:38:11.187482: step 1639, loss 0.386022, acc 0.859375\n",
      "2018-04-11T15:38:11.817512: step 1640, loss 0.459276, acc 0.84375\n",
      "2018-04-11T15:38:12.388536: step 1641, loss 0.37412, acc 0.90625\n",
      "2018-04-11T15:38:12.945187: step 1642, loss 0.328623, acc 0.890625\n",
      "2018-04-11T15:38:13.478730: step 1643, loss 0.361936, acc 0.890625\n",
      "2018-04-11T15:38:14.029193: step 1644, loss 0.38649, acc 0.828125\n",
      "2018-04-11T15:38:14.571405: step 1645, loss 0.557317, acc 0.8125\n",
      "2018-04-11T15:38:15.123837: step 1646, loss 0.381775, acc 0.875\n",
      "2018-04-11T15:38:15.686228: step 1647, loss 0.347376, acc 0.90625\n",
      "2018-04-11T15:38:16.310564: step 1648, loss 0.373598, acc 0.84375\n",
      "2018-04-11T15:38:16.860676: step 1649, loss 0.266193, acc 0.921875\n",
      "2018-04-11T15:38:17.467893: step 1650, loss 0.494593, acc 0.78125\n",
      "2018-04-11T15:38:18.006646: step 1651, loss 0.3882, acc 0.875\n",
      "2018-04-11T15:38:18.617013: step 1652, loss 0.313912, acc 0.890625\n",
      "2018-04-11T15:38:19.268696: step 1653, loss 0.416465, acc 0.859375\n",
      "2018-04-11T15:38:19.816464: step 1654, loss 0.405614, acc 0.859375\n",
      "2018-04-11T15:38:20.401845: step 1655, loss 0.313746, acc 0.859375\n",
      "2018-04-11T15:38:20.960270: step 1656, loss 0.301672, acc 0.90625\n",
      "2018-04-11T15:38:21.527024: step 1657, loss 0.301932, acc 0.890625\n",
      "2018-04-11T15:38:22.078875: step 1658, loss 0.306275, acc 0.90625\n",
      "2018-04-11T15:38:22.636454: step 1659, loss 0.431572, acc 0.890625\n",
      "2018-04-11T15:38:23.207773: step 1660, loss 0.274018, acc 0.890625\n",
      "2018-04-11T15:38:23.786746: step 1661, loss 0.437615, acc 0.875\n",
      "2018-04-11T15:38:24.357908: step 1662, loss 0.345625, acc 0.875\n",
      "2018-04-11T15:38:24.925230: step 1663, loss 0.352066, acc 0.84375\n",
      "2018-04-11T15:38:25.478025: step 1664, loss 0.354391, acc 0.890625\n",
      "2018-04-11T15:38:26.030006: step 1665, loss 0.469149, acc 0.859375\n",
      "2018-04-11T15:38:26.607415: step 1666, loss 0.412532, acc 0.859375\n",
      "2018-04-11T15:38:27.157951: step 1667, loss 0.45513, acc 0.84375\n",
      "2018-04-11T15:38:27.725218: step 1668, loss 0.391315, acc 0.859375\n",
      "2018-04-11T15:38:28.276218: step 1669, loss 0.36174, acc 0.875\n",
      "2018-04-11T15:38:28.815990: step 1670, loss 0.469307, acc 0.84375\n",
      "2018-04-11T15:38:29.364910: step 1671, loss 0.3831, acc 0.875\n",
      "2018-04-11T15:38:29.939714: step 1672, loss 0.389602, acc 0.90625\n",
      "2018-04-11T15:38:30.513925: step 1673, loss 0.325872, acc 0.90625\n",
      "2018-04-11T15:38:31.056353: step 1674, loss 0.321671, acc 0.890625\n",
      "2018-04-11T15:38:31.625103: step 1675, loss 0.35157, acc 0.890625\n",
      "2018-04-11T15:38:32.169769: step 1676, loss 0.609208, acc 0.78125\n",
      "2018-04-11T15:38:32.732495: step 1677, loss 0.340404, acc 0.875\n",
      "2018-04-11T15:38:33.290410: step 1678, loss 0.427642, acc 0.875\n",
      "2018-04-11T15:38:33.855287: step 1679, loss 0.248627, acc 0.921875\n",
      "2018-04-11T15:38:34.405277: step 1680, loss 0.343388, acc 0.875\n",
      "2018-04-11T15:38:34.970330: step 1681, loss 0.230254, acc 0.921875\n",
      "2018-04-11T15:38:35.531593: step 1682, loss 0.385948, acc 0.875\n",
      "2018-04-11T15:38:36.080576: step 1683, loss 0.362244, acc 0.875\n",
      "2018-04-11T15:38:36.636981: step 1684, loss 0.419545, acc 0.828125\n",
      "2018-04-11T15:38:37.183971: step 1685, loss 0.418817, acc 0.84375\n",
      "2018-04-11T15:38:37.742478: step 1686, loss 0.33724, acc 0.890625\n",
      "2018-04-11T15:38:38.291245: step 1687, loss 0.196974, acc 0.953125\n",
      "2018-04-11T15:38:38.842069: step 1688, loss 0.327248, acc 0.875\n",
      "2018-04-11T15:38:39.389354: step 1689, loss 0.388961, acc 0.8125\n",
      "2018-04-11T15:38:39.967699: step 1690, loss 0.544059, acc 0.796875\n",
      "2018-04-11T15:38:40.540069: step 1691, loss 0.381764, acc 0.890625\n",
      "2018-04-11T15:38:41.107236: step 1692, loss 0.359883, acc 0.890625\n",
      "2018-04-11T15:38:41.664896: step 1693, loss 0.310443, acc 0.875\n",
      "2018-04-11T15:38:42.216419: step 1694, loss 0.340969, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:38:42.794252: step 1695, loss 0.417431, acc 0.890625\n",
      "2018-04-11T15:38:43.361791: step 1696, loss 0.40366, acc 0.84375\n",
      "2018-04-11T15:38:43.925397: step 1697, loss 0.344476, acc 0.890625\n",
      "2018-04-11T15:38:44.502004: step 1698, loss 0.273792, acc 0.921875\n",
      "2018-04-11T15:38:45.097032: step 1699, loss 0.399713, acc 0.875\n",
      "2018-04-11T15:38:45.657348: step 1700, loss 0.291185, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:38:45.830309: step 1700, loss 0.506528, acc 0.77\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-1700\n",
      "\n",
      "2018-04-11T15:38:46.546397: step 1701, loss 0.481216, acc 0.875\n",
      "2018-04-11T15:38:47.130837: step 1702, loss 0.354306, acc 0.90625\n",
      "2018-04-11T15:38:47.686945: step 1703, loss 0.338879, acc 0.84375\n",
      "2018-04-11T15:38:48.260981: step 1704, loss 0.42373, acc 0.84375\n",
      "2018-04-11T15:38:48.829497: step 1705, loss 0.48326, acc 0.84375\n",
      "2018-04-11T15:38:49.373862: step 1706, loss 0.422431, acc 0.859375\n",
      "2018-04-11T15:38:49.946810: step 1707, loss 0.395261, acc 0.875\n",
      "2018-04-11T15:38:50.499692: step 1708, loss 0.40775, acc 0.84375\n",
      "2018-04-11T15:38:51.070129: step 1709, loss 0.371943, acc 0.875\n",
      "2018-04-11T15:38:51.635272: step 1710, loss 0.44626, acc 0.796875\n",
      "2018-04-11T15:38:52.180362: step 1711, loss 0.2973, acc 0.875\n",
      "2018-04-11T15:38:52.747355: step 1712, loss 0.314682, acc 0.90625\n",
      "2018-04-11T15:38:53.320429: step 1713, loss 0.366057, acc 0.90625\n",
      "2018-04-11T15:38:53.866020: step 1714, loss 0.405444, acc 0.828125\n",
      "2018-04-11T15:38:54.402642: step 1715, loss 0.416603, acc 0.859375\n",
      "2018-04-11T15:38:54.979988: step 1716, loss 0.24972, acc 0.890625\n",
      "2018-04-11T15:38:55.536058: step 1717, loss 0.455375, acc 0.859375\n",
      "2018-04-11T15:38:56.084625: step 1718, loss 0.37819, acc 0.859375\n",
      "2018-04-11T15:38:56.648328: step 1719, loss 0.312394, acc 0.890625\n",
      "2018-04-11T15:38:57.217839: step 1720, loss 0.385278, acc 0.890625\n",
      "2018-04-11T15:38:57.793675: step 1721, loss 0.254095, acc 0.9375\n",
      "2018-04-11T15:38:58.348613: step 1722, loss 0.429761, acc 0.875\n",
      "2018-04-11T15:38:58.930008: step 1723, loss 0.437909, acc 0.859375\n",
      "2018-04-11T15:38:59.494079: step 1724, loss 0.193534, acc 0.9375\n",
      "2018-04-11T15:39:00.038140: step 1725, loss 0.295266, acc 0.90625\n",
      "2018-04-11T15:39:00.587073: step 1726, loss 0.402913, acc 0.828125\n",
      "2018-04-11T15:39:01.154752: step 1727, loss 0.389193, acc 0.875\n",
      "2018-04-11T15:39:01.744802: step 1728, loss 0.325381, acc 0.890625\n",
      "2018-04-11T15:39:02.301882: step 1729, loss 0.346088, acc 0.890625\n",
      "2018-04-11T15:39:02.876081: step 1730, loss 0.294533, acc 0.890625\n",
      "2018-04-11T15:39:03.415902: step 1731, loss 0.287192, acc 0.890625\n",
      "2018-04-11T15:39:03.980131: step 1732, loss 0.173643, acc 0.96875\n",
      "2018-04-11T15:39:04.544985: step 1733, loss 0.349416, acc 0.875\n",
      "2018-04-11T15:39:05.095061: step 1734, loss 0.279707, acc 0.921875\n",
      "2018-04-11T15:39:05.645720: step 1735, loss 0.258977, acc 0.9375\n",
      "2018-04-11T15:39:06.216583: step 1736, loss 0.317044, acc 0.875\n",
      "2018-04-11T15:39:06.768255: step 1737, loss 0.521163, acc 0.859375\n",
      "2018-04-11T15:39:07.330442: step 1738, loss 0.495027, acc 0.84375\n",
      "2018-04-11T15:39:07.868864: step 1739, loss 0.427392, acc 0.84375\n",
      "2018-04-11T15:39:08.458183: step 1740, loss 0.139459, acc 0.984375\n",
      "2018-04-11T15:39:09.021372: step 1741, loss 0.342541, acc 0.890625\n",
      "2018-04-11T15:39:09.579887: step 1742, loss 0.345874, acc 0.875\n",
      "2018-04-11T15:39:10.123795: step 1743, loss 0.365341, acc 0.875\n",
      "2018-04-11T15:39:10.655660: step 1744, loss 0.318576, acc 0.90625\n",
      "2018-04-11T15:39:11.190123: step 1745, loss 0.375774, acc 0.828125\n",
      "2018-04-11T15:39:11.742890: step 1746, loss 0.425111, acc 0.859375\n",
      "2018-04-11T15:39:12.287111: step 1747, loss 0.366176, acc 0.9375\n",
      "2018-04-11T15:39:12.831907: step 1748, loss 0.540669, acc 0.78125\n",
      "2018-04-11T15:39:13.392515: step 1749, loss 0.346672, acc 0.90625\n",
      "2018-04-11T15:39:13.923915: step 1750, loss 0.3533, acc 0.890625\n",
      "2018-04-11T15:39:14.454199: step 1751, loss 0.404656, acc 0.828125\n",
      "2018-04-11T15:39:14.996258: step 1752, loss 0.242974, acc 0.9375\n",
      "2018-04-11T15:39:15.548495: step 1753, loss 0.367033, acc 0.84375\n",
      "2018-04-11T15:39:16.087961: step 1754, loss 0.32854, acc 0.890625\n",
      "2018-04-11T15:39:16.625387: step 1755, loss 0.268411, acc 0.875\n",
      "2018-04-11T15:39:17.157852: step 1756, loss 0.35394, acc 0.890625\n",
      "2018-04-11T15:39:17.699292: step 1757, loss 0.346043, acc 0.90625\n",
      "2018-04-11T15:39:18.246200: step 1758, loss 0.355711, acc 0.859375\n",
      "2018-04-11T15:39:18.793168: step 1759, loss 0.509275, acc 0.8125\n",
      "2018-04-11T15:39:19.350622: step 1760, loss 0.316219, acc 0.921875\n",
      "2018-04-11T15:39:19.896256: step 1761, loss 0.256756, acc 0.90625\n",
      "2018-04-11T15:39:20.444032: step 1762, loss 0.426317, acc 0.78125\n",
      "2018-04-11T15:39:20.994126: step 1763, loss 0.480234, acc 0.84375\n",
      "2018-04-11T15:39:21.534525: step 1764, loss 0.525606, acc 0.859375\n",
      "2018-04-11T15:39:22.082613: step 1765, loss 0.316356, acc 0.90625\n",
      "2018-04-11T15:39:22.634245: step 1766, loss 0.315783, acc 0.90625\n",
      "2018-04-11T15:39:23.171615: step 1767, loss 0.405465, acc 0.859375\n",
      "2018-04-11T15:39:23.712036: step 1768, loss 0.387659, acc 0.875\n",
      "2018-04-11T15:39:24.249905: step 1769, loss 0.313453, acc 0.921875\n",
      "2018-04-11T15:39:24.780048: step 1770, loss 0.2407, acc 0.9375\n",
      "2018-04-11T15:39:25.304471: step 1771, loss 0.547655, acc 0.8125\n",
      "2018-04-11T15:39:25.840755: step 1772, loss 0.43868, acc 0.859375\n",
      "2018-04-11T15:39:26.363069: step 1773, loss 0.457703, acc 0.8125\n",
      "2018-04-11T15:39:26.918067: step 1774, loss 0.302477, acc 0.90625\n",
      "2018-04-11T15:39:27.468832: step 1775, loss 0.448636, acc 0.796875\n",
      "2018-04-11T15:39:28.027824: step 1776, loss 0.391342, acc 0.890625\n",
      "2018-04-11T15:39:28.581663: step 1777, loss 0.567473, acc 0.796875\n",
      "2018-04-11T15:39:29.113058: step 1778, loss 0.352582, acc 0.890625\n",
      "2018-04-11T15:39:29.646071: step 1779, loss 0.372868, acc 0.875\n",
      "2018-04-11T15:39:30.183343: step 1780, loss 0.262696, acc 0.953125\n",
      "2018-04-11T15:39:30.720526: step 1781, loss 0.288826, acc 0.859375\n",
      "2018-04-11T15:39:31.267395: step 1782, loss 0.364067, acc 0.90625\n",
      "2018-04-11T15:39:31.800182: step 1783, loss 0.392698, acc 0.890625\n",
      "2018-04-11T15:39:32.348552: step 1784, loss 0.270432, acc 0.921875\n",
      "2018-04-11T15:39:32.892725: step 1785, loss 0.334632, acc 0.890625\n",
      "2018-04-11T15:39:33.429479: step 1786, loss 0.373862, acc 0.875\n",
      "2018-04-11T15:39:33.983783: step 1787, loss 0.459323, acc 0.859375\n",
      "2018-04-11T15:39:34.528042: step 1788, loss 0.426508, acc 0.828125\n",
      "2018-04-11T15:39:35.050687: step 1789, loss 0.347252, acc 0.921875\n",
      "2018-04-11T15:39:35.577940: step 1790, loss 0.406531, acc 0.890625\n",
      "2018-04-11T15:39:36.117145: step 1791, loss 0.465675, acc 0.875\n",
      "2018-04-11T15:39:36.653364: step 1792, loss 0.264287, acc 0.9375\n",
      "2018-04-11T15:39:37.200308: step 1793, loss 0.365022, acc 0.859375\n",
      "2018-04-11T15:39:37.730153: step 1794, loss 0.371598, acc 0.890625\n",
      "2018-04-11T15:39:38.266401: step 1795, loss 0.491303, acc 0.78125\n",
      "2018-04-11T15:39:38.813133: step 1796, loss 0.30033, acc 0.90625\n",
      "2018-04-11T15:39:39.356430: step 1797, loss 0.455131, acc 0.875\n",
      "2018-04-11T15:39:39.901191: step 1798, loss 0.339632, acc 0.84375\n",
      "2018-04-11T15:39:40.440890: step 1799, loss 0.456904, acc 0.859375\n",
      "2018-04-11T15:39:41.003220: step 1800, loss 0.259612, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:39:41.174449: step 1800, loss 0.392245, acc 0.89\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-1800\n",
      "\n",
      "2018-04-11T15:39:41.843254: step 1801, loss 0.402891, acc 0.859375\n",
      "2018-04-11T15:39:42.379766: step 1802, loss 0.421665, acc 0.875\n",
      "2018-04-11T15:39:42.930096: step 1803, loss 0.432266, acc 0.84375\n",
      "2018-04-11T15:39:43.489146: step 1804, loss 0.377385, acc 0.859375\n",
      "2018-04-11T15:39:44.013940: step 1805, loss 0.254051, acc 0.921875\n",
      "2018-04-11T15:39:44.554782: step 1806, loss 0.205368, acc 0.953125\n",
      "2018-04-11T15:39:45.112488: step 1807, loss 0.238833, acc 0.953125\n",
      "2018-04-11T15:39:45.650856: step 1808, loss 0.449403, acc 0.8125\n",
      "2018-04-11T15:39:46.188628: step 1809, loss 0.380375, acc 0.890625\n",
      "2018-04-11T15:39:46.737873: step 1810, loss 0.402855, acc 0.828125\n",
      "2018-04-11T15:39:47.276029: step 1811, loss 0.360101, acc 0.859375\n",
      "2018-04-11T15:39:47.808251: step 1812, loss 0.520387, acc 0.8125\n",
      "2018-04-11T15:39:48.362208: step 1813, loss 0.594521, acc 0.78125\n",
      "2018-04-11T15:39:48.905437: step 1814, loss 0.325954, acc 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:39:49.449146: step 1815, loss 0.436831, acc 0.84375\n",
      "2018-04-11T15:39:49.995161: step 1816, loss 0.514712, acc 0.78125\n",
      "2018-04-11T15:39:50.531902: step 1817, loss 0.428835, acc 0.875\n",
      "2018-04-11T15:39:51.061405: step 1818, loss 0.32151, acc 0.875\n",
      "2018-04-11T15:39:51.598836: step 1819, loss 0.568771, acc 0.8125\n",
      "2018-04-11T15:39:52.130828: step 1820, loss 0.332262, acc 0.890625\n",
      "2018-04-11T15:39:52.667959: step 1821, loss 0.336374, acc 0.9375\n",
      "2018-04-11T15:39:53.211067: step 1822, loss 0.446291, acc 0.890625\n",
      "2018-04-11T15:39:53.749106: step 1823, loss 0.385555, acc 0.890625\n",
      "2018-04-11T15:39:54.287005: step 1824, loss 0.32398, acc 0.859375\n",
      "2018-04-11T15:39:54.815038: step 1825, loss 0.347163, acc 0.890625\n",
      "2018-04-11T15:39:55.351768: step 1826, loss 0.341267, acc 0.875\n",
      "2018-04-11T15:39:55.899311: step 1827, loss 0.31398, acc 0.859375\n",
      "2018-04-11T15:39:56.422383: step 1828, loss 0.352652, acc 0.90625\n",
      "2018-04-11T15:39:56.964904: step 1829, loss 0.420484, acc 0.875\n",
      "2018-04-11T15:39:57.499031: step 1830, loss 0.265862, acc 0.921875\n",
      "2018-04-11T15:39:58.032727: step 1831, loss 0.303516, acc 0.921875\n",
      "2018-04-11T15:39:58.574048: step 1832, loss 0.425434, acc 0.859375\n",
      "2018-04-11T15:39:59.103195: step 1833, loss 0.364942, acc 0.875\n",
      "2018-04-11T15:39:59.644287: step 1834, loss 0.384086, acc 0.875\n",
      "2018-04-11T15:40:00.187378: step 1835, loss 0.314971, acc 0.875\n",
      "2018-04-11T15:40:00.710236: step 1836, loss 0.318043, acc 0.875\n",
      "2018-04-11T15:40:01.239138: step 1837, loss 0.449158, acc 0.859375\n",
      "2018-04-11T15:40:01.768825: step 1838, loss 0.45345, acc 0.78125\n",
      "2018-04-11T15:40:02.309618: step 1839, loss 0.329063, acc 0.890625\n",
      "2018-04-11T15:40:02.838795: step 1840, loss 0.307617, acc 0.875\n",
      "2018-04-11T15:40:03.390644: step 1841, loss 0.354421, acc 0.890625\n",
      "2018-04-11T15:40:03.923562: step 1842, loss 0.374629, acc 0.90625\n",
      "2018-04-11T15:40:04.448520: step 1843, loss 0.429971, acc 0.875\n",
      "2018-04-11T15:40:04.969034: step 1844, loss 0.342851, acc 0.890625\n",
      "2018-04-11T15:40:05.489082: step 1845, loss 0.326052, acc 0.859375\n",
      "2018-04-11T15:40:06.039028: step 1846, loss 0.371069, acc 0.90625\n",
      "2018-04-11T15:40:06.582106: step 1847, loss 0.41144, acc 0.875\n",
      "2018-04-11T15:40:07.124621: step 1848, loss 0.331335, acc 0.875\n",
      "2018-04-11T15:40:07.655864: step 1849, loss 0.443506, acc 0.8125\n",
      "2018-04-11T15:40:08.178985: step 1850, loss 0.401145, acc 0.859375\n",
      "2018-04-11T15:40:08.695888: step 1851, loss 0.394167, acc 0.875\n",
      "2018-04-11T15:40:09.231666: step 1852, loss 0.300937, acc 0.890625\n",
      "2018-04-11T15:40:09.778125: step 1853, loss 0.277773, acc 0.9375\n",
      "2018-04-11T15:40:10.317077: step 1854, loss 0.206828, acc 0.96875\n",
      "2018-04-11T15:40:10.843010: step 1855, loss 0.421908, acc 0.875\n",
      "2018-04-11T15:40:11.364490: step 1856, loss 0.423869, acc 0.859375\n",
      "2018-04-11T15:40:11.902179: step 1857, loss 0.36091, acc 0.90625\n",
      "2018-04-11T15:40:12.450949: step 1858, loss 0.506897, acc 0.78125\n",
      "2018-04-11T15:40:12.995922: step 1859, loss 0.390546, acc 0.890625\n",
      "2018-04-11T15:40:13.544542: step 1860, loss 0.641333, acc 0.796875\n",
      "2018-04-11T15:40:14.076719: step 1861, loss 0.469142, acc 0.859375\n",
      "2018-04-11T15:40:14.614915: step 1862, loss 0.388677, acc 0.875\n",
      "2018-04-11T15:40:15.153371: step 1863, loss 0.414596, acc 0.875\n",
      "2018-04-11T15:40:15.673429: step 1864, loss 0.527817, acc 0.84375\n",
      "2018-04-11T15:40:16.207885: step 1865, loss 0.283542, acc 0.9375\n",
      "2018-04-11T15:40:16.759283: step 1866, loss 0.391317, acc 0.828125\n",
      "2018-04-11T15:40:17.302582: step 1867, loss 0.459506, acc 0.8125\n",
      "2018-04-11T15:40:17.843785: step 1868, loss 0.615, acc 0.828125\n",
      "2018-04-11T15:40:18.373544: step 1869, loss 0.351375, acc 0.890625\n",
      "2018-04-11T15:40:18.923950: step 1870, loss 0.395051, acc 0.875\n",
      "2018-04-11T15:40:19.469785: step 1871, loss 0.423613, acc 0.84375\n",
      "2018-04-11T15:40:20.004495: step 1872, loss 0.444381, acc 0.84375\n",
      "2018-04-11T15:40:20.531101: step 1873, loss 0.408881, acc 0.828125\n",
      "2018-04-11T15:40:21.059295: step 1874, loss 0.399662, acc 0.890625\n",
      "2018-04-11T15:40:21.594079: step 1875, loss 0.361967, acc 0.890625\n",
      "2018-04-11T15:40:22.120138: step 1876, loss 0.432746, acc 0.859375\n",
      "2018-04-11T15:40:22.656035: step 1877, loss 0.483072, acc 0.796875\n",
      "2018-04-11T15:40:23.163044: step 1878, loss 0.34026, acc 0.890625\n",
      "2018-04-11T15:40:23.700878: step 1879, loss 0.460569, acc 0.90625\n",
      "2018-04-11T15:40:24.236827: step 1880, loss 0.399903, acc 0.875\n",
      "2018-04-11T15:40:24.859098: step 1881, loss 0.377288, acc 0.859375\n",
      "2018-04-11T15:40:25.389766: step 1882, loss 0.292608, acc 0.890625\n",
      "2018-04-11T15:40:25.918033: step 1883, loss 0.25153, acc 0.90625\n",
      "2018-04-11T15:40:26.447105: step 1884, loss 0.340313, acc 0.859375\n",
      "2018-04-11T15:40:26.979374: step 1885, loss 0.274052, acc 0.921875\n",
      "2018-04-11T15:40:27.524392: step 1886, loss 0.413691, acc 0.875\n",
      "2018-04-11T15:40:28.051756: step 1887, loss 0.189079, acc 0.9375\n",
      "2018-04-11T15:40:28.598418: step 1888, loss 0.330549, acc 0.90625\n",
      "2018-04-11T15:40:29.111742: step 1889, loss 0.575679, acc 0.78125\n",
      "2018-04-11T15:40:29.654921: step 1890, loss 0.315898, acc 0.84375\n",
      "2018-04-11T15:40:30.195234: step 1891, loss 0.348577, acc 0.84375\n",
      "2018-04-11T15:40:30.722724: step 1892, loss 0.394712, acc 0.90625\n",
      "2018-04-11T15:40:31.258407: step 1893, loss 0.337217, acc 0.90625\n",
      "2018-04-11T15:40:31.790316: step 1894, loss 0.260068, acc 0.921875\n",
      "2018-04-11T15:40:32.331476: step 1895, loss 0.481278, acc 0.84375\n",
      "2018-04-11T15:40:32.866010: step 1896, loss 0.244474, acc 0.9375\n",
      "2018-04-11T15:40:33.406251: step 1897, loss 0.379673, acc 0.84375\n",
      "2018-04-11T15:40:33.968307: step 1898, loss 0.510401, acc 0.8125\n",
      "2018-04-11T15:40:34.502032: step 1899, loss 0.379106, acc 0.875\n",
      "2018-04-11T15:40:35.029836: step 1900, loss 0.223087, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:40:35.190035: step 1900, loss 0.538229, acc 0.82\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-1900\n",
      "\n",
      "2018-04-11T15:40:35.861819: step 1901, loss 0.341749, acc 0.90625\n",
      "2018-04-11T15:40:36.397518: step 1902, loss 0.259954, acc 0.9375\n",
      "2018-04-11T15:40:36.929348: step 1903, loss 0.598921, acc 0.796875\n",
      "2018-04-11T15:40:37.464111: step 1904, loss 0.356476, acc 0.90625\n",
      "2018-04-11T15:40:37.996851: step 1905, loss 0.413277, acc 0.84375\n",
      "2018-04-11T15:40:38.516247: step 1906, loss 0.30402, acc 0.859375\n",
      "2018-04-11T15:40:39.043369: step 1907, loss 0.358132, acc 0.875\n",
      "2018-04-11T15:40:39.574979: step 1908, loss 0.364771, acc 0.859375\n",
      "2018-04-11T15:40:40.117659: step 1909, loss 0.329675, acc 0.9375\n",
      "2018-04-11T15:40:40.674156: step 1910, loss 0.30234, acc 0.921875\n",
      "2018-04-11T15:40:41.196896: step 1911, loss 0.318465, acc 0.890625\n",
      "2018-04-11T15:40:41.734921: step 1912, loss 0.286447, acc 0.90625\n",
      "2018-04-11T15:40:42.272587: step 1913, loss 0.558927, acc 0.828125\n",
      "2018-04-11T15:40:42.812979: step 1914, loss 0.462478, acc 0.828125\n",
      "2018-04-11T15:40:43.360985: step 1915, loss 0.468804, acc 0.84375\n",
      "2018-04-11T15:40:43.886873: step 1916, loss 0.460199, acc 0.828125\n",
      "2018-04-11T15:40:44.408115: step 1917, loss 0.431115, acc 0.84375\n",
      "2018-04-11T15:40:44.924304: step 1918, loss 0.627392, acc 0.734375\n",
      "2018-04-11T15:40:45.466588: step 1919, loss 0.353128, acc 0.921875\n",
      "2018-04-11T15:40:46.001006: step 1920, loss 0.26947, acc 0.890625\n",
      "2018-04-11T15:40:46.540080: step 1921, loss 0.378253, acc 0.859375\n",
      "2018-04-11T15:40:47.072904: step 1922, loss 0.253657, acc 0.921875\n",
      "2018-04-11T15:40:47.599477: step 1923, loss 0.370333, acc 0.875\n",
      "2018-04-11T15:40:48.124256: step 1924, loss 0.349266, acc 0.859375\n",
      "2018-04-11T15:40:48.651110: step 1925, loss 0.340383, acc 0.90625\n",
      "2018-04-11T15:40:49.190198: step 1926, loss 0.377526, acc 0.828125\n",
      "2018-04-11T15:40:49.714059: step 1927, loss 0.30159, acc 0.921875\n",
      "2018-04-11T15:40:50.250656: step 1928, loss 0.515691, acc 0.84375\n",
      "2018-04-11T15:40:50.758981: step 1929, loss 0.323728, acc 0.90625\n",
      "2018-04-11T15:40:51.294052: step 1930, loss 0.460583, acc 0.8125\n",
      "2018-04-11T15:40:51.825386: step 1931, loss 0.486589, acc 0.8125\n",
      "2018-04-11T15:40:52.365525: step 1932, loss 0.241911, acc 0.9375\n",
      "2018-04-11T15:40:52.894462: step 1933, loss 0.269607, acc 0.890625\n",
      "2018-04-11T15:40:53.433280: step 1934, loss 0.358367, acc 0.875\n",
      "2018-04-11T15:40:53.968849: step 1935, loss 0.233215, acc 0.921875\n",
      "2018-04-11T15:40:54.494246: step 1936, loss 0.387342, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:40:55.008940: step 1937, loss 0.431991, acc 0.84375\n",
      "2018-04-11T15:40:55.560098: step 1938, loss 0.253861, acc 0.921875\n",
      "2018-04-11T15:40:56.090101: step 1939, loss 0.240768, acc 0.9375\n",
      "2018-04-11T15:40:56.624050: step 1940, loss 0.309988, acc 0.875\n",
      "2018-04-11T15:40:57.157311: step 1941, loss 0.455973, acc 0.796875\n",
      "2018-04-11T15:40:57.712709: step 1942, loss 0.464454, acc 0.84375\n",
      "2018-04-11T15:40:58.250811: step 1943, loss 0.290933, acc 0.890625\n",
      "2018-04-11T15:40:58.777588: step 1944, loss 0.368591, acc 0.890625\n",
      "2018-04-11T15:40:59.311011: step 1945, loss 0.338517, acc 0.875\n",
      "2018-04-11T15:40:59.835302: step 1946, loss 0.321242, acc 0.921875\n",
      "2018-04-11T15:41:00.370873: step 1947, loss 0.432053, acc 0.828125\n",
      "2018-04-11T15:41:00.904722: step 1948, loss 0.342864, acc 0.90625\n",
      "2018-04-11T15:41:01.426234: step 1949, loss 0.32174, acc 0.90625\n",
      "2018-04-11T15:41:01.954120: step 1950, loss 0.284177, acc 0.90625\n",
      "2018-04-11T15:41:02.491282: step 1951, loss 0.357868, acc 0.875\n",
      "2018-04-11T15:41:03.011640: step 1952, loss 0.270427, acc 0.921875\n",
      "2018-04-11T15:41:03.527058: step 1953, loss 0.281959, acc 0.890625\n",
      "2018-04-11T15:41:04.065671: step 1954, loss 0.341329, acc 0.875\n",
      "2018-04-11T15:41:04.576121: step 1955, loss 0.360056, acc 0.875\n",
      "2018-04-11T15:41:05.097516: step 1956, loss 0.2536, acc 0.9375\n",
      "2018-04-11T15:41:05.640821: step 1957, loss 0.443608, acc 0.875\n",
      "2018-04-11T15:41:06.159737: step 1958, loss 0.402994, acc 0.875\n",
      "2018-04-11T15:41:06.703025: step 1959, loss 0.278299, acc 0.921875\n",
      "2018-04-11T15:41:07.249135: step 1960, loss 0.331939, acc 0.875\n",
      "2018-04-11T15:41:07.786737: step 1961, loss 0.381739, acc 0.859375\n",
      "2018-04-11T15:41:08.344713: step 1962, loss 0.472757, acc 0.8125\n",
      "2018-04-11T15:41:08.871003: step 1963, loss 0.369785, acc 0.828125\n",
      "2018-04-11T15:41:09.421964: step 1964, loss 0.337655, acc 0.84375\n",
      "2018-04-11T15:41:09.951822: step 1965, loss 0.500451, acc 0.828125\n",
      "2018-04-11T15:41:10.493252: step 1966, loss 0.426785, acc 0.796875\n",
      "2018-04-11T15:41:11.024414: step 1967, loss 0.362317, acc 0.828125\n",
      "2018-04-11T15:41:11.557233: step 1968, loss 0.48195, acc 0.8125\n",
      "2018-04-11T15:41:12.086471: step 1969, loss 0.395429, acc 0.875\n",
      "2018-04-11T15:41:12.628516: step 1970, loss 0.250432, acc 0.921875\n",
      "2018-04-11T15:41:13.163399: step 1971, loss 0.3732, acc 0.890625\n",
      "2018-04-11T15:41:13.697498: step 1972, loss 0.41555, acc 0.84375\n",
      "2018-04-11T15:41:14.228703: step 1973, loss 0.338315, acc 0.859375\n",
      "2018-04-11T15:41:14.757534: step 1974, loss 0.421969, acc 0.84375\n",
      "2018-04-11T15:41:15.273185: step 1975, loss 0.301432, acc 0.890625\n",
      "2018-04-11T15:41:15.799893: step 1976, loss 0.340125, acc 0.875\n",
      "2018-04-11T15:41:16.325562: step 1977, loss 0.308207, acc 0.890625\n",
      "2018-04-11T15:41:16.854036: step 1978, loss 0.417715, acc 0.890625\n",
      "2018-04-11T15:41:17.391801: step 1979, loss 0.364783, acc 0.84375\n",
      "2018-04-11T15:41:17.916752: step 1980, loss 0.292817, acc 0.90625\n",
      "2018-04-11T15:41:18.449315: step 1981, loss 0.59163, acc 0.78125\n",
      "2018-04-11T15:41:18.961748: step 1982, loss 0.490263, acc 0.84375\n",
      "2018-04-11T15:41:19.501299: step 1983, loss 0.352839, acc 0.859375\n",
      "2018-04-11T15:41:20.042108: step 1984, loss 0.40199, acc 0.90625\n",
      "2018-04-11T15:41:20.585404: step 1985, loss 0.376683, acc 0.875\n",
      "2018-04-11T15:41:21.118401: step 1986, loss 0.340554, acc 0.828125\n",
      "2018-04-11T15:41:21.645520: step 1987, loss 0.363055, acc 0.859375\n",
      "2018-04-11T15:41:22.182186: step 1988, loss 0.298168, acc 0.90625\n",
      "2018-04-11T15:41:22.711187: step 1989, loss 0.347619, acc 0.90625\n",
      "2018-04-11T15:41:23.230586: step 1990, loss 0.380052, acc 0.90625\n",
      "2018-04-11T15:41:23.754587: step 1991, loss 0.347589, acc 0.875\n",
      "2018-04-11T15:41:24.287453: step 1992, loss 0.445909, acc 0.859375\n",
      "2018-04-11T15:41:24.813539: step 1993, loss 0.337227, acc 0.875\n",
      "2018-04-11T15:41:25.344915: step 1994, loss 0.322787, acc 0.875\n",
      "2018-04-11T15:41:25.856685: step 1995, loss 0.602652, acc 0.75\n",
      "2018-04-11T15:41:26.361247: step 1996, loss 0.448694, acc 0.859375\n",
      "2018-04-11T15:41:26.875499: step 1997, loss 0.394002, acc 0.859375\n",
      "2018-04-11T15:41:27.408697: step 1998, loss 0.575742, acc 0.78125\n",
      "2018-04-11T15:41:27.937557: step 1999, loss 0.3665, acc 0.859375\n",
      "2018-04-11T15:41:28.479337: step 2000, loss 0.142848, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:41:28.640305: step 2000, loss 0.321464, acc 0.9\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-2000\n",
      "\n",
      "2018-04-11T15:41:29.304291: step 2001, loss 0.377023, acc 0.890625\n",
      "2018-04-11T15:41:29.843571: step 2002, loss 0.417903, acc 0.859375\n",
      "2018-04-11T15:41:30.374899: step 2003, loss 0.405282, acc 0.875\n",
      "2018-04-11T15:41:30.904055: step 2004, loss 0.490669, acc 0.8125\n",
      "2018-04-11T15:41:31.428104: step 2005, loss 0.452022, acc 0.875\n",
      "2018-04-11T15:41:31.946088: step 2006, loss 0.369571, acc 0.921875\n",
      "2018-04-11T15:41:32.487495: step 2007, loss 0.347143, acc 0.90625\n",
      "2018-04-11T15:41:33.014754: step 2008, loss 0.289106, acc 0.921875\n",
      "2018-04-11T15:41:33.548727: step 2009, loss 0.481332, acc 0.890625\n",
      "2018-04-11T15:41:34.086401: step 2010, loss 0.512932, acc 0.828125\n",
      "2018-04-11T15:41:34.606294: step 2011, loss 0.556953, acc 0.8125\n",
      "2018-04-11T15:41:35.137159: step 2012, loss 0.40861, acc 0.8125\n",
      "2018-04-11T15:41:35.653487: step 2013, loss 0.327851, acc 0.890625\n",
      "2018-04-11T15:41:36.183570: step 2014, loss 0.318472, acc 0.921875\n",
      "2018-04-11T15:41:36.713147: step 2015, loss 0.238798, acc 0.9375\n",
      "2018-04-11T15:41:37.258191: step 2016, loss 0.370382, acc 0.859375\n",
      "2018-04-11T15:41:37.793211: step 2017, loss 0.305858, acc 0.890625\n",
      "2018-04-11T15:41:38.338813: step 2018, loss 0.355636, acc 0.90625\n",
      "2018-04-11T15:41:38.863414: step 2019, loss 0.492014, acc 0.84375\n",
      "2018-04-11T15:41:39.383172: step 2020, loss 0.400375, acc 0.84375\n",
      "2018-04-11T15:41:39.920153: step 2021, loss 0.282095, acc 0.90625\n",
      "2018-04-11T15:41:40.447638: step 2022, loss 0.624106, acc 0.78125\n",
      "2018-04-11T15:41:40.973828: step 2023, loss 0.320491, acc 0.921875\n",
      "2018-04-11T15:41:41.502909: step 2024, loss 0.305129, acc 0.890625\n",
      "2018-04-11T15:41:42.029123: step 2025, loss 0.263951, acc 0.90625\n",
      "2018-04-11T15:41:42.567197: step 2026, loss 0.290312, acc 0.90625\n",
      "2018-04-11T15:41:43.086537: step 2027, loss 0.303218, acc 0.90625\n",
      "2018-04-11T15:41:43.668619: step 2028, loss 0.365051, acc 0.90625\n",
      "2018-04-11T15:41:44.203343: step 2029, loss 0.412296, acc 0.875\n",
      "2018-04-11T15:41:44.718572: step 2030, loss 0.311842, acc 0.890625\n",
      "2018-04-11T15:41:45.238294: step 2031, loss 0.554793, acc 0.765625\n",
      "2018-04-11T15:41:45.792878: step 2032, loss 0.319819, acc 0.890625\n",
      "2018-04-11T15:41:46.316507: step 2033, loss 0.421952, acc 0.875\n",
      "2018-04-11T15:41:46.845584: step 2034, loss 0.429228, acc 0.859375\n",
      "2018-04-11T15:41:47.371880: step 2035, loss 0.532423, acc 0.8125\n",
      "2018-04-11T15:41:47.904571: step 2036, loss 0.337021, acc 0.90625\n",
      "2018-04-11T15:41:48.416173: step 2037, loss 0.504543, acc 0.84375\n",
      "2018-04-11T15:41:48.943690: step 2038, loss 0.385355, acc 0.875\n",
      "2018-04-11T15:41:49.461998: step 2039, loss 0.378113, acc 0.90625\n",
      "2018-04-11T15:41:49.985018: step 2040, loss 0.413437, acc 0.84375\n",
      "2018-04-11T15:41:50.513904: step 2041, loss 0.410254, acc 0.859375\n",
      "2018-04-11T15:41:51.035345: step 2042, loss 0.396558, acc 0.84375\n",
      "2018-04-11T15:41:51.558913: step 2043, loss 0.323188, acc 0.90625\n",
      "2018-04-11T15:41:52.090267: step 2044, loss 0.334772, acc 0.890625\n",
      "2018-04-11T15:41:52.628784: step 2045, loss 0.410852, acc 0.875\n",
      "2018-04-11T15:41:53.149739: step 2046, loss 0.462264, acc 0.828125\n",
      "2018-04-11T15:41:53.675923: step 2047, loss 0.345742, acc 0.890625\n",
      "2018-04-11T15:41:54.212807: step 2048, loss 0.358304, acc 0.859375\n",
      "2018-04-11T15:41:54.733991: step 2049, loss 0.352856, acc 0.890625\n",
      "2018-04-11T15:41:55.259117: step 2050, loss 0.341846, acc 0.859375\n",
      "2018-04-11T15:41:55.791101: step 2051, loss 0.308075, acc 0.90625\n",
      "2018-04-11T15:41:55.999447: step 2052, loss 0.271323, acc 0.916667\n",
      "2018-04-11T15:41:56.534265: step 2053, loss 0.420798, acc 0.859375\n",
      "2018-04-11T15:41:57.070487: step 2054, loss 0.308007, acc 0.859375\n",
      "2018-04-11T15:41:57.606223: step 2055, loss 0.305354, acc 0.875\n",
      "2018-04-11T15:41:58.140083: step 2056, loss 0.304097, acc 0.90625\n",
      "2018-04-11T15:41:58.671505: step 2057, loss 0.469415, acc 0.859375\n",
      "2018-04-11T15:41:59.211111: step 2058, loss 0.243994, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:41:59.727031: step 2059, loss 0.227215, acc 0.953125\n",
      "2018-04-11T15:42:00.294118: step 2060, loss 0.375538, acc 0.921875\n",
      "2018-04-11T15:42:00.854877: step 2061, loss 0.177375, acc 0.953125\n",
      "2018-04-11T15:42:01.399550: step 2062, loss 0.18645, acc 0.953125\n",
      "2018-04-11T15:42:01.910300: step 2063, loss 0.234924, acc 0.921875\n",
      "2018-04-11T15:42:02.438416: step 2064, loss 0.244201, acc 0.9375\n",
      "2018-04-11T15:42:02.953918: step 2065, loss 0.414329, acc 0.84375\n",
      "2018-04-11T15:42:03.480628: step 2066, loss 0.239708, acc 0.890625\n",
      "2018-04-11T15:42:04.016615: step 2067, loss 0.301557, acc 0.90625\n",
      "2018-04-11T15:42:04.535496: step 2068, loss 0.381977, acc 0.890625\n",
      "2018-04-11T15:42:05.049501: step 2069, loss 0.237268, acc 0.921875\n",
      "2018-04-11T15:42:05.571362: step 2070, loss 0.309405, acc 0.921875\n",
      "2018-04-11T15:42:06.084772: step 2071, loss 0.425433, acc 0.875\n",
      "2018-04-11T15:42:06.615363: step 2072, loss 0.530788, acc 0.8125\n",
      "2018-04-11T15:42:07.150129: step 2073, loss 0.397519, acc 0.859375\n",
      "2018-04-11T15:42:07.689051: step 2074, loss 0.200819, acc 0.96875\n",
      "2018-04-11T15:42:08.212485: step 2075, loss 0.39545, acc 0.890625\n",
      "2018-04-11T15:42:08.731654: step 2076, loss 0.191863, acc 0.953125\n",
      "2018-04-11T15:42:09.274223: step 2077, loss 0.287141, acc 0.890625\n",
      "2018-04-11T15:42:09.816086: step 2078, loss 0.443171, acc 0.828125\n",
      "2018-04-11T15:42:10.333144: step 2079, loss 0.227944, acc 0.953125\n",
      "2018-04-11T15:42:10.870170: step 2080, loss 0.369137, acc 0.859375\n",
      "2018-04-11T15:42:11.404847: step 2081, loss 0.269806, acc 0.90625\n",
      "2018-04-11T15:42:11.942109: step 2082, loss 0.27795, acc 0.921875\n",
      "2018-04-11T15:42:12.476205: step 2083, loss 0.397524, acc 0.875\n",
      "2018-04-11T15:42:13.010205: step 2084, loss 0.266209, acc 0.890625\n",
      "2018-04-11T15:42:13.545850: step 2085, loss 0.262068, acc 0.9375\n",
      "2018-04-11T15:42:14.076463: step 2086, loss 0.307764, acc 0.875\n",
      "2018-04-11T15:42:14.615938: step 2087, loss 0.398114, acc 0.859375\n",
      "2018-04-11T15:42:15.137298: step 2088, loss 0.166055, acc 0.96875\n",
      "2018-04-11T15:42:15.654458: step 2089, loss 0.452946, acc 0.828125\n",
      "2018-04-11T15:42:16.169760: step 2090, loss 0.217205, acc 0.9375\n",
      "2018-04-11T15:42:16.684788: step 2091, loss 0.299734, acc 0.90625\n",
      "2018-04-11T15:42:17.205894: step 2092, loss 0.292394, acc 0.921875\n",
      "2018-04-11T15:42:17.736626: step 2093, loss 0.288695, acc 0.9375\n",
      "2018-04-11T15:42:18.281834: step 2094, loss 0.219842, acc 0.9375\n",
      "2018-04-11T15:42:18.819909: step 2095, loss 0.354759, acc 0.890625\n",
      "2018-04-11T15:42:19.356649: step 2096, loss 0.175474, acc 0.953125\n",
      "2018-04-11T15:42:19.897806: step 2097, loss 0.20565, acc 0.96875\n",
      "2018-04-11T15:42:20.424561: step 2098, loss 0.339038, acc 0.890625\n",
      "2018-04-11T15:42:20.947993: step 2099, loss 0.43175, acc 0.890625\n",
      "2018-04-11T15:42:21.465771: step 2100, loss 0.192678, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:42:21.624654: step 2100, loss 0.453167, acc 0.83\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-2100\n",
      "\n",
      "2018-04-11T15:42:22.292162: step 2101, loss 0.282999, acc 0.921875\n",
      "2018-04-11T15:42:22.820971: step 2102, loss 0.255461, acc 0.90625\n",
      "2018-04-11T15:42:23.361928: step 2103, loss 0.293055, acc 0.890625\n",
      "2018-04-11T15:42:23.888050: step 2104, loss 0.242359, acc 0.9375\n",
      "2018-04-11T15:42:24.425526: step 2105, loss 0.335513, acc 0.890625\n",
      "2018-04-11T15:42:24.957949: step 2106, loss 0.395646, acc 0.859375\n",
      "2018-04-11T15:42:25.486630: step 2107, loss 0.254687, acc 0.90625\n",
      "2018-04-11T15:42:26.019338: step 2108, loss 0.204009, acc 0.953125\n",
      "2018-04-11T15:42:26.552584: step 2109, loss 0.303247, acc 0.90625\n",
      "2018-04-11T15:42:27.073481: step 2110, loss 0.134278, acc 0.96875\n",
      "2018-04-11T15:42:27.600975: step 2111, loss 0.357264, acc 0.84375\n",
      "2018-04-11T15:42:28.119778: step 2112, loss 0.289988, acc 0.90625\n",
      "2018-04-11T15:42:28.646793: step 2113, loss 0.352084, acc 0.84375\n",
      "2018-04-11T15:42:29.185806: step 2114, loss 0.326359, acc 0.875\n",
      "2018-04-11T15:42:29.713874: step 2115, loss 0.355451, acc 0.890625\n",
      "2018-04-11T15:42:30.257156: step 2116, loss 0.299968, acc 0.875\n",
      "2018-04-11T15:42:30.791199: step 2117, loss 0.247228, acc 0.921875\n",
      "2018-04-11T15:42:31.325402: step 2118, loss 0.317177, acc 0.921875\n",
      "2018-04-11T15:42:31.851365: step 2119, loss 0.333738, acc 0.890625\n",
      "2018-04-11T15:42:32.377181: step 2120, loss 0.353639, acc 0.84375\n",
      "2018-04-11T15:42:32.901206: step 2121, loss 0.289455, acc 0.890625\n",
      "2018-04-11T15:42:33.422183: step 2122, loss 0.365423, acc 0.859375\n",
      "2018-04-11T15:42:33.944747: step 2123, loss 0.346213, acc 0.859375\n",
      "2018-04-11T15:42:34.469017: step 2124, loss 0.380506, acc 0.828125\n",
      "2018-04-11T15:42:35.001777: step 2125, loss 0.27941, acc 0.875\n",
      "2018-04-11T15:42:35.533565: step 2126, loss 0.193055, acc 0.96875\n",
      "2018-04-11T15:42:36.037752: step 2127, loss 0.374788, acc 0.859375\n",
      "2018-04-11T15:42:36.579618: step 2128, loss 0.436249, acc 0.875\n",
      "2018-04-11T15:42:37.131134: step 2129, loss 0.163445, acc 0.96875\n",
      "2018-04-11T15:42:37.674282: step 2130, loss 0.26812, acc 0.9375\n",
      "2018-04-11T15:42:38.204734: step 2131, loss 0.225797, acc 0.921875\n",
      "2018-04-11T15:42:38.726529: step 2132, loss 0.177432, acc 0.953125\n",
      "2018-04-11T15:42:39.244715: step 2133, loss 0.222036, acc 0.953125\n",
      "2018-04-11T15:42:39.761789: step 2134, loss 0.179557, acc 0.921875\n",
      "2018-04-11T15:42:40.280201: step 2135, loss 0.375115, acc 0.890625\n",
      "2018-04-11T15:42:40.818185: step 2136, loss 0.137092, acc 0.984375\n",
      "2018-04-11T15:42:41.355837: step 2137, loss 0.298394, acc 0.875\n",
      "2018-04-11T15:42:41.887996: step 2138, loss 0.353477, acc 0.90625\n",
      "2018-04-11T15:42:42.406206: step 2139, loss 0.340221, acc 0.875\n",
      "2018-04-11T15:42:42.935107: step 2140, loss 0.48992, acc 0.8125\n",
      "2018-04-11T15:42:43.479954: step 2141, loss 0.322478, acc 0.890625\n",
      "2018-04-11T15:42:44.012333: step 2142, loss 0.208808, acc 0.96875\n",
      "2018-04-11T15:42:44.542650: step 2143, loss 0.253392, acc 0.9375\n",
      "2018-04-11T15:42:45.067745: step 2144, loss 0.285802, acc 0.921875\n",
      "2018-04-11T15:42:45.596438: step 2145, loss 0.249763, acc 0.9375\n",
      "2018-04-11T15:42:46.125909: step 2146, loss 0.193542, acc 0.96875\n",
      "2018-04-11T15:42:46.650395: step 2147, loss 0.161237, acc 0.96875\n",
      "2018-04-11T15:42:47.185469: step 2148, loss 0.315505, acc 0.890625\n",
      "2018-04-11T15:42:47.721940: step 2149, loss 0.347261, acc 0.859375\n",
      "2018-04-11T15:42:48.244712: step 2150, loss 0.381488, acc 0.875\n",
      "2018-04-11T15:42:48.762200: step 2151, loss 0.31402, acc 0.890625\n",
      "2018-04-11T15:42:49.285972: step 2152, loss 0.264715, acc 0.953125\n",
      "2018-04-11T15:42:49.809855: step 2153, loss 0.238988, acc 0.921875\n",
      "2018-04-11T15:42:50.338901: step 2154, loss 0.288479, acc 0.90625\n",
      "2018-04-11T15:42:50.870488: step 2155, loss 0.351468, acc 0.828125\n",
      "2018-04-11T15:42:51.396039: step 2156, loss 0.412081, acc 0.859375\n",
      "2018-04-11T15:42:51.928096: step 2157, loss 0.317858, acc 0.9375\n",
      "2018-04-11T15:42:52.456231: step 2158, loss 0.238522, acc 0.921875\n",
      "2018-04-11T15:42:52.996181: step 2159, loss 0.33591, acc 0.890625\n",
      "2018-04-11T15:42:53.534463: step 2160, loss 0.400608, acc 0.84375\n",
      "2018-04-11T15:42:54.071600: step 2161, loss 0.247902, acc 0.890625\n",
      "2018-04-11T15:42:54.604862: step 2162, loss 0.168943, acc 0.96875\n",
      "2018-04-11T15:42:55.146352: step 2163, loss 0.440089, acc 0.875\n",
      "2018-04-11T15:42:55.682728: step 2164, loss 0.235535, acc 0.9375\n",
      "2018-04-11T15:42:56.249891: step 2165, loss 0.273098, acc 0.921875\n",
      "2018-04-11T15:42:56.783049: step 2166, loss 0.29169, acc 0.90625\n",
      "2018-04-11T15:42:57.309560: step 2167, loss 0.426848, acc 0.859375\n",
      "2018-04-11T15:42:57.849639: step 2168, loss 0.330872, acc 0.890625\n",
      "2018-04-11T15:42:58.364095: step 2169, loss 0.192637, acc 0.953125\n",
      "2018-04-11T15:42:58.878737: step 2170, loss 0.31611, acc 0.890625\n",
      "2018-04-11T15:42:59.412478: step 2171, loss 0.271736, acc 0.921875\n",
      "2018-04-11T15:42:59.939856: step 2172, loss 0.298243, acc 0.890625\n",
      "2018-04-11T15:43:00.463857: step 2173, loss 0.315128, acc 0.90625\n",
      "2018-04-11T15:43:00.986923: step 2174, loss 0.279626, acc 0.875\n",
      "2018-04-11T15:43:01.507625: step 2175, loss 0.272905, acc 0.890625\n",
      "2018-04-11T15:43:02.040332: step 2176, loss 0.23539, acc 0.9375\n",
      "2018-04-11T15:43:02.565687: step 2177, loss 0.221825, acc 0.9375\n",
      "2018-04-11T15:43:03.088784: step 2178, loss 0.270299, acc 0.921875\n",
      "2018-04-11T15:43:03.625367: step 2179, loss 0.303203, acc 0.890625\n",
      "2018-04-11T15:43:04.154044: step 2180, loss 0.228422, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:43:04.660415: step 2181, loss 0.231729, acc 0.921875\n",
      "2018-04-11T15:43:05.180311: step 2182, loss 0.315703, acc 0.90625\n",
      "2018-04-11T15:43:05.704170: step 2183, loss 0.345457, acc 0.890625\n",
      "2018-04-11T15:43:06.242093: step 2184, loss 0.350721, acc 0.84375\n",
      "2018-04-11T15:43:06.770225: step 2185, loss 0.264051, acc 0.90625\n",
      "2018-04-11T15:43:07.311920: step 2186, loss 0.366216, acc 0.875\n",
      "2018-04-11T15:43:07.839801: step 2187, loss 0.194094, acc 0.953125\n",
      "2018-04-11T15:43:08.395628: step 2188, loss 0.227965, acc 0.890625\n",
      "2018-04-11T15:43:08.914321: step 2189, loss 0.232467, acc 0.9375\n",
      "2018-04-11T15:43:09.461868: step 2190, loss 0.301038, acc 0.890625\n",
      "2018-04-11T15:43:09.982805: step 2191, loss 0.168111, acc 0.953125\n",
      "2018-04-11T15:43:10.501180: step 2192, loss 0.215689, acc 0.9375\n",
      "2018-04-11T15:43:11.033482: step 2193, loss 0.281129, acc 0.890625\n",
      "2018-04-11T15:43:11.549487: step 2194, loss 0.162621, acc 0.984375\n",
      "2018-04-11T15:43:12.096688: step 2195, loss 0.253129, acc 0.9375\n",
      "2018-04-11T15:43:12.603525: step 2196, loss 0.434418, acc 0.84375\n",
      "2018-04-11T15:43:13.129768: step 2197, loss 0.452039, acc 0.859375\n",
      "2018-04-11T15:43:13.660364: step 2198, loss 0.289997, acc 0.890625\n",
      "2018-04-11T15:43:14.180520: step 2199, loss 0.345795, acc 0.890625\n",
      "2018-04-11T15:43:14.716170: step 2200, loss 0.320407, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:43:14.877543: step 2200, loss 0.662913, acc 0.78\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-2200\n",
      "\n",
      "2018-04-11T15:43:15.540972: step 2201, loss 0.294295, acc 0.90625\n",
      "2018-04-11T15:43:16.085648: step 2202, loss 0.451655, acc 0.84375\n",
      "2018-04-11T15:43:16.732621: step 2203, loss 0.389053, acc 0.859375\n",
      "2018-04-11T15:43:17.309114: step 2204, loss 0.325103, acc 0.875\n",
      "2018-04-11T15:43:17.837694: step 2205, loss 0.264695, acc 0.921875\n",
      "2018-04-11T15:43:18.358214: step 2206, loss 0.211682, acc 0.9375\n",
      "2018-04-11T15:43:18.887703: step 2207, loss 0.365925, acc 0.875\n",
      "2018-04-11T15:43:19.421332: step 2208, loss 0.276883, acc 0.9375\n",
      "2018-04-11T15:43:19.955243: step 2209, loss 0.397416, acc 0.90625\n",
      "2018-04-11T15:43:20.480660: step 2210, loss 0.329481, acc 0.875\n",
      "2018-04-11T15:43:21.004382: step 2211, loss 0.248924, acc 0.953125\n",
      "2018-04-11T15:43:21.532719: step 2212, loss 0.256015, acc 0.890625\n",
      "2018-04-11T15:43:22.049695: step 2213, loss 0.272036, acc 0.9375\n",
      "2018-04-11T15:43:22.596297: step 2214, loss 0.173924, acc 0.953125\n",
      "2018-04-11T15:43:23.131050: step 2215, loss 0.31675, acc 0.875\n",
      "2018-04-11T15:43:23.660309: step 2216, loss 0.36112, acc 0.875\n",
      "2018-04-11T15:43:24.205570: step 2217, loss 0.266727, acc 0.875\n",
      "2018-04-11T15:43:24.723872: step 2218, loss 0.294982, acc 0.890625\n",
      "2018-04-11T15:43:25.256817: step 2219, loss 0.351212, acc 0.921875\n",
      "2018-04-11T15:43:25.782341: step 2220, loss 0.33978, acc 0.875\n",
      "2018-04-11T15:43:26.322654: step 2221, loss 0.223025, acc 0.953125\n",
      "2018-04-11T15:43:26.857082: step 2222, loss 0.241852, acc 0.90625\n",
      "2018-04-11T15:43:27.379516: step 2223, loss 0.430847, acc 0.828125\n",
      "2018-04-11T15:43:27.899230: step 2224, loss 0.21479, acc 0.9375\n",
      "2018-04-11T15:43:28.431609: step 2225, loss 0.30484, acc 0.9375\n",
      "2018-04-11T15:43:28.971356: step 2226, loss 0.294323, acc 0.890625\n",
      "2018-04-11T15:43:29.486742: step 2227, loss 0.343192, acc 0.890625\n",
      "2018-04-11T15:43:30.030674: step 2228, loss 0.166096, acc 0.953125\n",
      "2018-04-11T15:43:30.548763: step 2229, loss 0.379117, acc 0.84375\n",
      "2018-04-11T15:43:31.087237: step 2230, loss 0.32858, acc 0.84375\n",
      "2018-04-11T15:43:31.606699: step 2231, loss 0.206305, acc 0.9375\n",
      "2018-04-11T15:43:32.130525: step 2232, loss 0.324273, acc 0.875\n",
      "2018-04-11T15:43:32.664920: step 2233, loss 0.276894, acc 0.921875\n",
      "2018-04-11T15:43:33.211160: step 2234, loss 0.219363, acc 0.984375\n",
      "2018-04-11T15:43:33.752649: step 2235, loss 0.371238, acc 0.8125\n",
      "2018-04-11T15:43:34.277096: step 2236, loss 0.273825, acc 0.90625\n",
      "2018-04-11T15:43:34.806371: step 2237, loss 0.325804, acc 0.90625\n",
      "2018-04-11T15:43:35.335259: step 2238, loss 0.346058, acc 0.859375\n",
      "2018-04-11T15:43:35.870551: step 2239, loss 0.32328, acc 0.875\n",
      "2018-04-11T15:43:36.394762: step 2240, loss 0.286881, acc 0.921875\n",
      "2018-04-11T15:43:36.920358: step 2241, loss 0.242581, acc 0.90625\n",
      "2018-04-11T15:43:37.439694: step 2242, loss 0.172269, acc 0.9375\n",
      "2018-04-11T15:43:37.971186: step 2243, loss 0.269369, acc 0.953125\n",
      "2018-04-11T15:43:38.498792: step 2244, loss 0.236166, acc 0.90625\n",
      "2018-04-11T15:43:39.042615: step 2245, loss 0.298408, acc 0.90625\n",
      "2018-04-11T15:43:39.580918: step 2246, loss 0.339945, acc 0.875\n",
      "2018-04-11T15:43:40.122285: step 2247, loss 0.332453, acc 0.84375\n",
      "2018-04-11T15:43:40.648010: step 2248, loss 0.182847, acc 0.921875\n",
      "2018-04-11T15:43:41.206818: step 2249, loss 0.325215, acc 0.875\n",
      "2018-04-11T15:43:41.725385: step 2250, loss 0.267273, acc 0.921875\n",
      "2018-04-11T15:43:42.243492: step 2251, loss 0.279444, acc 0.921875\n",
      "2018-04-11T15:43:42.772266: step 2252, loss 0.370926, acc 0.875\n",
      "2018-04-11T15:43:43.308604: step 2253, loss 0.164475, acc 0.96875\n",
      "2018-04-11T15:43:43.840407: step 2254, loss 0.261083, acc 0.890625\n",
      "2018-04-11T15:43:44.392173: step 2255, loss 0.243413, acc 0.9375\n",
      "2018-04-11T15:43:44.924660: step 2256, loss 0.246459, acc 0.9375\n",
      "2018-04-11T15:43:45.465101: step 2257, loss 0.384912, acc 0.875\n",
      "2018-04-11T15:43:46.011856: step 2258, loss 0.228373, acc 0.96875\n",
      "2018-04-11T15:43:46.546605: step 2259, loss 0.322415, acc 0.875\n",
      "2018-04-11T15:43:47.091311: step 2260, loss 0.513836, acc 0.8125\n",
      "2018-04-11T15:43:47.611583: step 2261, loss 0.317536, acc 0.890625\n",
      "2018-04-11T15:43:48.143923: step 2262, loss 0.255119, acc 0.9375\n",
      "2018-04-11T15:43:48.680013: step 2263, loss 0.233256, acc 0.90625\n",
      "2018-04-11T15:43:49.219531: step 2264, loss 0.315796, acc 0.9375\n",
      "2018-04-11T15:43:49.743210: step 2265, loss 0.315537, acc 0.90625\n",
      "2018-04-11T15:43:50.281739: step 2266, loss 0.383445, acc 0.84375\n",
      "2018-04-11T15:43:50.805243: step 2267, loss 0.47067, acc 0.890625\n",
      "2018-04-11T15:43:51.339795: step 2268, loss 0.259341, acc 0.921875\n",
      "2018-04-11T15:43:51.854835: step 2269, loss 0.274962, acc 0.90625\n",
      "2018-04-11T15:43:52.387512: step 2270, loss 0.293881, acc 0.9375\n",
      "2018-04-11T15:43:52.908452: step 2271, loss 0.343496, acc 0.921875\n",
      "2018-04-11T15:43:53.432664: step 2272, loss 0.219482, acc 0.953125\n",
      "2018-04-11T15:43:53.963393: step 2273, loss 0.22416, acc 0.921875\n",
      "2018-04-11T15:43:54.500600: step 2274, loss 0.438585, acc 0.84375\n",
      "2018-04-11T15:43:55.027664: step 2275, loss 0.350753, acc 0.875\n",
      "2018-04-11T15:43:55.545335: step 2276, loss 0.332986, acc 0.859375\n",
      "2018-04-11T15:43:56.069546: step 2277, loss 0.330499, acc 0.875\n",
      "2018-04-11T15:43:56.593195: step 2278, loss 0.182616, acc 0.9375\n",
      "2018-04-11T15:43:57.113802: step 2279, loss 0.334055, acc 0.90625\n",
      "2018-04-11T15:43:57.642312: step 2280, loss 0.191372, acc 0.953125\n",
      "2018-04-11T15:43:58.151075: step 2281, loss 0.432609, acc 0.875\n",
      "2018-04-11T15:43:58.691192: step 2282, loss 0.339727, acc 0.859375\n",
      "2018-04-11T15:43:59.204435: step 2283, loss 0.279973, acc 0.890625\n",
      "2018-04-11T15:43:59.743397: step 2284, loss 0.181958, acc 0.921875\n",
      "2018-04-11T15:44:00.268794: step 2285, loss 0.277673, acc 0.90625\n",
      "2018-04-11T15:44:00.786866: step 2286, loss 0.313993, acc 0.890625\n",
      "2018-04-11T15:44:01.306091: step 2287, loss 0.259423, acc 0.890625\n",
      "2018-04-11T15:44:01.828079: step 2288, loss 0.291341, acc 0.90625\n",
      "2018-04-11T15:44:02.353853: step 2289, loss 0.345616, acc 0.875\n",
      "2018-04-11T15:44:02.872224: step 2290, loss 0.234623, acc 0.9375\n",
      "2018-04-11T15:44:03.388073: step 2291, loss 0.351208, acc 0.859375\n",
      "2018-04-11T15:44:03.923427: step 2292, loss 0.493414, acc 0.84375\n",
      "2018-04-11T15:44:04.452462: step 2293, loss 0.323472, acc 0.890625\n",
      "2018-04-11T15:44:04.987196: step 2294, loss 0.257875, acc 0.921875\n",
      "2018-04-11T15:44:05.535439: step 2295, loss 0.230898, acc 0.921875\n",
      "2018-04-11T15:44:06.058459: step 2296, loss 0.274536, acc 0.90625\n",
      "2018-04-11T15:44:06.580623: step 2297, loss 0.159963, acc 0.96875\n",
      "2018-04-11T15:44:07.110754: step 2298, loss 0.232105, acc 0.96875\n",
      "2018-04-11T15:44:07.664186: step 2299, loss 0.254165, acc 0.921875\n",
      "2018-04-11T15:44:08.184880: step 2300, loss 0.323763, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:44:08.343054: step 2300, loss 0.502781, acc 0.83\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-2300\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:44:09.006962: step 2301, loss 0.273855, acc 0.90625\n",
      "2018-04-11T15:44:09.529016: step 2302, loss 0.29075, acc 0.890625\n",
      "2018-04-11T15:44:10.071095: step 2303, loss 0.311912, acc 0.90625\n",
      "2018-04-11T15:44:10.598548: step 2304, loss 0.36013, acc 0.890625\n",
      "2018-04-11T15:44:11.128416: step 2305, loss 0.181837, acc 0.9375\n",
      "2018-04-11T15:44:11.649700: step 2306, loss 0.300885, acc 0.859375\n",
      "2018-04-11T15:44:12.174165: step 2307, loss 0.397825, acc 0.875\n",
      "2018-04-11T15:44:12.703452: step 2308, loss 0.323307, acc 0.875\n",
      "2018-04-11T15:44:13.220252: step 2309, loss 0.281096, acc 0.875\n",
      "2018-04-11T15:44:13.728045: step 2310, loss 0.340926, acc 0.84375\n",
      "2018-04-11T15:44:14.253272: step 2311, loss 0.235858, acc 0.90625\n",
      "2018-04-11T15:44:14.790151: step 2312, loss 0.288556, acc 0.90625\n",
      "2018-04-11T15:44:15.314382: step 2313, loss 0.231503, acc 0.921875\n",
      "2018-04-11T15:44:15.829407: step 2314, loss 0.266342, acc 0.90625\n",
      "2018-04-11T15:44:16.348030: step 2315, loss 0.392124, acc 0.875\n",
      "2018-04-11T15:44:16.857624: step 2316, loss 0.423107, acc 0.890625\n",
      "2018-04-11T15:44:17.392468: step 2317, loss 0.225906, acc 0.90625\n",
      "2018-04-11T15:44:17.898448: step 2318, loss 0.284555, acc 0.90625\n",
      "2018-04-11T15:44:18.426136: step 2319, loss 0.332509, acc 0.875\n",
      "2018-04-11T15:44:18.957923: step 2320, loss 0.258346, acc 0.921875\n",
      "2018-04-11T15:44:19.472992: step 2321, loss 0.23638, acc 0.921875\n",
      "2018-04-11T15:44:19.997863: step 2322, loss 0.292083, acc 0.921875\n",
      "2018-04-11T15:44:20.498784: step 2323, loss 0.145267, acc 0.9375\n",
      "2018-04-11T15:44:21.017456: step 2324, loss 0.394106, acc 0.875\n",
      "2018-04-11T15:44:21.550995: step 2325, loss 0.177436, acc 0.96875\n",
      "2018-04-11T15:44:22.092534: step 2326, loss 0.202939, acc 0.9375\n",
      "2018-04-11T15:44:22.621383: step 2327, loss 0.124784, acc 0.96875\n",
      "2018-04-11T15:44:23.133889: step 2328, loss 0.234463, acc 0.96875\n",
      "2018-04-11T15:44:23.656117: step 2329, loss 0.310595, acc 0.921875\n",
      "2018-04-11T15:44:24.183276: step 2330, loss 0.332413, acc 0.875\n",
      "2018-04-11T15:44:24.705571: step 2331, loss 0.185454, acc 0.953125\n",
      "2018-04-11T15:44:25.240151: step 2332, loss 0.502046, acc 0.84375\n",
      "2018-04-11T15:44:25.764610: step 2333, loss 0.236676, acc 0.921875\n",
      "2018-04-11T15:44:26.287852: step 2334, loss 0.302598, acc 0.875\n",
      "2018-04-11T15:44:26.806299: step 2335, loss 0.146408, acc 0.984375\n",
      "2018-04-11T15:44:27.337717: step 2336, loss 0.261469, acc 0.875\n",
      "2018-04-11T15:44:27.875148: step 2337, loss 0.205352, acc 0.9375\n",
      "2018-04-11T15:44:28.404348: step 2338, loss 0.309793, acc 0.84375\n",
      "2018-04-11T15:44:28.934785: step 2339, loss 0.301026, acc 0.90625\n",
      "2018-04-11T15:44:29.450442: step 2340, loss 0.292991, acc 0.90625\n",
      "2018-04-11T15:44:29.985503: step 2341, loss 0.313034, acc 0.875\n",
      "2018-04-11T15:44:30.506226: step 2342, loss 0.358303, acc 0.875\n",
      "2018-04-11T15:44:31.030953: step 2343, loss 0.296768, acc 0.890625\n",
      "2018-04-11T15:44:31.557865: step 2344, loss 0.491186, acc 0.8125\n",
      "2018-04-11T15:44:32.087266: step 2345, loss 0.305105, acc 0.921875\n",
      "2018-04-11T15:44:32.628893: step 2346, loss 0.298555, acc 0.921875\n",
      "2018-04-11T15:44:33.168956: step 2347, loss 0.236434, acc 0.953125\n",
      "2018-04-11T15:44:33.711058: step 2348, loss 0.208317, acc 0.9375\n",
      "2018-04-11T15:44:34.239881: step 2349, loss 0.265869, acc 0.9375\n",
      "2018-04-11T15:44:34.770011: step 2350, loss 0.240866, acc 0.9375\n",
      "2018-04-11T15:44:35.299312: step 2351, loss 0.196416, acc 0.90625\n",
      "2018-04-11T15:44:35.818325: step 2352, loss 0.285725, acc 0.890625\n",
      "2018-04-11T15:44:36.332915: step 2353, loss 0.287874, acc 0.890625\n",
      "2018-04-11T15:44:36.859892: step 2354, loss 0.197702, acc 0.9375\n",
      "2018-04-11T15:44:37.388257: step 2355, loss 0.194266, acc 0.9375\n",
      "2018-04-11T15:44:37.926643: step 2356, loss 0.28259, acc 0.90625\n",
      "2018-04-11T15:44:38.442228: step 2357, loss 0.20585, acc 0.90625\n",
      "2018-04-11T15:44:38.955230: step 2358, loss 0.260704, acc 0.890625\n",
      "2018-04-11T15:44:39.498701: step 2359, loss 0.331793, acc 0.890625\n",
      "2018-04-11T15:44:40.034869: step 2360, loss 0.193179, acc 0.9375\n",
      "2018-04-11T15:44:40.557534: step 2361, loss 0.311808, acc 0.90625\n",
      "2018-04-11T15:44:41.073991: step 2362, loss 0.307326, acc 0.875\n",
      "2018-04-11T15:44:41.610631: step 2363, loss 0.507522, acc 0.84375\n",
      "2018-04-11T15:44:42.137994: step 2364, loss 0.431967, acc 0.859375\n",
      "2018-04-11T15:44:42.656504: step 2365, loss 0.361329, acc 0.875\n",
      "2018-04-11T15:44:43.199145: step 2366, loss 0.579678, acc 0.84375\n",
      "2018-04-11T15:44:43.721776: step 2367, loss 0.37947, acc 0.84375\n",
      "2018-04-11T15:44:44.238594: step 2368, loss 0.35142, acc 0.875\n",
      "2018-04-11T15:44:44.764600: step 2369, loss 0.268439, acc 0.921875\n",
      "2018-04-11T15:44:45.305619: step 2370, loss 0.265835, acc 0.921875\n",
      "2018-04-11T15:44:45.832022: step 2371, loss 0.257282, acc 0.9375\n",
      "2018-04-11T15:44:46.353380: step 2372, loss 0.359523, acc 0.890625\n",
      "2018-04-11T15:44:46.867184: step 2373, loss 0.183009, acc 0.953125\n",
      "2018-04-11T15:44:47.377855: step 2374, loss 0.317416, acc 0.921875\n",
      "2018-04-11T15:44:47.897501: step 2375, loss 0.313137, acc 0.9375\n",
      "2018-04-11T15:44:48.425744: step 2376, loss 0.386662, acc 0.875\n",
      "2018-04-11T15:44:48.956057: step 2377, loss 0.328521, acc 0.84375\n",
      "2018-04-11T15:44:49.470915: step 2378, loss 0.23244, acc 0.921875\n",
      "2018-04-11T15:44:49.989815: step 2379, loss 0.256874, acc 0.921875\n",
      "2018-04-11T15:44:50.504020: step 2380, loss 0.346184, acc 0.875\n",
      "2018-04-11T15:44:51.015859: step 2381, loss 0.352648, acc 0.9375\n",
      "2018-04-11T15:44:51.540464: step 2382, loss 0.252516, acc 0.921875\n",
      "2018-04-11T15:44:52.077729: step 2383, loss 0.218478, acc 0.9375\n",
      "2018-04-11T15:44:52.610924: step 2384, loss 0.21174, acc 0.9375\n",
      "2018-04-11T15:44:53.136834: step 2385, loss 0.356397, acc 0.84375\n",
      "2018-04-11T15:44:53.669162: step 2386, loss 0.376833, acc 0.859375\n",
      "2018-04-11T15:44:54.190851: step 2387, loss 0.364517, acc 0.90625\n",
      "2018-04-11T15:44:54.714963: step 2388, loss 0.251463, acc 0.921875\n",
      "2018-04-11T15:44:55.242029: step 2389, loss 0.297602, acc 0.90625\n",
      "2018-04-11T15:44:55.778335: step 2390, loss 0.320814, acc 0.9375\n",
      "2018-04-11T15:44:56.311280: step 2391, loss 0.108444, acc 0.984375\n",
      "2018-04-11T15:44:56.824901: step 2392, loss 0.44882, acc 0.828125\n",
      "2018-04-11T15:44:57.346220: step 2393, loss 0.378158, acc 0.90625\n",
      "2018-04-11T15:44:57.882792: step 2394, loss 0.418057, acc 0.859375\n",
      "2018-04-11T15:44:58.408605: step 2395, loss 0.287803, acc 0.9375\n",
      "2018-04-11T15:44:58.932716: step 2396, loss 0.42407, acc 0.859375\n",
      "2018-04-11T15:44:59.446704: step 2397, loss 0.355962, acc 0.859375\n",
      "2018-04-11T15:44:59.983146: step 2398, loss 0.261372, acc 0.921875\n",
      "2018-04-11T15:45:00.514062: step 2399, loss 0.313447, acc 0.890625\n",
      "2018-04-11T15:45:01.034033: step 2400, loss 0.17773, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:45:01.194352: step 2400, loss 0.376595, acc 0.84\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-2400\n",
      "\n",
      "2018-04-11T15:45:01.843727: step 2401, loss 0.294161, acc 0.90625\n",
      "2018-04-11T15:45:02.375812: step 2402, loss 0.214533, acc 0.9375\n",
      "2018-04-11T15:45:02.902279: step 2403, loss 0.254097, acc 0.90625\n",
      "2018-04-11T15:45:03.442153: step 2404, loss 0.21436, acc 0.9375\n",
      "2018-04-11T15:45:03.983041: step 2405, loss 0.389592, acc 0.875\n",
      "2018-04-11T15:45:04.505705: step 2406, loss 0.250431, acc 0.90625\n",
      "2018-04-11T15:45:05.016948: step 2407, loss 0.312913, acc 0.90625\n",
      "2018-04-11T15:45:05.530046: step 2408, loss 0.163124, acc 0.9375\n",
      "2018-04-11T15:45:06.057378: step 2409, loss 0.259318, acc 0.90625\n",
      "2018-04-11T15:45:06.582028: step 2410, loss 0.363706, acc 0.890625\n",
      "2018-04-11T15:45:07.127208: step 2411, loss 0.337036, acc 0.890625\n",
      "2018-04-11T15:45:07.649040: step 2412, loss 0.501128, acc 0.84375\n",
      "2018-04-11T15:45:08.197126: step 2413, loss 0.201535, acc 0.921875\n",
      "2018-04-11T15:45:08.715714: step 2414, loss 0.385686, acc 0.890625\n",
      "2018-04-11T15:45:09.231115: step 2415, loss 0.20576, acc 0.9375\n",
      "2018-04-11T15:45:09.759286: step 2416, loss 0.334953, acc 0.921875\n",
      "2018-04-11T15:45:10.276058: step 2417, loss 0.27729, acc 0.9375\n",
      "2018-04-11T15:45:10.796312: step 2418, loss 0.366569, acc 0.875\n",
      "2018-04-11T15:45:11.317307: step 2419, loss 0.361717, acc 0.84375\n",
      "2018-04-11T15:45:11.850772: step 2420, loss 0.242742, acc 0.90625\n",
      "2018-04-11T15:45:12.383090: step 2421, loss 0.395538, acc 0.859375\n",
      "2018-04-11T15:45:12.914322: step 2422, loss 0.272965, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:45:13.443972: step 2423, loss 0.358835, acc 0.859375\n",
      "2018-04-11T15:45:13.971303: step 2424, loss 0.16028, acc 0.96875\n",
      "2018-04-11T15:45:14.503040: step 2425, loss 0.351948, acc 0.875\n",
      "2018-04-11T15:45:15.039414: step 2426, loss 0.204542, acc 0.9375\n",
      "2018-04-11T15:45:15.566604: step 2427, loss 0.246905, acc 0.90625\n",
      "2018-04-11T15:45:16.091497: step 2428, loss 0.245636, acc 0.9375\n",
      "2018-04-11T15:45:16.615424: step 2429, loss 0.177827, acc 0.9375\n",
      "2018-04-11T15:45:17.140649: step 2430, loss 0.328455, acc 0.84375\n",
      "2018-04-11T15:45:17.659735: step 2431, loss 0.271122, acc 0.90625\n",
      "2018-04-11T15:45:18.188460: step 2432, loss 0.404628, acc 0.875\n",
      "2018-04-11T15:45:18.709955: step 2433, loss 0.396495, acc 0.875\n",
      "2018-04-11T15:45:19.228316: step 2434, loss 0.302068, acc 0.9375\n",
      "2018-04-11T15:45:19.764719: step 2435, loss 0.373305, acc 0.875\n",
      "2018-04-11T15:45:20.295392: step 2436, loss 0.274459, acc 0.90625\n",
      "2018-04-11T15:45:20.871251: step 2437, loss 0.39446, acc 0.859375\n",
      "2018-04-11T15:45:21.508935: step 2438, loss 0.348488, acc 0.9375\n",
      "2018-04-11T15:45:22.188256: step 2439, loss 0.384699, acc 0.859375\n",
      "2018-04-11T15:45:22.902091: step 2440, loss 0.351986, acc 0.84375\n",
      "2018-04-11T15:45:23.439253: step 2441, loss 0.261668, acc 0.90625\n",
      "2018-04-11T15:45:24.038576: step 2442, loss 0.284254, acc 0.890625\n",
      "2018-04-11T15:45:24.627426: step 2443, loss 0.363776, acc 0.90625\n",
      "2018-04-11T15:45:25.165419: step 2444, loss 0.346157, acc 0.890625\n",
      "2018-04-11T15:45:25.715816: step 2445, loss 0.345654, acc 0.890625\n",
      "2018-04-11T15:45:26.299747: step 2446, loss 0.24448, acc 0.921875\n",
      "2018-04-11T15:45:26.855191: step 2447, loss 0.234637, acc 0.90625\n",
      "2018-04-11T15:45:27.465269: step 2448, loss 0.553203, acc 0.828125\n",
      "2018-04-11T15:45:27.992812: step 2449, loss 0.260558, acc 0.90625\n",
      "2018-04-11T15:45:28.560171: step 2450, loss 0.357688, acc 0.890625\n",
      "2018-04-11T15:45:29.183944: step 2451, loss 0.308174, acc 0.90625\n",
      "2018-04-11T15:45:29.853242: step 2452, loss 0.308463, acc 0.890625\n",
      "2018-04-11T15:45:30.409645: step 2453, loss 0.276597, acc 0.890625\n",
      "2018-04-11T15:45:30.975085: step 2454, loss 0.368512, acc 0.890625\n",
      "2018-04-11T15:45:31.542739: step 2455, loss 0.24262, acc 0.9375\n",
      "2018-04-11T15:45:32.184102: step 2456, loss 0.221925, acc 0.921875\n",
      "2018-04-11T15:45:32.711344: step 2457, loss 0.399393, acc 0.84375\n",
      "2018-04-11T15:45:33.267669: step 2458, loss 0.319429, acc 0.890625\n",
      "2018-04-11T15:45:33.828940: step 2459, loss 0.341358, acc 0.890625\n",
      "2018-04-11T15:45:34.376291: step 2460, loss 0.257842, acc 0.921875\n",
      "2018-04-11T15:45:34.925771: step 2461, loss 0.384716, acc 0.84375\n",
      "2018-04-11T15:45:35.464468: step 2462, loss 0.326949, acc 0.859375\n",
      "2018-04-11T15:45:36.016551: step 2463, loss 0.467584, acc 0.828125\n",
      "2018-04-11T15:45:36.562658: step 2464, loss 0.341296, acc 0.90625\n",
      "2018-04-11T15:45:37.105180: step 2465, loss 0.244414, acc 0.9375\n",
      "2018-04-11T15:45:37.652349: step 2466, loss 0.509638, acc 0.796875\n",
      "2018-04-11T15:45:38.202838: step 2467, loss 0.294741, acc 0.96875\n",
      "2018-04-11T15:45:38.733287: step 2468, loss 0.248438, acc 0.921875\n",
      "2018-04-11T15:45:39.301788: step 2469, loss 0.271475, acc 0.90625\n",
      "2018-04-11T15:45:39.919291: step 2470, loss 0.385599, acc 0.875\n",
      "2018-04-11T15:45:40.459719: step 2471, loss 0.227327, acc 0.9375\n",
      "2018-04-11T15:45:41.048838: step 2472, loss 0.311023, acc 0.859375\n",
      "2018-04-11T15:45:41.621467: step 2473, loss 0.276763, acc 0.90625\n",
      "2018-04-11T15:45:42.186540: step 2474, loss 0.223339, acc 0.921875\n",
      "2018-04-11T15:45:42.741976: step 2475, loss 0.282062, acc 0.9375\n",
      "2018-04-11T15:45:43.276778: step 2476, loss 0.221927, acc 0.921875\n",
      "2018-04-11T15:45:43.808612: step 2477, loss 0.234271, acc 0.890625\n",
      "2018-04-11T15:45:44.362903: step 2478, loss 0.187432, acc 0.953125\n",
      "2018-04-11T15:45:44.900595: step 2479, loss 0.296412, acc 0.90625\n",
      "2018-04-11T15:45:45.468742: step 2480, loss 0.374074, acc 0.875\n",
      "2018-04-11T15:45:45.994585: step 2481, loss 0.138531, acc 0.96875\n",
      "2018-04-11T15:45:46.531047: step 2482, loss 0.306265, acc 0.90625\n",
      "2018-04-11T15:45:47.097010: step 2483, loss 0.183113, acc 0.953125\n",
      "2018-04-11T15:45:47.662742: step 2484, loss 0.709857, acc 0.78125\n",
      "2018-04-11T15:45:48.231788: step 2485, loss 0.21649, acc 0.953125\n",
      "2018-04-11T15:45:48.776972: step 2486, loss 0.360404, acc 0.828125\n",
      "2018-04-11T15:45:49.333510: step 2487, loss 0.307214, acc 0.921875\n",
      "2018-04-11T15:45:49.865524: step 2488, loss 0.188257, acc 0.953125\n",
      "2018-04-11T15:45:50.401013: step 2489, loss 0.337454, acc 0.890625\n",
      "2018-04-11T15:45:50.960936: step 2490, loss 0.347782, acc 0.90625\n",
      "2018-04-11T15:45:51.503091: step 2491, loss 0.238105, acc 0.90625\n",
      "2018-04-11T15:45:52.054507: step 2492, loss 0.383506, acc 0.84375\n",
      "2018-04-11T15:45:52.582992: step 2493, loss 0.244075, acc 0.890625\n",
      "2018-04-11T15:45:53.135972: step 2494, loss 0.156679, acc 0.96875\n",
      "2018-04-11T15:45:53.676002: step 2495, loss 0.388477, acc 0.90625\n",
      "2018-04-11T15:45:54.226085: step 2496, loss 0.297507, acc 0.90625\n",
      "2018-04-11T15:45:54.770718: step 2497, loss 0.292866, acc 0.890625\n",
      "2018-04-11T15:45:55.319105: step 2498, loss 0.190234, acc 0.96875\n",
      "2018-04-11T15:45:55.881473: step 2499, loss 0.30335, acc 0.875\n",
      "2018-04-11T15:45:56.441986: step 2500, loss 0.196996, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:45:56.616930: step 2500, loss 0.600562, acc 0.78\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-2500\n",
      "\n",
      "2018-04-11T15:45:57.301021: step 2501, loss 0.260192, acc 0.90625\n",
      "2018-04-11T15:45:57.828735: step 2502, loss 0.305143, acc 0.875\n",
      "2018-04-11T15:45:58.380993: step 2503, loss 0.370714, acc 0.9375\n",
      "2018-04-11T15:45:58.916747: step 2504, loss 0.345353, acc 0.890625\n",
      "2018-04-11T15:45:59.469781: step 2505, loss 0.435951, acc 0.875\n",
      "2018-04-11T15:46:00.040423: step 2506, loss 0.178097, acc 0.953125\n",
      "2018-04-11T15:46:00.616036: step 2507, loss 0.253827, acc 0.890625\n",
      "2018-04-11T15:46:01.174873: step 2508, loss 0.337673, acc 0.90625\n",
      "2018-04-11T15:46:01.730936: step 2509, loss 0.349068, acc 0.90625\n",
      "2018-04-11T15:46:02.297932: step 2510, loss 0.278936, acc 0.890625\n",
      "2018-04-11T15:46:02.856800: step 2511, loss 0.405556, acc 0.859375\n",
      "2018-04-11T15:46:03.417748: step 2512, loss 0.174747, acc 0.96875\n",
      "2018-04-11T15:46:03.959717: step 2513, loss 0.275521, acc 0.890625\n",
      "2018-04-11T15:46:04.529070: step 2514, loss 0.237677, acc 0.953125\n",
      "2018-04-11T15:46:05.083597: step 2515, loss 0.188634, acc 0.9375\n",
      "2018-04-11T15:46:05.626402: step 2516, loss 0.186813, acc 0.953125\n",
      "2018-04-11T15:46:06.198753: step 2517, loss 0.512377, acc 0.828125\n",
      "2018-04-11T15:46:06.756765: step 2518, loss 0.247297, acc 0.9375\n",
      "2018-04-11T15:46:07.320258: step 2519, loss 0.420407, acc 0.84375\n",
      "2018-04-11T15:46:07.870889: step 2520, loss 0.20258, acc 0.9375\n",
      "2018-04-11T15:46:08.434903: step 2521, loss 0.338429, acc 0.890625\n",
      "2018-04-11T15:46:09.002289: step 2522, loss 0.51384, acc 0.8125\n",
      "2018-04-11T15:46:09.553713: step 2523, loss 0.240349, acc 0.890625\n",
      "2018-04-11T15:46:10.103072: step 2524, loss 0.263897, acc 0.921875\n",
      "2018-04-11T15:46:10.675755: step 2525, loss 0.369643, acc 0.84375\n",
      "2018-04-11T15:46:11.242885: step 2526, loss 0.409575, acc 0.859375\n",
      "2018-04-11T15:46:11.790538: step 2527, loss 0.285296, acc 0.921875\n",
      "2018-04-11T15:46:12.342954: step 2528, loss 0.198883, acc 0.9375\n",
      "2018-04-11T15:46:12.915261: step 2529, loss 0.364319, acc 0.890625\n",
      "2018-04-11T15:46:13.459897: step 2530, loss 0.233133, acc 0.90625\n",
      "2018-04-11T15:46:14.039074: step 2531, loss 0.339424, acc 0.859375\n",
      "2018-04-11T15:46:14.612879: step 2532, loss 0.312061, acc 0.9375\n",
      "2018-04-11T15:46:15.171677: step 2533, loss 0.416884, acc 0.828125\n",
      "2018-04-11T15:46:15.734843: step 2534, loss 0.255645, acc 0.890625\n",
      "2018-04-11T15:46:16.288473: step 2535, loss 0.233925, acc 0.9375\n",
      "2018-04-11T15:46:16.846250: step 2536, loss 0.198634, acc 0.96875\n",
      "2018-04-11T15:46:17.405676: step 2537, loss 0.281302, acc 0.9375\n",
      "2018-04-11T15:46:17.975855: step 2538, loss 0.341452, acc 0.921875\n",
      "2018-04-11T15:46:18.517625: step 2539, loss 0.277203, acc 0.890625\n",
      "2018-04-11T15:46:19.069859: step 2540, loss 0.319589, acc 0.921875\n",
      "2018-04-11T15:46:19.615254: step 2541, loss 0.343584, acc 0.875\n",
      "2018-04-11T15:46:20.154939: step 2542, loss 0.21795, acc 0.9375\n",
      "2018-04-11T15:46:20.720304: step 2543, loss 0.244614, acc 0.921875\n",
      "2018-04-11T15:46:21.282138: step 2544, loss 0.203622, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:46:21.824538: step 2545, loss 0.279764, acc 0.9375\n",
      "2018-04-11T15:46:22.367538: step 2546, loss 0.306047, acc 0.890625\n",
      "2018-04-11T15:46:22.928307: step 2547, loss 0.176696, acc 0.96875\n",
      "2018-04-11T15:46:23.489437: step 2548, loss 0.285541, acc 0.90625\n",
      "2018-04-11T15:46:24.045829: step 2549, loss 0.32249, acc 0.890625\n",
      "2018-04-11T15:46:24.594297: step 2550, loss 0.268342, acc 0.90625\n",
      "2018-04-11T15:46:25.146757: step 2551, loss 0.275638, acc 0.921875\n",
      "2018-04-11T15:46:25.707881: step 2552, loss 0.263017, acc 0.90625\n",
      "2018-04-11T15:46:26.265382: step 2553, loss 0.124711, acc 0.96875\n",
      "2018-04-11T15:46:26.829318: step 2554, loss 0.518894, acc 0.84375\n",
      "2018-04-11T15:46:27.382982: step 2555, loss 0.295281, acc 0.90625\n",
      "2018-04-11T15:46:27.946179: step 2556, loss 0.336524, acc 0.890625\n",
      "2018-04-11T15:46:28.498444: step 2557, loss 0.359623, acc 0.859375\n",
      "2018-04-11T15:46:29.026214: step 2558, loss 0.284568, acc 0.890625\n",
      "2018-04-11T15:46:29.591757: step 2559, loss 0.196655, acc 0.921875\n",
      "2018-04-11T15:46:30.153941: step 2560, loss 0.409072, acc 0.90625\n",
      "2018-04-11T15:46:30.690000: step 2561, loss 0.28604, acc 0.90625\n",
      "2018-04-11T15:46:31.249695: step 2562, loss 0.331313, acc 0.890625\n",
      "2018-04-11T15:46:31.811977: step 2563, loss 0.177946, acc 0.984375\n",
      "2018-04-11T15:46:32.361730: step 2564, loss 0.376183, acc 0.890625\n",
      "2018-04-11T15:46:32.582936: step 2565, loss 0.364449, acc 0.75\n",
      "2018-04-11T15:46:33.117692: step 2566, loss 0.311651, acc 0.890625\n",
      "2018-04-11T15:46:33.701553: step 2567, loss 0.220174, acc 0.890625\n",
      "2018-04-11T15:46:34.250918: step 2568, loss 0.233419, acc 0.9375\n",
      "2018-04-11T15:46:34.783469: step 2569, loss 0.169353, acc 0.953125\n",
      "2018-04-11T15:46:35.310345: step 2570, loss 0.202001, acc 0.90625\n",
      "2018-04-11T15:46:35.853001: step 2571, loss 0.280472, acc 0.90625\n",
      "2018-04-11T15:46:36.415969: step 2572, loss 0.190915, acc 0.921875\n",
      "2018-04-11T15:46:36.979752: step 2573, loss 0.133444, acc 0.96875\n",
      "2018-04-11T15:46:37.518063: step 2574, loss 0.175138, acc 0.9375\n",
      "2018-04-11T15:46:38.048039: step 2575, loss 0.212244, acc 0.921875\n",
      "2018-04-11T15:46:38.595947: step 2576, loss 0.249902, acc 0.90625\n",
      "2018-04-11T15:46:39.149219: step 2577, loss 0.179929, acc 0.9375\n",
      "2018-04-11T15:46:39.704679: step 2578, loss 0.230779, acc 0.9375\n",
      "2018-04-11T15:46:40.249690: step 2579, loss 0.197392, acc 0.921875\n",
      "2018-04-11T15:46:40.772876: step 2580, loss 0.357308, acc 0.921875\n",
      "2018-04-11T15:46:41.308891: step 2581, loss 0.278067, acc 0.9375\n",
      "2018-04-11T15:46:41.853249: step 2582, loss 0.373136, acc 0.84375\n",
      "2018-04-11T15:46:42.401638: step 2583, loss 0.178092, acc 0.953125\n",
      "2018-04-11T15:46:42.942924: step 2584, loss 0.178378, acc 0.96875\n",
      "2018-04-11T15:46:43.501662: step 2585, loss 0.150962, acc 0.9375\n",
      "2018-04-11T15:46:44.056254: step 2586, loss 0.322076, acc 0.859375\n",
      "2018-04-11T15:46:44.601995: step 2587, loss 0.259359, acc 0.9375\n",
      "2018-04-11T15:46:45.167435: step 2588, loss 0.300933, acc 0.921875\n",
      "2018-04-11T15:46:45.710329: step 2589, loss 0.30515, acc 0.890625\n",
      "2018-04-11T15:46:46.261222: step 2590, loss 0.205841, acc 0.90625\n",
      "2018-04-11T15:46:46.822195: step 2591, loss 0.283861, acc 0.90625\n",
      "2018-04-11T15:46:47.360715: step 2592, loss 0.147555, acc 0.984375\n",
      "2018-04-11T15:46:47.894841: step 2593, loss 0.272669, acc 0.90625\n",
      "2018-04-11T15:46:48.418968: step 2594, loss 0.190478, acc 0.953125\n",
      "2018-04-11T15:46:48.958725: step 2595, loss 0.161327, acc 0.96875\n",
      "2018-04-11T15:46:49.495349: step 2596, loss 0.255546, acc 0.90625\n",
      "2018-04-11T15:46:50.045851: step 2597, loss 0.259634, acc 0.921875\n",
      "2018-04-11T15:46:50.585821: step 2598, loss 0.191796, acc 0.921875\n",
      "2018-04-11T15:46:51.116122: step 2599, loss 0.149016, acc 0.953125\n",
      "2018-04-11T15:46:51.669900: step 2600, loss 0.250423, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:46:51.840066: step 2600, loss 0.574403, acc 0.82\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-2600\n",
      "\n",
      "2018-04-11T15:46:52.510308: step 2601, loss 0.175273, acc 0.9375\n",
      "2018-04-11T15:46:53.052845: step 2602, loss 0.256301, acc 0.921875\n",
      "2018-04-11T15:46:53.581163: step 2603, loss 0.169824, acc 0.9375\n",
      "2018-04-11T15:46:54.131927: step 2604, loss 0.251411, acc 0.921875\n",
      "2018-04-11T15:46:54.676871: step 2605, loss 0.356176, acc 0.890625\n",
      "2018-04-11T15:46:55.213350: step 2606, loss 0.192214, acc 0.953125\n",
      "2018-04-11T15:46:55.776306: step 2607, loss 0.150237, acc 0.9375\n",
      "2018-04-11T15:46:56.320884: step 2608, loss 0.2078, acc 0.921875\n",
      "2018-04-11T15:46:56.849393: step 2609, loss 0.281492, acc 0.90625\n",
      "2018-04-11T15:46:57.392012: step 2610, loss 0.120816, acc 0.96875\n",
      "2018-04-11T15:46:57.928427: step 2611, loss 0.148386, acc 0.984375\n",
      "2018-04-11T15:46:58.481044: step 2612, loss 0.398428, acc 0.875\n",
      "2018-04-11T15:46:59.028718: step 2613, loss 0.158902, acc 0.96875\n",
      "2018-04-11T15:46:59.568796: step 2614, loss 0.19922, acc 0.921875\n",
      "2018-04-11T15:47:00.100093: step 2615, loss 0.341993, acc 0.890625\n",
      "2018-04-11T15:47:00.649622: step 2616, loss 0.280399, acc 0.875\n",
      "2018-04-11T15:47:01.169146: step 2617, loss 0.254084, acc 0.90625\n",
      "2018-04-11T15:47:01.707395: step 2618, loss 0.193976, acc 0.953125\n",
      "2018-04-11T15:47:02.261428: step 2619, loss 0.245227, acc 0.921875\n",
      "2018-04-11T15:47:02.841968: step 2620, loss 0.279497, acc 0.890625\n",
      "2018-04-11T15:47:03.370546: step 2621, loss 0.312891, acc 0.921875\n",
      "2018-04-11T15:47:03.910036: step 2622, loss 0.209049, acc 0.953125\n",
      "2018-04-11T15:47:04.445824: step 2623, loss 0.278826, acc 0.90625\n",
      "2018-04-11T15:47:04.985390: step 2624, loss 0.243785, acc 0.9375\n",
      "2018-04-11T15:47:05.540642: step 2625, loss 0.231978, acc 0.9375\n",
      "2018-04-11T15:47:06.070818: step 2626, loss 0.152544, acc 0.96875\n",
      "2018-04-11T15:47:06.598013: step 2627, loss 0.177637, acc 0.9375\n",
      "2018-04-11T15:47:07.137257: step 2628, loss 0.221985, acc 0.9375\n",
      "2018-04-11T15:47:07.660270: step 2629, loss 0.534581, acc 0.828125\n",
      "2018-04-11T15:47:08.215298: step 2630, loss 0.204097, acc 0.9375\n",
      "2018-04-11T15:47:08.745776: step 2631, loss 0.208922, acc 0.921875\n",
      "2018-04-11T15:47:09.271706: step 2632, loss 0.213404, acc 0.921875\n",
      "2018-04-11T15:47:09.815710: step 2633, loss 0.293299, acc 0.9375\n",
      "2018-04-11T15:47:10.351002: step 2634, loss 0.240846, acc 0.921875\n",
      "2018-04-11T15:47:10.883916: step 2635, loss 0.249379, acc 0.953125\n",
      "2018-04-11T15:47:11.407740: step 2636, loss 0.164456, acc 0.9375\n",
      "2018-04-11T15:47:11.947255: step 2637, loss 0.326294, acc 0.890625\n",
      "2018-04-11T15:47:12.484056: step 2638, loss 0.197568, acc 0.953125\n",
      "2018-04-11T15:47:13.012003: step 2639, loss 0.163239, acc 0.953125\n",
      "2018-04-11T15:47:13.534983: step 2640, loss 0.273282, acc 0.9375\n",
      "2018-04-11T15:47:14.061287: step 2641, loss 0.220759, acc 0.90625\n",
      "2018-04-11T15:47:14.582432: step 2642, loss 0.134683, acc 0.96875\n",
      "2018-04-11T15:47:15.105429: step 2643, loss 0.203462, acc 0.9375\n",
      "2018-04-11T15:47:15.627469: step 2644, loss 0.217745, acc 0.9375\n",
      "2018-04-11T15:47:16.160252: step 2645, loss 0.198121, acc 0.953125\n",
      "2018-04-11T15:47:16.690234: step 2646, loss 0.151933, acc 0.984375\n",
      "2018-04-11T15:47:17.225962: step 2647, loss 0.204348, acc 0.90625\n",
      "2018-04-11T15:47:17.756250: step 2648, loss 0.260642, acc 0.890625\n",
      "2018-04-11T15:47:18.290396: step 2649, loss 0.331291, acc 0.921875\n",
      "2018-04-11T15:47:18.818716: step 2650, loss 0.193223, acc 0.953125\n",
      "2018-04-11T15:47:19.368140: step 2651, loss 0.137408, acc 0.96875\n",
      "2018-04-11T15:47:19.900347: step 2652, loss 0.248375, acc 0.890625\n",
      "2018-04-11T15:47:20.446676: step 2653, loss 0.22032, acc 0.953125\n",
      "2018-04-11T15:47:20.998099: step 2654, loss 0.319643, acc 0.890625\n",
      "2018-04-11T15:47:21.540333: step 2655, loss 0.184685, acc 0.953125\n",
      "2018-04-11T15:47:22.070558: step 2656, loss 0.25759, acc 0.921875\n",
      "2018-04-11T15:47:22.612226: step 2657, loss 0.263598, acc 0.921875\n",
      "2018-04-11T15:47:23.154725: step 2658, loss 0.233373, acc 0.921875\n",
      "2018-04-11T15:47:23.689321: step 2659, loss 0.245445, acc 0.9375\n",
      "2018-04-11T15:47:24.226781: step 2660, loss 0.259165, acc 0.890625\n",
      "2018-04-11T15:47:24.753218: step 2661, loss 0.238372, acc 0.921875\n",
      "2018-04-11T15:47:25.294935: step 2662, loss 0.202702, acc 0.953125\n",
      "2018-04-11T15:47:25.837076: step 2663, loss 0.13343, acc 0.984375\n",
      "2018-04-11T15:47:26.371848: step 2664, loss 0.186984, acc 0.9375\n",
      "2018-04-11T15:47:26.887767: step 2665, loss 0.206782, acc 0.9375\n",
      "2018-04-11T15:47:27.430780: step 2666, loss 0.365544, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:47:27.963327: step 2667, loss 0.211489, acc 0.921875\n",
      "2018-04-11T15:47:28.493814: step 2668, loss 0.187295, acc 0.96875\n",
      "2018-04-11T15:47:29.040951: step 2669, loss 0.184983, acc 0.921875\n",
      "2018-04-11T15:47:29.570752: step 2670, loss 0.259741, acc 0.890625\n",
      "2018-04-11T15:47:30.095018: step 2671, loss 0.216277, acc 0.90625\n",
      "2018-04-11T15:47:30.639192: step 2672, loss 0.195553, acc 0.9375\n",
      "2018-04-11T15:47:31.170824: step 2673, loss 0.30305, acc 0.921875\n",
      "2018-04-11T15:47:31.711562: step 2674, loss 0.231641, acc 0.890625\n",
      "2018-04-11T15:47:32.235455: step 2675, loss 0.107836, acc 0.984375\n",
      "2018-04-11T15:47:32.770112: step 2676, loss 0.248345, acc 0.90625\n",
      "2018-04-11T15:47:33.299340: step 2677, loss 0.260608, acc 0.875\n",
      "2018-04-11T15:47:33.854695: step 2678, loss 0.328182, acc 0.890625\n",
      "2018-04-11T15:47:34.387441: step 2679, loss 0.225182, acc 0.9375\n",
      "2018-04-11T15:47:34.903015: step 2680, loss 0.123521, acc 0.96875\n",
      "2018-04-11T15:47:35.415401: step 2681, loss 0.359499, acc 0.828125\n",
      "2018-04-11T15:47:35.947668: step 2682, loss 0.248113, acc 0.90625\n",
      "2018-04-11T15:47:36.487300: step 2683, loss 0.245662, acc 0.9375\n",
      "2018-04-11T15:47:37.023474: step 2684, loss 0.328378, acc 0.875\n",
      "2018-04-11T15:47:37.547866: step 2685, loss 0.285793, acc 0.890625\n",
      "2018-04-11T15:47:38.076199: step 2686, loss 0.332013, acc 0.84375\n",
      "2018-04-11T15:47:38.602351: step 2687, loss 0.249297, acc 0.9375\n",
      "2018-04-11T15:47:39.128047: step 2688, loss 0.206245, acc 0.90625\n",
      "2018-04-11T15:47:39.656377: step 2689, loss 0.136997, acc 0.984375\n",
      "2018-04-11T15:47:40.183359: step 2690, loss 0.224868, acc 0.9375\n",
      "2018-04-11T15:47:40.721260: step 2691, loss 0.129741, acc 0.96875\n",
      "2018-04-11T15:47:41.251878: step 2692, loss 0.177081, acc 0.953125\n",
      "2018-04-11T15:47:41.787409: step 2693, loss 0.342451, acc 0.921875\n",
      "2018-04-11T15:47:42.326117: step 2694, loss 0.165191, acc 0.953125\n",
      "2018-04-11T15:47:42.849374: step 2695, loss 0.169669, acc 0.9375\n",
      "2018-04-11T15:47:43.386010: step 2696, loss 0.177615, acc 0.96875\n",
      "2018-04-11T15:47:43.918875: step 2697, loss 0.391814, acc 0.890625\n",
      "2018-04-11T15:47:44.433481: step 2698, loss 0.286015, acc 0.90625\n",
      "2018-04-11T15:47:44.953883: step 2699, loss 0.375299, acc 0.921875\n",
      "2018-04-11T15:47:45.487574: step 2700, loss 0.325967, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:47:45.652167: step 2700, loss 0.717046, acc 0.75\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-2700\n",
      "\n",
      "2018-04-11T15:47:46.299997: step 2701, loss 0.228749, acc 0.921875\n",
      "2018-04-11T15:47:46.830516: step 2702, loss 0.272063, acc 0.921875\n",
      "2018-04-11T15:47:47.353691: step 2703, loss 0.308588, acc 0.921875\n",
      "2018-04-11T15:47:47.866785: step 2704, loss 0.262325, acc 0.921875\n",
      "2018-04-11T15:47:48.405101: step 2705, loss 0.214953, acc 0.953125\n",
      "2018-04-11T15:47:48.939960: step 2706, loss 0.190843, acc 0.953125\n",
      "2018-04-11T15:47:49.458883: step 2707, loss 0.302492, acc 0.890625\n",
      "2018-04-11T15:47:49.996718: step 2708, loss 0.231402, acc 0.9375\n",
      "2018-04-11T15:47:50.524094: step 2709, loss 0.224731, acc 0.96875\n",
      "2018-04-11T15:47:51.059160: step 2710, loss 0.360811, acc 0.890625\n",
      "2018-04-11T15:47:51.594977: step 2711, loss 0.308511, acc 0.875\n",
      "2018-04-11T15:47:52.120903: step 2712, loss 0.202349, acc 0.953125\n",
      "2018-04-11T15:47:52.650431: step 2713, loss 0.178451, acc 0.9375\n",
      "2018-04-11T15:47:53.174451: step 2714, loss 0.253947, acc 0.9375\n",
      "2018-04-11T15:47:53.697204: step 2715, loss 0.340988, acc 0.953125\n",
      "2018-04-11T15:47:54.230340: step 2716, loss 0.251821, acc 0.921875\n",
      "2018-04-11T15:47:54.754159: step 2717, loss 0.265643, acc 0.90625\n",
      "2018-04-11T15:47:55.273489: step 2718, loss 0.173882, acc 0.953125\n",
      "2018-04-11T15:47:55.792151: step 2719, loss 0.11452, acc 0.984375\n",
      "2018-04-11T15:47:56.340895: step 2720, loss 0.179901, acc 0.9375\n",
      "2018-04-11T15:47:56.952800: step 2721, loss 0.21874, acc 0.921875\n",
      "2018-04-11T15:47:57.543711: step 2722, loss 0.247973, acc 0.921875\n",
      "2018-04-11T15:47:58.177420: step 2723, loss 0.186973, acc 0.921875\n",
      "2018-04-11T15:47:58.726074: step 2724, loss 0.209026, acc 0.953125\n",
      "2018-04-11T15:47:59.299698: step 2725, loss 0.233278, acc 0.953125\n",
      "2018-04-11T15:47:59.877230: step 2726, loss 0.283307, acc 0.90625\n",
      "2018-04-11T15:48:00.419760: step 2727, loss 0.316506, acc 0.890625\n",
      "2018-04-11T15:48:00.957087: step 2728, loss 0.133741, acc 0.984375\n",
      "2018-04-11T15:48:01.475188: step 2729, loss 0.323276, acc 0.890625\n",
      "2018-04-11T15:48:02.027680: step 2730, loss 0.214591, acc 0.921875\n",
      "2018-04-11T15:48:02.573748: step 2731, loss 0.347865, acc 0.875\n",
      "2018-04-11T15:48:03.115482: step 2732, loss 0.205161, acc 0.9375\n",
      "2018-04-11T15:48:03.643918: step 2733, loss 0.264074, acc 0.890625\n",
      "2018-04-11T15:48:04.195307: step 2734, loss 0.206662, acc 0.953125\n",
      "2018-04-11T15:48:04.732393: step 2735, loss 0.173488, acc 0.96875\n",
      "2018-04-11T15:48:05.275028: step 2736, loss 0.335819, acc 0.921875\n",
      "2018-04-11T15:48:05.818336: step 2737, loss 0.165773, acc 0.96875\n",
      "2018-04-11T15:48:06.359436: step 2738, loss 0.112946, acc 0.984375\n",
      "2018-04-11T15:48:06.893318: step 2739, loss 0.315815, acc 0.90625\n",
      "2018-04-11T15:48:07.428107: step 2740, loss 0.206478, acc 0.953125\n",
      "2018-04-11T15:48:07.968431: step 2741, loss 0.239527, acc 0.921875\n",
      "2018-04-11T15:48:08.520303: step 2742, loss 0.178177, acc 0.9375\n",
      "2018-04-11T15:48:09.238376: step 2743, loss 0.3105, acc 0.890625\n",
      "2018-04-11T15:48:09.890267: step 2744, loss 0.242355, acc 0.9375\n",
      "2018-04-11T15:48:10.438416: step 2745, loss 0.16469, acc 0.9375\n",
      "2018-04-11T15:48:10.992363: step 2746, loss 0.210444, acc 0.9375\n",
      "2018-04-11T15:48:11.554026: step 2747, loss 0.241125, acc 0.90625\n",
      "2018-04-11T15:48:12.085087: step 2748, loss 0.190834, acc 0.953125\n",
      "2018-04-11T15:48:12.621524: step 2749, loss 0.200971, acc 0.953125\n",
      "2018-04-11T15:48:13.171428: step 2750, loss 0.176049, acc 0.953125\n",
      "2018-04-11T15:48:13.693656: step 2751, loss 0.202808, acc 0.953125\n",
      "2018-04-11T15:48:14.251635: step 2752, loss 0.244771, acc 0.90625\n",
      "2018-04-11T15:48:14.800603: step 2753, loss 0.191725, acc 0.953125\n",
      "2018-04-11T15:48:15.346745: step 2754, loss 0.216091, acc 0.90625\n",
      "2018-04-11T15:48:15.893537: step 2755, loss 0.20353, acc 0.953125\n",
      "2018-04-11T15:48:16.467830: step 2756, loss 0.294602, acc 0.90625\n",
      "2018-04-11T15:48:17.011812: step 2757, loss 0.257378, acc 0.90625\n",
      "2018-04-11T15:48:17.728734: step 2758, loss 0.229035, acc 0.90625\n",
      "2018-04-11T15:48:18.404145: step 2759, loss 0.275183, acc 0.890625\n",
      "2018-04-11T15:48:18.985215: step 2760, loss 0.1658, acc 0.96875\n",
      "2018-04-11T15:48:19.595934: step 2761, loss 0.266802, acc 0.890625\n",
      "2018-04-11T15:48:20.189404: step 2762, loss 0.232351, acc 0.9375\n",
      "2018-04-11T15:48:20.733431: step 2763, loss 0.324423, acc 0.890625\n",
      "2018-04-11T15:48:21.274095: step 2764, loss 0.187434, acc 0.953125\n",
      "2018-04-11T15:48:21.831797: step 2765, loss 0.189911, acc 0.9375\n",
      "2018-04-11T15:48:22.407570: step 2766, loss 0.266955, acc 0.921875\n",
      "2018-04-11T15:48:23.003219: step 2767, loss 0.293456, acc 0.90625\n",
      "2018-04-11T15:48:23.559516: step 2768, loss 0.184149, acc 0.9375\n",
      "2018-04-11T15:48:24.112893: step 2769, loss 0.212609, acc 0.921875\n",
      "2018-04-11T15:48:24.681273: step 2770, loss 0.266667, acc 0.921875\n",
      "2018-04-11T15:48:25.243533: step 2771, loss 0.213266, acc 0.90625\n",
      "2018-04-11T15:48:25.798788: step 2772, loss 0.106553, acc 0.984375\n",
      "2018-04-11T15:48:26.383603: step 2773, loss 0.352659, acc 0.875\n",
      "2018-04-11T15:48:26.938472: step 2774, loss 0.250475, acc 0.890625\n",
      "2018-04-11T15:48:27.503500: step 2775, loss 0.223373, acc 0.9375\n",
      "2018-04-11T15:48:28.093862: step 2776, loss 0.286088, acc 0.90625\n",
      "2018-04-11T15:48:28.738134: step 2777, loss 0.213974, acc 0.921875\n",
      "2018-04-11T15:48:29.384787: step 2778, loss 0.25, acc 0.90625\n",
      "2018-04-11T15:48:29.963767: step 2779, loss 0.218143, acc 0.9375\n",
      "2018-04-11T15:48:30.511383: step 2780, loss 0.239478, acc 0.953125\n",
      "2018-04-11T15:48:31.062643: step 2781, loss 0.273426, acc 0.890625\n",
      "2018-04-11T15:48:31.722801: step 2782, loss 0.264619, acc 0.9375\n",
      "2018-04-11T15:48:32.323826: step 2783, loss 0.248264, acc 0.90625\n",
      "2018-04-11T15:48:32.925670: step 2784, loss 0.234506, acc 0.90625\n",
      "2018-04-11T15:48:33.514562: step 2785, loss 0.162912, acc 0.953125\n",
      "2018-04-11T15:48:34.094572: step 2786, loss 0.196713, acc 0.921875\n",
      "2018-04-11T15:48:34.753102: step 2787, loss 0.192773, acc 0.9375\n",
      "2018-04-11T15:48:35.328382: step 2788, loss 0.156637, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:48:35.954944: step 2789, loss 0.294678, acc 0.90625\n",
      "2018-04-11T15:48:36.650322: step 2790, loss 0.154075, acc 0.9375\n",
      "2018-04-11T15:48:37.313727: step 2791, loss 0.331418, acc 0.875\n",
      "2018-04-11T15:48:38.003682: step 2792, loss 0.270445, acc 0.90625\n",
      "2018-04-11T15:48:38.708397: step 2793, loss 0.130005, acc 0.96875\n",
      "2018-04-11T15:48:39.276925: step 2794, loss 0.257603, acc 0.953125\n",
      "2018-04-11T15:48:39.847619: step 2795, loss 0.152027, acc 0.96875\n",
      "2018-04-11T15:48:40.557768: step 2796, loss 0.201771, acc 0.9375\n",
      "2018-04-11T15:48:41.147661: step 2797, loss 0.27539, acc 0.875\n",
      "2018-04-11T15:48:41.835018: step 2798, loss 0.307853, acc 0.890625\n",
      "2018-04-11T15:48:42.403637: step 2799, loss 0.209119, acc 0.9375\n",
      "2018-04-11T15:48:42.966484: step 2800, loss 0.175828, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:48:43.152461: step 2800, loss 0.645688, acc 0.8\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-2800\n",
      "\n",
      "2018-04-11T15:48:43.876818: step 2801, loss 0.138452, acc 0.984375\n",
      "2018-04-11T15:48:44.439510: step 2802, loss 0.198571, acc 0.96875\n",
      "2018-04-11T15:48:44.997447: step 2803, loss 0.213364, acc 0.921875\n",
      "2018-04-11T15:48:45.586278: step 2804, loss 0.157585, acc 0.96875\n",
      "2018-04-11T15:48:46.181188: step 2805, loss 0.387328, acc 0.921875\n",
      "2018-04-11T15:48:46.757535: step 2806, loss 0.235459, acc 0.90625\n",
      "2018-04-11T15:48:47.344251: step 2807, loss 0.275526, acc 0.90625\n",
      "2018-04-11T15:48:47.906689: step 2808, loss 0.296589, acc 0.90625\n",
      "2018-04-11T15:48:48.490448: step 2809, loss 0.283555, acc 0.90625\n",
      "2018-04-11T15:48:49.071392: step 2810, loss 0.253228, acc 0.9375\n",
      "2018-04-11T15:48:49.639293: step 2811, loss 0.205498, acc 0.9375\n",
      "2018-04-11T15:48:50.210137: step 2812, loss 0.255184, acc 0.90625\n",
      "2018-04-11T15:48:50.780057: step 2813, loss 0.193019, acc 0.953125\n",
      "2018-04-11T15:48:51.369241: step 2814, loss 0.183619, acc 0.953125\n",
      "2018-04-11T15:48:51.946175: step 2815, loss 0.160855, acc 0.96875\n",
      "2018-04-11T15:48:52.536867: step 2816, loss 0.257177, acc 0.921875\n",
      "2018-04-11T15:48:53.135166: step 2817, loss 0.273696, acc 0.921875\n",
      "2018-04-11T15:48:53.735266: step 2818, loss 0.159788, acc 0.96875\n",
      "2018-04-11T15:48:54.338508: step 2819, loss 0.329876, acc 0.890625\n",
      "2018-04-11T15:48:54.914229: step 2820, loss 0.148604, acc 0.953125\n",
      "2018-04-11T15:48:55.488638: step 2821, loss 0.169921, acc 0.921875\n",
      "2018-04-11T15:48:56.110116: step 2822, loss 0.327803, acc 0.921875\n",
      "2018-04-11T15:48:56.709189: step 2823, loss 0.16225, acc 0.9375\n",
      "2018-04-11T15:48:57.282005: step 2824, loss 0.247576, acc 0.921875\n",
      "2018-04-11T15:48:57.984079: step 2825, loss 0.153212, acc 0.96875\n",
      "2018-04-11T15:48:58.673193: step 2826, loss 0.240297, acc 0.9375\n",
      "2018-04-11T15:48:59.239104: step 2827, loss 0.129819, acc 0.953125\n",
      "2018-04-11T15:48:59.805877: step 2828, loss 0.225013, acc 0.921875\n",
      "2018-04-11T15:49:00.522084: step 2829, loss 0.200726, acc 0.96875\n",
      "2018-04-11T15:49:01.096775: step 2830, loss 0.289715, acc 0.921875\n",
      "2018-04-11T15:49:01.685297: step 2831, loss 0.228221, acc 0.9375\n",
      "2018-04-11T15:49:02.380844: step 2832, loss 0.199125, acc 0.921875\n",
      "2018-04-11T15:49:03.012011: step 2833, loss 0.237592, acc 0.9375\n",
      "2018-04-11T15:49:03.640695: step 2834, loss 0.389446, acc 0.828125\n",
      "2018-04-11T15:49:04.318911: step 2835, loss 0.315269, acc 0.90625\n",
      "2018-04-11T15:49:04.979132: step 2836, loss 0.265668, acc 0.890625\n",
      "2018-04-11T15:49:05.733018: step 2837, loss 0.266078, acc 0.90625\n",
      "2018-04-11T15:49:06.427398: step 2838, loss 0.330714, acc 0.90625\n",
      "2018-04-11T15:49:07.202434: step 2839, loss 0.262075, acc 0.921875\n",
      "2018-04-11T15:49:07.909365: step 2840, loss 0.139194, acc 0.96875\n",
      "2018-04-11T15:49:08.569673: step 2841, loss 0.223339, acc 0.921875\n",
      "2018-04-11T15:49:09.259832: step 2842, loss 0.198086, acc 0.9375\n",
      "2018-04-11T15:49:10.009728: step 2843, loss 0.24126, acc 0.921875\n",
      "2018-04-11T15:49:10.683760: step 2844, loss 0.222089, acc 0.875\n",
      "2018-04-11T15:49:11.301637: step 2845, loss 0.360424, acc 0.859375\n",
      "2018-04-11T15:49:11.912183: step 2846, loss 0.3304, acc 0.90625\n",
      "2018-04-11T15:49:12.564861: step 2847, loss 0.194424, acc 0.953125\n",
      "2018-04-11T15:49:13.271667: step 2848, loss 0.208651, acc 0.921875\n",
      "2018-04-11T15:49:13.977375: step 2849, loss 0.317705, acc 0.90625\n",
      "2018-04-11T15:49:14.557166: step 2850, loss 0.301437, acc 0.90625\n",
      "2018-04-11T15:49:15.193308: step 2851, loss 0.259082, acc 0.921875\n",
      "2018-04-11T15:49:15.848912: step 2852, loss 0.30857, acc 0.890625\n",
      "2018-04-11T15:49:16.488291: step 2853, loss 0.250916, acc 0.90625\n",
      "2018-04-11T15:49:17.163672: step 2854, loss 0.284677, acc 0.890625\n",
      "2018-04-11T15:49:17.828509: step 2855, loss 0.252995, acc 0.921875\n",
      "2018-04-11T15:49:18.484861: step 2856, loss 0.221362, acc 0.921875\n",
      "2018-04-11T15:49:19.129055: step 2857, loss 0.181178, acc 0.921875\n",
      "2018-04-11T15:49:19.780886: step 2858, loss 0.200787, acc 0.953125\n",
      "2018-04-11T15:49:20.523907: step 2859, loss 0.269139, acc 0.90625\n",
      "2018-04-11T15:49:21.226790: step 2860, loss 0.306093, acc 0.90625\n",
      "2018-04-11T15:49:21.896179: step 2861, loss 0.262829, acc 0.90625\n",
      "2018-04-11T15:49:22.612423: step 2862, loss 0.327667, acc 0.890625\n",
      "2018-04-11T15:49:23.343161: step 2863, loss 0.172544, acc 0.9375\n",
      "2018-04-11T15:49:24.032376: step 2864, loss 0.329048, acc 0.828125\n",
      "2018-04-11T15:49:24.667045: step 2865, loss 0.214128, acc 0.9375\n",
      "2018-04-11T15:49:25.341878: step 2866, loss 0.184735, acc 0.96875\n",
      "2018-04-11T15:49:25.982294: step 2867, loss 0.191704, acc 0.9375\n",
      "2018-04-11T15:49:26.596041: step 2868, loss 0.275971, acc 0.921875\n",
      "2018-04-11T15:49:27.200302: step 2869, loss 0.137163, acc 0.984375\n",
      "2018-04-11T15:49:27.784967: step 2870, loss 0.252354, acc 0.9375\n",
      "2018-04-11T15:49:28.383726: step 2871, loss 0.283996, acc 0.875\n",
      "2018-04-11T15:49:28.978391: step 2872, loss 0.370614, acc 0.84375\n",
      "2018-04-11T15:49:29.581728: step 2873, loss 0.277117, acc 0.921875\n",
      "2018-04-11T15:49:30.189895: step 2874, loss 0.224663, acc 0.921875\n",
      "2018-04-11T15:49:30.789432: step 2875, loss 0.187308, acc 0.953125\n",
      "2018-04-11T15:49:31.382251: step 2876, loss 0.165595, acc 0.96875\n",
      "2018-04-11T15:49:31.985423: step 2877, loss 0.199149, acc 0.921875\n",
      "2018-04-11T15:49:32.569934: step 2878, loss 0.309391, acc 0.890625\n",
      "2018-04-11T15:49:33.157398: step 2879, loss 0.134395, acc 0.96875\n",
      "2018-04-11T15:49:33.752482: step 2880, loss 0.105901, acc 1\n",
      "2018-04-11T15:49:34.351818: step 2881, loss 0.191935, acc 0.953125\n",
      "2018-04-11T15:49:34.937806: step 2882, loss 0.155902, acc 0.9375\n",
      "2018-04-11T15:49:35.540185: step 2883, loss 0.288204, acc 0.90625\n",
      "2018-04-11T15:49:36.151851: step 2884, loss 0.172447, acc 0.953125\n",
      "2018-04-11T15:49:36.755875: step 2885, loss 0.348704, acc 0.921875\n",
      "2018-04-11T15:49:37.357737: step 2886, loss 0.485642, acc 0.828125\n",
      "2018-04-11T15:49:37.949639: step 2887, loss 0.269352, acc 0.90625\n",
      "2018-04-11T15:49:38.545489: step 2888, loss 0.243205, acc 0.90625\n",
      "2018-04-11T15:49:39.148362: step 2889, loss 0.172618, acc 0.953125\n",
      "2018-04-11T15:49:39.718210: step 2890, loss 0.299891, acc 0.890625\n",
      "2018-04-11T15:49:40.287308: step 2891, loss 0.247667, acc 0.90625\n",
      "2018-04-11T15:49:40.877176: step 2892, loss 0.252764, acc 0.875\n",
      "2018-04-11T15:49:41.452863: step 2893, loss 0.132899, acc 0.96875\n",
      "2018-04-11T15:49:42.054560: step 2894, loss 0.298063, acc 0.90625\n",
      "2018-04-11T15:49:42.649485: step 2895, loss 0.210346, acc 0.9375\n",
      "2018-04-11T15:49:43.230114: step 2896, loss 0.272586, acc 0.921875\n",
      "2018-04-11T15:49:43.801622: step 2897, loss 0.195013, acc 0.953125\n",
      "2018-04-11T15:49:44.399748: step 2898, loss 0.27883, acc 0.890625\n",
      "2018-04-11T15:49:44.952632: step 2899, loss 0.235787, acc 0.9375\n",
      "2018-04-11T15:49:45.549016: step 2900, loss 0.237836, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:49:45.729813: step 2900, loss 0.807147, acc 0.79\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-2900\n",
      "\n",
      "2018-04-11T15:49:46.470754: step 2901, loss 0.217297, acc 0.953125\n",
      "2018-04-11T15:49:47.100302: step 2902, loss 0.180365, acc 0.953125\n",
      "2018-04-11T15:49:47.685269: step 2903, loss 0.16881, acc 0.921875\n",
      "2018-04-11T15:49:48.263230: step 2904, loss 0.297427, acc 0.84375\n",
      "2018-04-11T15:49:48.860713: step 2905, loss 0.159126, acc 0.953125\n",
      "2018-04-11T15:49:49.451648: step 2906, loss 0.187796, acc 0.953125\n",
      "2018-04-11T15:49:50.082482: step 2907, loss 0.174846, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:49:50.673797: step 2908, loss 0.165287, acc 0.9375\n",
      "2018-04-11T15:49:51.271079: step 2909, loss 0.278231, acc 0.890625\n",
      "2018-04-11T15:49:51.876718: step 2910, loss 0.265687, acc 0.921875\n",
      "2018-04-11T15:49:52.465628: step 2911, loss 0.183439, acc 0.9375\n",
      "2018-04-11T15:49:53.017741: step 2912, loss 0.476348, acc 0.890625\n",
      "2018-04-11T15:49:53.635495: step 2913, loss 0.194493, acc 0.953125\n",
      "2018-04-11T15:49:54.232819: step 2914, loss 0.246759, acc 0.953125\n",
      "2018-04-11T15:49:54.785091: step 2915, loss 0.177872, acc 0.96875\n",
      "2018-04-11T15:49:55.388821: step 2916, loss 0.218726, acc 0.921875\n",
      "2018-04-11T15:49:56.014158: step 2917, loss 0.190633, acc 0.9375\n",
      "2018-04-11T15:49:56.641188: step 2918, loss 0.183744, acc 0.96875\n",
      "2018-04-11T15:49:57.242333: step 2919, loss 0.211007, acc 0.9375\n",
      "2018-04-11T15:49:57.836510: step 2920, loss 0.306078, acc 0.90625\n",
      "2018-04-11T15:49:58.456563: step 2921, loss 0.405859, acc 0.890625\n",
      "2018-04-11T15:49:59.094525: step 2922, loss 0.155048, acc 0.96875\n",
      "2018-04-11T15:49:59.693873: step 2923, loss 0.136597, acc 0.984375\n",
      "2018-04-11T15:50:00.379047: step 2924, loss 0.128624, acc 0.96875\n",
      "2018-04-11T15:50:01.094881: step 2925, loss 0.225599, acc 0.9375\n",
      "2018-04-11T15:50:01.807730: step 2926, loss 0.159918, acc 0.953125\n",
      "2018-04-11T15:50:02.435881: step 2927, loss 0.390327, acc 0.875\n",
      "2018-04-11T15:50:03.038956: step 2928, loss 0.195437, acc 0.9375\n",
      "2018-04-11T15:50:03.711643: step 2929, loss 0.372544, acc 0.875\n",
      "2018-04-11T15:50:04.400811: step 2930, loss 0.151646, acc 0.953125\n",
      "2018-04-11T15:50:05.000152: step 2931, loss 0.176949, acc 0.921875\n",
      "2018-04-11T15:50:05.601360: step 2932, loss 0.506795, acc 0.84375\n",
      "2018-04-11T15:50:06.215624: step 2933, loss 0.420902, acc 0.84375\n",
      "2018-04-11T15:50:06.860135: step 2934, loss 0.133649, acc 0.984375\n",
      "2018-04-11T15:50:07.498637: step 2935, loss 0.159604, acc 0.953125\n",
      "2018-04-11T15:50:08.135307: step 2936, loss 0.356691, acc 0.90625\n",
      "2018-04-11T15:50:08.739253: step 2937, loss 0.278406, acc 0.90625\n",
      "2018-04-11T15:50:09.356255: step 2938, loss 0.278988, acc 0.859375\n",
      "2018-04-11T15:50:09.981325: step 2939, loss 0.280494, acc 0.90625\n",
      "2018-04-11T15:50:10.598546: step 2940, loss 0.204185, acc 0.90625\n",
      "2018-04-11T15:50:11.194136: step 2941, loss 0.293418, acc 0.921875\n",
      "2018-04-11T15:50:11.771434: step 2942, loss 0.307069, acc 0.90625\n",
      "2018-04-11T15:50:12.366239: step 2943, loss 0.207487, acc 0.9375\n",
      "2018-04-11T15:50:13.009509: step 2944, loss 0.280722, acc 0.90625\n",
      "2018-04-11T15:50:13.602455: step 2945, loss 0.237803, acc 0.921875\n",
      "2018-04-11T15:50:14.314840: step 2946, loss 0.226874, acc 0.921875\n",
      "2018-04-11T15:50:15.058381: step 2947, loss 0.201908, acc 0.90625\n",
      "2018-04-11T15:50:15.869960: step 2948, loss 0.237468, acc 0.890625\n",
      "2018-04-11T15:50:16.625310: step 2949, loss 0.170797, acc 0.921875\n",
      "2018-04-11T15:50:17.292171: step 2950, loss 0.242129, acc 0.9375\n",
      "2018-04-11T15:50:18.023259: step 2951, loss 0.285116, acc 0.9375\n",
      "2018-04-11T15:50:18.801983: step 2952, loss 0.122662, acc 0.984375\n",
      "2018-04-11T15:50:19.621427: step 2953, loss 0.187785, acc 0.953125\n",
      "2018-04-11T15:50:20.416740: step 2954, loss 0.227155, acc 0.953125\n",
      "2018-04-11T15:50:21.152131: step 2955, loss 0.211997, acc 0.9375\n",
      "2018-04-11T15:50:21.817664: step 2956, loss 0.281623, acc 0.953125\n",
      "2018-04-11T15:50:22.503110: step 2957, loss 0.190925, acc 0.9375\n",
      "2018-04-11T15:50:23.101534: step 2958, loss 0.218897, acc 0.96875\n",
      "2018-04-11T15:50:23.685331: step 2959, loss 0.199242, acc 0.96875\n",
      "2018-04-11T15:50:24.386582: step 2960, loss 0.365879, acc 0.90625\n",
      "2018-04-11T15:50:25.139372: step 2961, loss 0.203687, acc 0.953125\n",
      "2018-04-11T15:50:25.740463: step 2962, loss 0.1414, acc 0.953125\n",
      "2018-04-11T15:50:26.324097: step 2963, loss 0.276727, acc 0.90625\n",
      "2018-04-11T15:50:26.926955: step 2964, loss 0.188176, acc 0.953125\n",
      "2018-04-11T15:50:27.521758: step 2965, loss 0.273249, acc 0.90625\n",
      "2018-04-11T15:50:28.115203: step 2966, loss 0.18142, acc 0.953125\n",
      "2018-04-11T15:50:28.711238: step 2967, loss 0.187692, acc 0.9375\n",
      "2018-04-11T15:50:29.357932: step 2968, loss 0.17502, acc 0.96875\n",
      "2018-04-11T15:50:29.939631: step 2969, loss 0.304845, acc 0.890625\n",
      "2018-04-11T15:50:30.511605: step 2970, loss 0.17679, acc 0.953125\n",
      "2018-04-11T15:50:31.086421: step 2971, loss 0.185783, acc 0.953125\n",
      "2018-04-11T15:50:31.721677: step 2972, loss 0.225998, acc 0.921875\n",
      "2018-04-11T15:50:32.343487: step 2973, loss 0.211509, acc 0.953125\n",
      "2018-04-11T15:50:32.945319: step 2974, loss 0.16782, acc 0.953125\n",
      "2018-04-11T15:50:33.563867: step 2975, loss 0.32399, acc 0.84375\n",
      "2018-04-11T15:50:34.178784: step 2976, loss 0.0925662, acc 1\n",
      "2018-04-11T15:50:34.752358: step 2977, loss 0.195162, acc 0.953125\n",
      "2018-04-11T15:50:35.352305: step 2978, loss 0.263014, acc 0.921875\n",
      "2018-04-11T15:50:36.022577: step 2979, loss 0.159983, acc 0.953125\n",
      "2018-04-11T15:50:36.616323: step 2980, loss 0.218371, acc 0.9375\n",
      "2018-04-11T15:50:37.232386: step 2981, loss 0.264396, acc 0.90625\n",
      "2018-04-11T15:50:37.805796: step 2982, loss 0.269575, acc 0.921875\n",
      "2018-04-11T15:50:38.395145: step 2983, loss 0.254472, acc 0.9375\n",
      "2018-04-11T15:50:39.021550: step 2984, loss 0.247796, acc 0.890625\n",
      "2018-04-11T15:50:39.606954: step 2985, loss 0.206333, acc 0.984375\n",
      "2018-04-11T15:50:40.198798: step 2986, loss 0.301794, acc 0.90625\n",
      "2018-04-11T15:50:40.797513: step 2987, loss 0.193928, acc 0.921875\n",
      "2018-04-11T15:50:41.464295: step 2988, loss 0.196494, acc 0.96875\n",
      "2018-04-11T15:50:42.048524: step 2989, loss 0.227326, acc 0.921875\n",
      "2018-04-11T15:50:42.646044: step 2990, loss 0.156591, acc 0.96875\n",
      "2018-04-11T15:50:43.276669: step 2991, loss 0.21889, acc 0.9375\n",
      "2018-04-11T15:50:43.922996: step 2992, loss 0.380767, acc 0.859375\n",
      "2018-04-11T15:50:44.561290: step 2993, loss 0.217709, acc 0.921875\n",
      "2018-04-11T15:50:45.201894: step 2994, loss 0.198751, acc 0.9375\n",
      "2018-04-11T15:50:45.812851: step 2995, loss 0.276491, acc 0.9375\n",
      "2018-04-11T15:50:46.409999: step 2996, loss 0.112392, acc 0.984375\n",
      "2018-04-11T15:50:47.008672: step 2997, loss 0.186177, acc 0.953125\n",
      "2018-04-11T15:50:47.621148: step 2998, loss 0.288557, acc 0.90625\n",
      "2018-04-11T15:50:48.244522: step 2999, loss 0.210033, acc 0.921875\n",
      "2018-04-11T15:50:48.897764: step 3000, loss 0.311526, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:50:49.078350: step 3000, loss 0.655682, acc 0.82\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-3000\n",
      "\n",
      "2018-04-11T15:50:49.796260: step 3001, loss 0.197848, acc 0.953125\n",
      "2018-04-11T15:50:50.443021: step 3002, loss 0.282272, acc 0.890625\n",
      "2018-04-11T15:50:51.046297: step 3003, loss 0.308492, acc 0.875\n",
      "2018-04-11T15:50:51.648336: step 3004, loss 0.33588, acc 0.890625\n",
      "2018-04-11T15:50:52.301437: step 3005, loss 0.193272, acc 0.9375\n",
      "2018-04-11T15:50:52.902911: step 3006, loss 0.443259, acc 0.84375\n",
      "2018-04-11T15:50:53.534145: step 3007, loss 0.165776, acc 0.953125\n",
      "2018-04-11T15:50:54.235717: step 3008, loss 0.321298, acc 0.921875\n",
      "2018-04-11T15:50:54.847086: step 3009, loss 0.208183, acc 0.953125\n",
      "2018-04-11T15:50:55.430918: step 3010, loss 0.21562, acc 0.9375\n",
      "2018-04-11T15:50:56.036035: step 3011, loss 0.304528, acc 0.921875\n",
      "2018-04-11T15:50:56.638087: step 3012, loss 0.367006, acc 0.875\n",
      "2018-04-11T15:50:57.225917: step 3013, loss 0.190471, acc 0.9375\n",
      "2018-04-11T15:50:57.828332: step 3014, loss 0.231321, acc 0.921875\n",
      "2018-04-11T15:50:58.483082: step 3015, loss 0.325124, acc 0.859375\n",
      "2018-04-11T15:50:59.060953: step 3016, loss 0.229581, acc 0.9375\n",
      "2018-04-11T15:50:59.723434: step 3017, loss 0.268725, acc 0.859375\n",
      "2018-04-11T15:51:00.331560: step 3018, loss 0.150096, acc 0.96875\n",
      "2018-04-11T15:51:00.912528: step 3019, loss 0.126149, acc 0.96875\n",
      "2018-04-11T15:51:01.543971: step 3020, loss 0.253312, acc 0.90625\n",
      "2018-04-11T15:51:02.132790: step 3021, loss 0.20048, acc 0.9375\n",
      "2018-04-11T15:51:02.720013: step 3022, loss 0.275853, acc 0.921875\n",
      "2018-04-11T15:51:03.310402: step 3023, loss 0.176171, acc 0.953125\n",
      "2018-04-11T15:51:03.888338: step 3024, loss 0.247592, acc 0.9375\n",
      "2018-04-11T15:51:04.495291: step 3025, loss 0.221976, acc 0.953125\n",
      "2018-04-11T15:51:05.119384: step 3026, loss 0.113167, acc 0.953125\n",
      "2018-04-11T15:51:05.702848: step 3027, loss 0.292784, acc 0.90625\n",
      "2018-04-11T15:51:06.309175: step 3028, loss 0.174519, acc 0.953125\n",
      "2018-04-11T15:51:06.901082: step 3029, loss 0.138394, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:51:07.500670: step 3030, loss 0.268647, acc 0.90625\n",
      "2018-04-11T15:51:08.113646: step 3031, loss 0.171525, acc 0.96875\n",
      "2018-04-11T15:51:08.729402: step 3032, loss 0.277962, acc 0.875\n",
      "2018-04-11T15:51:09.321993: step 3033, loss 0.302684, acc 0.90625\n",
      "2018-04-11T15:51:09.967904: step 3034, loss 0.176095, acc 0.984375\n",
      "2018-04-11T15:51:10.623780: step 3035, loss 0.3451, acc 0.859375\n",
      "2018-04-11T15:51:11.187536: step 3036, loss 0.378283, acc 0.890625\n",
      "2018-04-11T15:51:11.796530: step 3037, loss 0.167058, acc 0.953125\n",
      "2018-04-11T15:51:12.459474: step 3038, loss 0.195108, acc 0.9375\n",
      "2018-04-11T15:51:13.094298: step 3039, loss 0.248798, acc 0.9375\n",
      "2018-04-11T15:51:13.745118: step 3040, loss 0.253225, acc 0.890625\n",
      "2018-04-11T15:51:14.404545: step 3041, loss 0.113573, acc 0.984375\n",
      "2018-04-11T15:51:15.001318: step 3042, loss 0.352034, acc 0.875\n",
      "2018-04-11T15:51:15.612348: step 3043, loss 0.138441, acc 0.96875\n",
      "2018-04-11T15:51:16.234542: step 3044, loss 0.226096, acc 0.921875\n",
      "2018-04-11T15:51:16.853549: step 3045, loss 0.125434, acc 0.984375\n",
      "2018-04-11T15:51:17.489499: step 3046, loss 0.192371, acc 0.9375\n",
      "2018-04-11T15:51:18.097237: step 3047, loss 0.335505, acc 0.921875\n",
      "2018-04-11T15:51:18.710374: step 3048, loss 0.232512, acc 0.921875\n",
      "2018-04-11T15:51:19.321209: step 3049, loss 0.350519, acc 0.890625\n",
      "2018-04-11T15:51:19.896868: step 3050, loss 0.230092, acc 0.921875\n",
      "2018-04-11T15:51:20.517379: step 3051, loss 0.16804, acc 0.96875\n",
      "2018-04-11T15:51:21.103494: step 3052, loss 0.148064, acc 0.984375\n",
      "2018-04-11T15:51:21.690934: step 3053, loss 0.24911, acc 0.90625\n",
      "2018-04-11T15:51:22.259201: step 3054, loss 0.334558, acc 0.875\n",
      "2018-04-11T15:51:22.843921: step 3055, loss 0.262121, acc 0.921875\n",
      "2018-04-11T15:51:23.425658: step 3056, loss 0.2593, acc 0.890625\n",
      "2018-04-11T15:51:24.010491: step 3057, loss 0.185272, acc 0.953125\n",
      "2018-04-11T15:51:24.575876: step 3058, loss 0.208061, acc 0.921875\n",
      "2018-04-11T15:51:25.144727: step 3059, loss 0.209103, acc 0.90625\n",
      "2018-04-11T15:51:25.707596: step 3060, loss 0.109925, acc 0.984375\n",
      "2018-04-11T15:51:26.386251: step 3061, loss 0.216554, acc 0.9375\n",
      "2018-04-11T15:51:27.013940: step 3062, loss 0.285778, acc 0.9375\n",
      "2018-04-11T15:51:27.613752: step 3063, loss 0.23599, acc 0.921875\n",
      "2018-04-11T15:51:28.197993: step 3064, loss 0.283452, acc 0.90625\n",
      "2018-04-11T15:51:28.803168: step 3065, loss 0.292959, acc 0.921875\n",
      "2018-04-11T15:51:29.409590: step 3066, loss 0.326256, acc 0.90625\n",
      "2018-04-11T15:51:30.011355: step 3067, loss 0.150519, acc 0.953125\n",
      "2018-04-11T15:51:30.577035: step 3068, loss 0.252984, acc 0.9375\n",
      "2018-04-11T15:51:31.147135: step 3069, loss 0.210198, acc 0.9375\n",
      "2018-04-11T15:51:31.732256: step 3070, loss 0.263178, acc 0.875\n",
      "2018-04-11T15:51:32.306700: step 3071, loss 0.249473, acc 0.953125\n",
      "2018-04-11T15:51:32.878963: step 3072, loss 0.212908, acc 0.921875\n",
      "2018-04-11T15:51:33.472065: step 3073, loss 0.20884, acc 0.953125\n",
      "2018-04-11T15:51:34.040241: step 3074, loss 0.426103, acc 0.875\n",
      "2018-04-11T15:51:34.627548: step 3075, loss 0.162766, acc 0.9375\n",
      "2018-04-11T15:51:35.205138: step 3076, loss 0.34295, acc 0.875\n",
      "2018-04-11T15:51:35.785112: step 3077, loss 0.265027, acc 0.90625\n",
      "2018-04-11T15:51:36.018328: step 3078, loss 0.676032, acc 0.666667\n",
      "2018-04-11T15:51:36.600686: step 3079, loss 0.150268, acc 0.96875\n",
      "2018-04-11T15:51:37.166551: step 3080, loss 0.141865, acc 0.96875\n",
      "2018-04-11T15:51:37.748819: step 3081, loss 0.149857, acc 0.984375\n",
      "2018-04-11T15:51:38.350021: step 3082, loss 0.208097, acc 0.9375\n",
      "2018-04-11T15:51:38.919651: step 3083, loss 0.18923, acc 0.90625\n",
      "2018-04-11T15:51:39.490792: step 3084, loss 0.115338, acc 0.96875\n",
      "2018-04-11T15:51:40.078301: step 3085, loss 0.244602, acc 0.890625\n",
      "2018-04-11T15:51:40.657485: step 3086, loss 0.32148, acc 0.875\n",
      "2018-04-11T15:51:41.226976: step 3087, loss 0.247049, acc 0.90625\n",
      "2018-04-11T15:51:41.798799: step 3088, loss 0.141192, acc 0.96875\n",
      "2018-04-11T15:51:42.402780: step 3089, loss 0.276645, acc 0.890625\n",
      "2018-04-11T15:51:42.993695: step 3090, loss 0.188933, acc 0.953125\n",
      "2018-04-11T15:51:43.594983: step 3091, loss 0.148326, acc 0.96875\n",
      "2018-04-11T15:51:44.160533: step 3092, loss 0.115806, acc 1\n",
      "2018-04-11T15:51:44.740210: step 3093, loss 0.258884, acc 0.875\n",
      "2018-04-11T15:51:45.381255: step 3094, loss 0.215813, acc 0.953125\n",
      "2018-04-11T15:51:45.975318: step 3095, loss 0.23145, acc 0.890625\n",
      "2018-04-11T15:51:46.556216: step 3096, loss 0.126044, acc 0.984375\n",
      "2018-04-11T15:51:47.197307: step 3097, loss 0.170048, acc 0.9375\n",
      "2018-04-11T15:51:47.830828: step 3098, loss 0.220167, acc 0.984375\n",
      "2018-04-11T15:51:48.475349: step 3099, loss 0.142006, acc 0.96875\n",
      "2018-04-11T15:51:49.180850: step 3100, loss 0.217195, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:51:49.407508: step 3100, loss 0.683083, acc 0.78\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-3100\n",
      "\n",
      "2018-04-11T15:51:50.224163: step 3101, loss 0.202622, acc 0.9375\n",
      "2018-04-11T15:51:50.907532: step 3102, loss 0.123401, acc 0.96875\n",
      "2018-04-11T15:51:51.573128: step 3103, loss 0.172649, acc 0.96875\n",
      "2018-04-11T15:51:52.307330: step 3104, loss 0.227879, acc 0.90625\n",
      "2018-04-11T15:51:53.043454: step 3105, loss 0.190981, acc 0.953125\n",
      "2018-04-11T15:51:53.794619: step 3106, loss 0.233578, acc 0.9375\n",
      "2018-04-11T15:51:54.521119: step 3107, loss 0.223415, acc 0.90625\n",
      "2018-04-11T15:51:55.208778: step 3108, loss 0.0681651, acc 0.984375\n",
      "2018-04-11T15:51:55.805967: step 3109, loss 0.205807, acc 0.953125\n",
      "2018-04-11T15:51:56.470478: step 3110, loss 0.145292, acc 0.953125\n",
      "2018-04-11T15:51:57.118749: step 3111, loss 0.345797, acc 0.859375\n",
      "2018-04-11T15:51:57.767495: step 3112, loss 0.0862451, acc 0.984375\n",
      "2018-04-11T15:51:58.424135: step 3113, loss 0.166629, acc 0.953125\n",
      "2018-04-11T15:51:59.080150: step 3114, loss 0.21332, acc 0.90625\n",
      "2018-04-11T15:51:59.775963: step 3115, loss 0.0933731, acc 1\n",
      "2018-04-11T15:52:00.489436: step 3116, loss 0.211367, acc 0.921875\n",
      "2018-04-11T15:52:01.218637: step 3117, loss 0.114849, acc 1\n",
      "2018-04-11T15:52:01.920382: step 3118, loss 0.124235, acc 0.984375\n",
      "2018-04-11T15:52:02.609298: step 3119, loss 0.178024, acc 0.953125\n",
      "2018-04-11T15:52:03.270641: step 3120, loss 0.230202, acc 0.953125\n",
      "2018-04-11T15:52:03.863653: step 3121, loss 0.127371, acc 0.96875\n",
      "2018-04-11T15:52:04.581283: step 3122, loss 0.116672, acc 0.96875\n",
      "2018-04-11T15:52:05.288053: step 3123, loss 0.132304, acc 0.96875\n",
      "2018-04-11T15:52:05.940946: step 3124, loss 0.162105, acc 0.96875\n",
      "2018-04-11T15:52:06.601543: step 3125, loss 0.249349, acc 0.9375\n",
      "2018-04-11T15:52:07.192592: step 3126, loss 0.200949, acc 0.9375\n",
      "2018-04-11T15:52:07.791648: step 3127, loss 0.233013, acc 0.921875\n",
      "2018-04-11T15:52:08.444616: step 3128, loss 0.241067, acc 0.921875\n",
      "2018-04-11T15:52:09.106214: step 3129, loss 0.151273, acc 0.953125\n",
      "2018-04-11T15:52:09.738105: step 3130, loss 0.0961385, acc 0.984375\n",
      "2018-04-11T15:52:10.438399: step 3131, loss 0.163086, acc 0.984375\n",
      "2018-04-11T15:52:11.026388: step 3132, loss 0.152927, acc 0.953125\n",
      "2018-04-11T15:52:11.680806: step 3133, loss 0.118156, acc 0.96875\n",
      "2018-04-11T15:52:12.294090: step 3134, loss 0.192047, acc 0.9375\n",
      "2018-04-11T15:52:12.919969: step 3135, loss 0.169263, acc 0.9375\n",
      "2018-04-11T15:52:13.664827: step 3136, loss 0.184878, acc 0.953125\n",
      "2018-04-11T15:52:14.359575: step 3137, loss 0.17435, acc 0.9375\n",
      "2018-04-11T15:52:15.038567: step 3138, loss 0.295358, acc 0.90625\n",
      "2018-04-11T15:52:15.622785: step 3139, loss 0.166556, acc 0.953125\n",
      "2018-04-11T15:52:16.226629: step 3140, loss 0.241946, acc 0.90625\n",
      "2018-04-11T15:52:16.829452: step 3141, loss 0.103417, acc 0.984375\n",
      "2018-04-11T15:52:17.465484: step 3142, loss 0.124661, acc 0.96875\n",
      "2018-04-11T15:52:18.157320: step 3143, loss 0.203044, acc 0.90625\n",
      "2018-04-11T15:52:18.845914: step 3144, loss 0.175389, acc 0.96875\n",
      "2018-04-11T15:52:19.509715: step 3145, loss 0.208506, acc 0.921875\n",
      "2018-04-11T15:52:20.195903: step 3146, loss 0.172612, acc 0.9375\n",
      "2018-04-11T15:52:20.804078: step 3147, loss 0.182348, acc 0.9375\n",
      "2018-04-11T15:52:21.434528: step 3148, loss 0.16709, acc 0.953125\n",
      "2018-04-11T15:52:22.080342: step 3149, loss 0.247322, acc 0.90625\n",
      "2018-04-11T15:52:22.728841: step 3150, loss 0.18407, acc 0.953125\n",
      "2018-04-11T15:52:23.347120: step 3151, loss 0.165044, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:52:23.982762: step 3152, loss 0.132822, acc 0.96875\n",
      "2018-04-11T15:52:24.636903: step 3153, loss 0.177092, acc 0.953125\n",
      "2018-04-11T15:52:25.288474: step 3154, loss 0.182179, acc 0.9375\n",
      "2018-04-11T15:52:25.974560: step 3155, loss 0.0979575, acc 0.96875\n",
      "2018-04-11T15:52:26.631216: step 3156, loss 0.119622, acc 0.984375\n",
      "2018-04-11T15:52:27.285376: step 3157, loss 0.214855, acc 0.921875\n",
      "2018-04-11T15:52:27.928168: step 3158, loss 0.190924, acc 0.953125\n",
      "2018-04-11T15:52:28.582504: step 3159, loss 0.275703, acc 0.9375\n",
      "2018-04-11T15:52:29.232706: step 3160, loss 0.325206, acc 0.9375\n",
      "2018-04-11T15:52:29.823058: step 3161, loss 0.276008, acc 0.921875\n",
      "2018-04-11T15:52:30.413359: step 3162, loss 0.195262, acc 0.9375\n",
      "2018-04-11T15:52:31.028903: step 3163, loss 0.20278, acc 0.953125\n",
      "2018-04-11T15:52:31.655462: step 3164, loss 0.319363, acc 0.890625\n",
      "2018-04-11T15:52:32.272384: step 3165, loss 0.109194, acc 0.96875\n",
      "2018-04-11T15:52:32.876700: step 3166, loss 0.123923, acc 0.96875\n",
      "2018-04-11T15:52:33.478678: step 3167, loss 0.29073, acc 0.890625\n",
      "2018-04-11T15:52:34.075432: step 3168, loss 0.244771, acc 0.875\n",
      "2018-04-11T15:52:34.678782: step 3169, loss 0.295789, acc 0.921875\n",
      "2018-04-11T15:52:35.256309: step 3170, loss 0.141614, acc 0.96875\n",
      "2018-04-11T15:52:35.855169: step 3171, loss 0.150995, acc 0.96875\n",
      "2018-04-11T15:52:36.445484: step 3172, loss 0.208212, acc 0.953125\n",
      "2018-04-11T15:52:37.052252: step 3173, loss 0.290272, acc 0.9375\n",
      "2018-04-11T15:52:37.698033: step 3174, loss 0.205523, acc 0.9375\n",
      "2018-04-11T15:52:38.363210: step 3175, loss 0.20364, acc 0.9375\n",
      "2018-04-11T15:52:38.988422: step 3176, loss 0.251074, acc 0.875\n",
      "2018-04-11T15:52:39.666412: step 3177, loss 0.267507, acc 0.890625\n",
      "2018-04-11T15:52:40.313860: step 3178, loss 0.33859, acc 0.890625\n",
      "2018-04-11T15:52:41.040880: step 3179, loss 0.197521, acc 0.9375\n",
      "2018-04-11T15:52:41.627518: step 3180, loss 0.200264, acc 0.921875\n",
      "2018-04-11T15:52:42.293173: step 3181, loss 0.2766, acc 0.9375\n",
      "2018-04-11T15:52:42.892697: step 3182, loss 0.148777, acc 0.96875\n",
      "2018-04-11T15:52:43.527412: step 3183, loss 0.187487, acc 0.96875\n",
      "2018-04-11T15:52:44.150142: step 3184, loss 0.206171, acc 0.921875\n",
      "2018-04-11T15:52:44.850530: step 3185, loss 0.178276, acc 0.953125\n",
      "2018-04-11T15:52:45.580783: step 3186, loss 0.223324, acc 0.90625\n",
      "2018-04-11T15:52:46.202303: step 3187, loss 0.176289, acc 0.96875\n",
      "2018-04-11T15:52:46.803931: step 3188, loss 0.141226, acc 0.96875\n",
      "2018-04-11T15:52:47.494473: step 3189, loss 0.16622, acc 0.953125\n",
      "2018-04-11T15:52:48.107016: step 3190, loss 0.149864, acc 0.953125\n",
      "2018-04-11T15:52:48.757842: step 3191, loss 0.160466, acc 0.953125\n",
      "2018-04-11T15:52:49.426978: step 3192, loss 0.219742, acc 0.9375\n",
      "2018-04-11T15:52:50.145544: step 3193, loss 0.215788, acc 0.9375\n",
      "2018-04-11T15:52:50.751126: step 3194, loss 0.148995, acc 0.9375\n",
      "2018-04-11T15:52:51.385876: step 3195, loss 0.111677, acc 0.984375\n",
      "2018-04-11T15:52:52.001374: step 3196, loss 0.136316, acc 0.953125\n",
      "2018-04-11T15:52:52.606898: step 3197, loss 0.282004, acc 0.921875\n",
      "2018-04-11T15:52:53.237864: step 3198, loss 0.134025, acc 0.96875\n",
      "2018-04-11T15:52:53.920055: step 3199, loss 0.294342, acc 0.875\n",
      "2018-04-11T15:52:54.507587: step 3200, loss 0.175814, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:52:54.692833: step 3200, loss 0.621427, acc 0.84\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-3200\n",
      "\n",
      "2018-04-11T15:52:55.535325: step 3201, loss 0.182617, acc 0.953125\n",
      "2018-04-11T15:52:56.228397: step 3202, loss 0.119535, acc 0.96875\n",
      "2018-04-11T15:52:56.946014: step 3203, loss 0.254104, acc 0.953125\n",
      "2018-04-11T15:52:57.563442: step 3204, loss 0.142766, acc 1\n",
      "2018-04-11T15:52:58.152146: step 3205, loss 0.130603, acc 0.984375\n",
      "2018-04-11T15:52:58.746368: step 3206, loss 0.163393, acc 0.96875\n",
      "2018-04-11T15:52:59.389892: step 3207, loss 0.23362, acc 0.921875\n",
      "2018-04-11T15:53:00.070497: step 3208, loss 0.121638, acc 0.984375\n",
      "2018-04-11T15:53:00.728034: step 3209, loss 0.213437, acc 0.90625\n",
      "2018-04-11T15:53:01.383317: step 3210, loss 0.218196, acc 0.9375\n",
      "2018-04-11T15:53:02.007879: step 3211, loss 0.218037, acc 0.9375\n",
      "2018-04-11T15:53:02.687416: step 3212, loss 0.220172, acc 0.953125\n",
      "2018-04-11T15:53:03.411168: step 3213, loss 0.183605, acc 0.953125\n",
      "2018-04-11T15:53:04.062157: step 3214, loss 0.258738, acc 0.90625\n",
      "2018-04-11T15:53:04.796672: step 3215, loss 0.240306, acc 0.90625\n",
      "2018-04-11T15:53:05.526993: step 3216, loss 0.149299, acc 0.953125\n",
      "2018-04-11T15:53:06.157101: step 3217, loss 0.231844, acc 0.9375\n",
      "2018-04-11T15:53:06.745951: step 3218, loss 0.322619, acc 0.875\n",
      "2018-04-11T15:53:07.338242: step 3219, loss 0.115647, acc 0.96875\n",
      "2018-04-11T15:53:07.964032: step 3220, loss 0.131761, acc 0.96875\n",
      "2018-04-11T15:53:08.651148: step 3221, loss 0.223679, acc 0.953125\n",
      "2018-04-11T15:53:09.224935: step 3222, loss 0.181601, acc 0.96875\n",
      "2018-04-11T15:53:09.814850: step 3223, loss 0.148213, acc 0.953125\n",
      "2018-04-11T15:53:10.412231: step 3224, loss 0.114582, acc 0.96875\n",
      "2018-04-11T15:53:10.997331: step 3225, loss 0.142908, acc 0.96875\n",
      "2018-04-11T15:53:11.566132: step 3226, loss 0.298647, acc 0.890625\n",
      "2018-04-11T15:53:12.146256: step 3227, loss 0.180256, acc 0.953125\n",
      "2018-04-11T15:53:12.739865: step 3228, loss 0.203624, acc 0.9375\n",
      "2018-04-11T15:53:13.305573: step 3229, loss 0.192476, acc 0.921875\n",
      "2018-04-11T15:53:13.877699: step 3230, loss 0.293435, acc 0.921875\n",
      "2018-04-11T15:53:14.476925: step 3231, loss 0.209058, acc 0.953125\n",
      "2018-04-11T15:53:15.042089: step 3232, loss 0.131797, acc 0.953125\n",
      "2018-04-11T15:53:15.635772: step 3233, loss 0.197995, acc 0.953125\n",
      "2018-04-11T15:53:16.201688: step 3234, loss 0.226964, acc 0.921875\n",
      "2018-04-11T15:53:16.768107: step 3235, loss 0.191728, acc 0.9375\n",
      "2018-04-11T15:53:17.352137: step 3236, loss 0.193485, acc 0.9375\n",
      "2018-04-11T15:53:17.931261: step 3237, loss 0.176587, acc 0.953125\n",
      "2018-04-11T15:53:18.496870: step 3238, loss 0.17331, acc 0.984375\n",
      "2018-04-11T15:53:19.066748: step 3239, loss 0.260174, acc 0.90625\n",
      "2018-04-11T15:53:19.657699: step 3240, loss 0.144281, acc 0.96875\n",
      "2018-04-11T15:53:20.293386: step 3241, loss 0.184854, acc 0.953125\n",
      "2018-04-11T15:53:20.912753: step 3242, loss 0.114851, acc 0.96875\n",
      "2018-04-11T15:53:21.560554: step 3243, loss 0.105019, acc 1\n",
      "2018-04-11T15:53:22.235914: step 3244, loss 0.375848, acc 0.921875\n",
      "2018-04-11T15:53:22.911045: step 3245, loss 0.163949, acc 0.9375\n",
      "2018-04-11T15:53:23.553088: step 3246, loss 0.327859, acc 0.84375\n",
      "2018-04-11T15:53:24.245745: step 3247, loss 0.0997026, acc 0.984375\n",
      "2018-04-11T15:53:24.891748: step 3248, loss 0.139721, acc 0.96875\n",
      "2018-04-11T15:53:25.604422: step 3249, loss 0.133374, acc 0.96875\n",
      "2018-04-11T15:53:26.244057: step 3250, loss 0.245368, acc 0.953125\n",
      "2018-04-11T15:53:26.882471: step 3251, loss 0.219429, acc 0.90625\n",
      "2018-04-11T15:53:27.520296: step 3252, loss 0.162071, acc 0.9375\n",
      "2018-04-11T15:53:28.163086: step 3253, loss 0.160369, acc 0.96875\n",
      "2018-04-11T15:53:28.811430: step 3254, loss 0.210288, acc 0.9375\n",
      "2018-04-11T15:53:29.415550: step 3255, loss 0.21249, acc 0.90625\n",
      "2018-04-11T15:53:30.042689: step 3256, loss 0.204945, acc 0.953125\n",
      "2018-04-11T15:53:30.685494: step 3257, loss 0.140994, acc 0.96875\n",
      "2018-04-11T15:53:31.309003: step 3258, loss 0.175043, acc 0.984375\n",
      "2018-04-11T15:53:31.950308: step 3259, loss 0.264286, acc 0.921875\n",
      "2018-04-11T15:53:32.597902: step 3260, loss 0.160369, acc 0.953125\n",
      "2018-04-11T15:53:33.208886: step 3261, loss 0.181217, acc 0.953125\n",
      "2018-04-11T15:53:33.836751: step 3262, loss 0.111623, acc 0.984375\n",
      "2018-04-11T15:53:34.530642: step 3263, loss 0.154438, acc 0.984375\n",
      "2018-04-11T15:53:35.140680: step 3264, loss 0.0896714, acc 1\n",
      "2018-04-11T15:53:35.814739: step 3265, loss 0.117647, acc 0.96875\n",
      "2018-04-11T15:53:36.475560: step 3266, loss 0.144694, acc 0.96875\n",
      "2018-04-11T15:53:37.126049: step 3267, loss 0.23818, acc 0.90625\n",
      "2018-04-11T15:53:37.765367: step 3268, loss 0.286824, acc 0.921875\n",
      "2018-04-11T15:53:38.498593: step 3269, loss 0.119505, acc 0.96875\n",
      "2018-04-11T15:53:39.282962: step 3270, loss 0.209416, acc 0.9375\n",
      "2018-04-11T15:53:40.049806: step 3271, loss 0.181198, acc 0.96875\n",
      "2018-04-11T15:53:40.813247: step 3272, loss 0.199498, acc 0.953125\n",
      "2018-04-11T15:53:41.575482: step 3273, loss 0.169875, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:53:42.254441: step 3274, loss 0.177138, acc 0.953125\n",
      "2018-04-11T15:53:42.929487: step 3275, loss 0.134382, acc 0.96875\n",
      "2018-04-11T15:53:43.690940: step 3276, loss 0.276608, acc 0.875\n",
      "2018-04-11T15:53:44.453103: step 3277, loss 0.14104, acc 0.96875\n",
      "2018-04-11T15:53:45.224745: step 3278, loss 0.17258, acc 0.953125\n",
      "2018-04-11T15:53:45.962698: step 3279, loss 0.156143, acc 0.96875\n",
      "2018-04-11T15:53:46.641515: step 3280, loss 0.153752, acc 0.96875\n",
      "2018-04-11T15:53:47.318944: step 3281, loss 0.142545, acc 0.96875\n",
      "2018-04-11T15:53:47.978076: step 3282, loss 0.157392, acc 0.953125\n",
      "2018-04-11T15:53:48.706868: step 3283, loss 0.244869, acc 0.90625\n",
      "2018-04-11T15:53:49.337346: step 3284, loss 0.224795, acc 0.921875\n",
      "2018-04-11T15:53:49.988673: step 3285, loss 0.105821, acc 1\n",
      "2018-04-11T15:53:50.721199: step 3286, loss 0.242999, acc 0.890625\n",
      "2018-04-11T15:53:51.317791: step 3287, loss 0.154238, acc 0.953125\n",
      "2018-04-11T15:53:51.966873: step 3288, loss 0.121708, acc 0.96875\n",
      "2018-04-11T15:53:52.751351: step 3289, loss 0.135624, acc 0.96875\n",
      "2018-04-11T15:53:53.417313: step 3290, loss 0.164039, acc 0.96875\n",
      "2018-04-11T15:53:54.053730: step 3291, loss 0.248244, acc 0.890625\n",
      "2018-04-11T15:53:54.773527: step 3292, loss 0.164098, acc 0.96875\n",
      "2018-04-11T15:53:55.357516: step 3293, loss 0.253819, acc 0.921875\n",
      "2018-04-11T15:53:56.026748: step 3294, loss 0.13508, acc 0.984375\n",
      "2018-04-11T15:53:56.752248: step 3295, loss 0.222605, acc 0.9375\n",
      "2018-04-11T15:53:57.528654: step 3296, loss 0.148999, acc 0.953125\n",
      "2018-04-11T15:53:58.253137: step 3297, loss 0.247312, acc 0.9375\n",
      "2018-04-11T15:53:58.975646: step 3298, loss 0.23329, acc 0.9375\n",
      "2018-04-11T15:53:59.725097: step 3299, loss 0.14671, acc 0.953125\n",
      "2018-04-11T15:54:00.513893: step 3300, loss 0.184055, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:54:00.770208: step 3300, loss 0.853661, acc 0.77\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-3300\n",
      "\n",
      "2018-04-11T15:54:01.712706: step 3301, loss 0.1841, acc 0.9375\n",
      "2018-04-11T15:54:02.386600: step 3302, loss 0.157761, acc 0.96875\n",
      "2018-04-11T15:54:03.047311: step 3303, loss 0.12907, acc 0.96875\n",
      "2018-04-11T15:54:03.671156: step 3304, loss 0.241795, acc 0.921875\n",
      "2018-04-11T15:54:04.294702: step 3305, loss 0.137194, acc 0.984375\n",
      "2018-04-11T15:54:04.999577: step 3306, loss 0.213908, acc 0.953125\n",
      "2018-04-11T15:54:05.599745: step 3307, loss 0.113893, acc 0.953125\n",
      "2018-04-11T15:54:06.295704: step 3308, loss 0.177531, acc 0.953125\n",
      "2018-04-11T15:54:06.927171: step 3309, loss 0.281473, acc 0.875\n",
      "2018-04-11T15:54:07.510373: step 3310, loss 0.133608, acc 0.96875\n",
      "2018-04-11T15:54:08.151341: step 3311, loss 0.130182, acc 0.953125\n",
      "2018-04-11T15:54:08.764307: step 3312, loss 0.1609, acc 0.9375\n",
      "2018-04-11T15:54:09.397036: step 3313, loss 0.261828, acc 0.921875\n",
      "2018-04-11T15:54:10.060195: step 3314, loss 0.173069, acc 0.953125\n",
      "2018-04-11T15:54:10.696033: step 3315, loss 0.138273, acc 0.96875\n",
      "2018-04-11T15:54:11.385762: step 3316, loss 0.203657, acc 0.9375\n",
      "2018-04-11T15:54:12.031799: step 3317, loss 0.108743, acc 1\n",
      "2018-04-11T15:54:12.690893: step 3318, loss 0.10812, acc 0.96875\n",
      "2018-04-11T15:54:13.293029: step 3319, loss 0.188627, acc 0.953125\n",
      "2018-04-11T15:54:13.946690: step 3320, loss 0.16741, acc 0.953125\n",
      "2018-04-11T15:54:14.581977: step 3321, loss 0.108613, acc 0.984375\n",
      "2018-04-11T15:54:15.259656: step 3322, loss 0.200077, acc 0.9375\n",
      "2018-04-11T15:54:15.901968: step 3323, loss 0.190623, acc 0.9375\n",
      "2018-04-11T15:54:16.604222: step 3324, loss 0.202276, acc 0.9375\n",
      "2018-04-11T15:54:17.242578: step 3325, loss 0.158176, acc 0.953125\n",
      "2018-04-11T15:54:17.824075: step 3326, loss 0.16173, acc 0.96875\n",
      "2018-04-11T15:54:18.460159: step 3327, loss 0.120137, acc 0.953125\n",
      "2018-04-11T15:54:19.133712: step 3328, loss 0.135998, acc 0.953125\n",
      "2018-04-11T15:54:19.722133: step 3329, loss 0.120375, acc 0.984375\n",
      "2018-04-11T15:54:20.330493: step 3330, loss 0.131264, acc 0.96875\n",
      "2018-04-11T15:54:21.010242: step 3331, loss 0.0864091, acc 0.984375\n",
      "2018-04-11T15:54:21.616090: step 3332, loss 0.255412, acc 0.921875\n",
      "2018-04-11T15:54:22.307220: step 3333, loss 0.150953, acc 0.9375\n",
      "2018-04-11T15:54:22.934511: step 3334, loss 0.18059, acc 0.96875\n",
      "2018-04-11T15:54:23.599886: step 3335, loss 0.162504, acc 0.9375\n",
      "2018-04-11T15:54:24.234184: step 3336, loss 0.245978, acc 0.921875\n",
      "2018-04-11T15:54:24.891767: step 3337, loss 0.282388, acc 0.921875\n",
      "2018-04-11T15:54:25.543040: step 3338, loss 0.200006, acc 0.9375\n",
      "2018-04-11T15:54:26.188301: step 3339, loss 0.297987, acc 0.875\n",
      "2018-04-11T15:54:26.790687: step 3340, loss 0.212924, acc 0.953125\n",
      "2018-04-11T15:54:27.427893: step 3341, loss 0.182673, acc 0.9375\n",
      "2018-04-11T15:54:28.123954: step 3342, loss 0.37103, acc 0.859375\n",
      "2018-04-11T15:54:28.768180: step 3343, loss 0.165335, acc 0.96875\n",
      "2018-04-11T15:54:29.372708: step 3344, loss 0.177904, acc 0.953125\n",
      "2018-04-11T15:54:29.940272: step 3345, loss 0.16869, acc 0.96875\n",
      "2018-04-11T15:54:30.515025: step 3346, loss 0.217602, acc 0.9375\n",
      "2018-04-11T15:54:31.101125: step 3347, loss 0.119482, acc 0.984375\n",
      "2018-04-11T15:54:31.671089: step 3348, loss 0.114085, acc 0.96875\n",
      "2018-04-11T15:54:32.243829: step 3349, loss 0.174637, acc 0.96875\n",
      "2018-04-11T15:54:32.817862: step 3350, loss 0.332761, acc 0.890625\n",
      "2018-04-11T15:54:33.393014: step 3351, loss 0.255848, acc 0.90625\n",
      "2018-04-11T15:54:34.005773: step 3352, loss 0.206813, acc 0.953125\n",
      "2018-04-11T15:54:34.604679: step 3353, loss 0.277395, acc 0.9375\n",
      "2018-04-11T15:54:35.168016: step 3354, loss 0.228933, acc 0.890625\n",
      "2018-04-11T15:54:35.793632: step 3355, loss 0.143189, acc 0.96875\n",
      "2018-04-11T15:54:36.417207: step 3356, loss 0.249283, acc 0.90625\n",
      "2018-04-11T15:54:36.985360: step 3357, loss 0.246637, acc 0.9375\n",
      "2018-04-11T15:54:37.581079: step 3358, loss 0.302976, acc 0.890625\n",
      "2018-04-11T15:54:38.188617: step 3359, loss 0.108406, acc 0.984375\n",
      "2018-04-11T15:54:38.799060: step 3360, loss 0.162771, acc 0.96875\n",
      "2018-04-11T15:54:39.430858: step 3361, loss 0.321465, acc 0.890625\n",
      "2018-04-11T15:54:40.012817: step 3362, loss 0.236642, acc 0.921875\n",
      "2018-04-11T15:54:40.594411: step 3363, loss 0.108762, acc 1\n",
      "2018-04-11T15:54:41.170934: step 3364, loss 0.226498, acc 0.9375\n",
      "2018-04-11T15:54:41.757651: step 3365, loss 0.0560246, acc 1\n",
      "2018-04-11T15:54:42.349149: step 3366, loss 0.212909, acc 0.96875\n",
      "2018-04-11T15:54:42.949630: step 3367, loss 0.122616, acc 0.96875\n",
      "2018-04-11T15:54:43.553263: step 3368, loss 0.152961, acc 0.984375\n",
      "2018-04-11T15:54:44.152943: step 3369, loss 0.236202, acc 0.953125\n",
      "2018-04-11T15:54:44.750903: step 3370, loss 0.224976, acc 0.9375\n",
      "2018-04-11T15:54:45.362418: step 3371, loss 0.274703, acc 0.90625\n",
      "2018-04-11T15:54:45.948732: step 3372, loss 0.2651, acc 0.90625\n",
      "2018-04-11T15:54:46.533393: step 3373, loss 0.224735, acc 0.9375\n",
      "2018-04-11T15:54:47.177066: step 3374, loss 0.254977, acc 0.921875\n",
      "2018-04-11T15:54:47.747012: step 3375, loss 0.226122, acc 0.9375\n",
      "2018-04-11T15:54:48.310952: step 3376, loss 0.177281, acc 0.9375\n",
      "2018-04-11T15:54:48.910819: step 3377, loss 0.165601, acc 0.953125\n",
      "2018-04-11T15:54:49.509494: step 3378, loss 0.190761, acc 0.953125\n",
      "2018-04-11T15:54:50.161304: step 3379, loss 0.154487, acc 0.953125\n",
      "2018-04-11T15:54:50.806218: step 3380, loss 0.202626, acc 0.953125\n",
      "2018-04-11T15:54:51.438594: step 3381, loss 0.129789, acc 0.984375\n",
      "2018-04-11T15:54:52.146677: step 3382, loss 0.158977, acc 0.96875\n",
      "2018-04-11T15:54:52.891896: step 3383, loss 0.172583, acc 0.953125\n",
      "2018-04-11T15:54:53.599915: step 3384, loss 0.15797, acc 0.953125\n",
      "2018-04-11T15:54:54.250628: step 3385, loss 0.186877, acc 0.96875\n",
      "2018-04-11T15:54:54.954001: step 3386, loss 0.166601, acc 0.953125\n",
      "2018-04-11T15:54:55.707925: step 3387, loss 0.202953, acc 0.9375\n",
      "2018-04-11T15:54:56.350491: step 3388, loss 0.205566, acc 0.953125\n",
      "2018-04-11T15:54:57.005637: step 3389, loss 0.123822, acc 0.96875\n",
      "2018-04-11T15:54:57.586907: step 3390, loss 0.281922, acc 0.96875\n",
      "2018-04-11T15:54:58.275041: step 3391, loss 0.172146, acc 0.953125\n",
      "2018-04-11T15:54:58.982425: step 3392, loss 0.145927, acc 0.953125\n",
      "2018-04-11T15:54:59.657684: step 3393, loss 0.241569, acc 0.953125\n",
      "2018-04-11T15:55:00.283996: step 3394, loss 0.132954, acc 0.953125\n",
      "2018-04-11T15:55:00.933636: step 3395, loss 0.111553, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:55:01.618349: step 3396, loss 0.241683, acc 0.890625\n",
      "2018-04-11T15:55:02.285333: step 3397, loss 0.22986, acc 0.9375\n",
      "2018-04-11T15:55:02.921739: step 3398, loss 0.202944, acc 0.921875\n",
      "2018-04-11T15:55:03.580869: step 3399, loss 0.173734, acc 0.9375\n",
      "2018-04-11T15:55:04.144035: step 3400, loss 0.320076, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:55:04.322002: step 3400, loss 0.569141, acc 0.84\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-3400\n",
      "\n",
      "2018-04-11T15:55:05.050839: step 3401, loss 0.123905, acc 0.96875\n",
      "2018-04-11T15:55:05.641286: step 3402, loss 0.202677, acc 0.921875\n",
      "2018-04-11T15:55:06.209724: step 3403, loss 0.261577, acc 0.953125\n",
      "2018-04-11T15:55:06.781503: step 3404, loss 0.194161, acc 0.9375\n",
      "2018-04-11T15:55:07.400257: step 3405, loss 0.144828, acc 0.96875\n",
      "2018-04-11T15:55:07.977928: step 3406, loss 0.137845, acc 0.96875\n",
      "2018-04-11T15:55:08.598165: step 3407, loss 0.239722, acc 0.9375\n",
      "2018-04-11T15:55:09.215803: step 3408, loss 0.171624, acc 0.96875\n",
      "2018-04-11T15:55:09.836585: step 3409, loss 0.141307, acc 0.96875\n",
      "2018-04-11T15:55:10.429129: step 3410, loss 0.122183, acc 0.984375\n",
      "2018-04-11T15:55:11.034567: step 3411, loss 0.220364, acc 0.9375\n",
      "2018-04-11T15:55:11.650198: step 3412, loss 0.236285, acc 0.953125\n",
      "2018-04-11T15:55:12.238504: step 3413, loss 0.181793, acc 0.9375\n",
      "2018-04-11T15:55:12.811252: step 3414, loss 0.181128, acc 0.96875\n",
      "2018-04-11T15:55:13.399503: step 3415, loss 0.198113, acc 0.96875\n",
      "2018-04-11T15:55:14.006929: step 3416, loss 0.111607, acc 0.96875\n",
      "2018-04-11T15:55:14.577209: step 3417, loss 0.159272, acc 0.953125\n",
      "2018-04-11T15:55:15.145288: step 3418, loss 0.192709, acc 0.921875\n",
      "2018-04-11T15:55:15.737326: step 3419, loss 0.245488, acc 0.921875\n",
      "2018-04-11T15:55:16.332715: step 3420, loss 0.147123, acc 0.96875\n",
      "2018-04-11T15:55:16.937700: step 3421, loss 0.193183, acc 0.9375\n",
      "2018-04-11T15:55:17.518050: step 3422, loss 0.13386, acc 0.96875\n",
      "2018-04-11T15:55:18.092762: step 3423, loss 0.180074, acc 0.9375\n",
      "2018-04-11T15:55:18.651874: step 3424, loss 0.248004, acc 0.921875\n",
      "2018-04-11T15:55:19.245266: step 3425, loss 0.16401, acc 0.9375\n",
      "2018-04-11T15:55:19.816426: step 3426, loss 0.281415, acc 0.9375\n",
      "2018-04-11T15:55:20.379758: step 3427, loss 0.196252, acc 0.921875\n",
      "2018-04-11T15:55:20.975009: step 3428, loss 0.186657, acc 0.90625\n",
      "2018-04-11T15:55:21.551549: step 3429, loss 0.160342, acc 0.9375\n",
      "2018-04-11T15:55:22.142187: step 3430, loss 0.125681, acc 0.96875\n",
      "2018-04-11T15:55:22.717509: step 3431, loss 0.23525, acc 0.9375\n",
      "2018-04-11T15:55:23.298186: step 3432, loss 0.239127, acc 0.921875\n",
      "2018-04-11T15:55:23.894444: step 3433, loss 0.18998, acc 0.953125\n",
      "2018-04-11T15:55:24.467453: step 3434, loss 0.124683, acc 0.953125\n",
      "2018-04-11T15:55:25.065778: step 3435, loss 0.355989, acc 0.921875\n",
      "2018-04-11T15:55:25.663284: step 3436, loss 0.225824, acc 0.953125\n",
      "2018-04-11T15:55:26.236841: step 3437, loss 0.262369, acc 0.921875\n",
      "2018-04-11T15:55:26.808895: step 3438, loss 0.302937, acc 0.921875\n",
      "2018-04-11T15:55:27.407987: step 3439, loss 0.312183, acc 0.90625\n",
      "2018-04-11T15:55:28.002378: step 3440, loss 0.189628, acc 0.953125\n",
      "2018-04-11T15:55:28.581466: step 3441, loss 0.258401, acc 0.9375\n",
      "2018-04-11T15:55:29.180438: step 3442, loss 0.228732, acc 0.921875\n",
      "2018-04-11T15:55:29.741560: step 3443, loss 0.159918, acc 0.953125\n",
      "2018-04-11T15:55:30.310308: step 3444, loss 0.296739, acc 0.90625\n",
      "2018-04-11T15:55:30.884008: step 3445, loss 0.203675, acc 0.875\n",
      "2018-04-11T15:55:31.466048: step 3446, loss 0.130981, acc 0.96875\n",
      "2018-04-11T15:55:32.069591: step 3447, loss 0.152354, acc 0.953125\n",
      "2018-04-11T15:55:32.646799: step 3448, loss 0.202578, acc 0.953125\n",
      "2018-04-11T15:55:33.231458: step 3449, loss 0.159179, acc 0.984375\n",
      "2018-04-11T15:55:33.834662: step 3450, loss 0.255533, acc 0.921875\n",
      "2018-04-11T15:55:34.407495: step 3451, loss 0.28798, acc 0.9375\n",
      "2018-04-11T15:55:35.019907: step 3452, loss 0.210419, acc 0.9375\n",
      "2018-04-11T15:55:35.593570: step 3453, loss 0.154897, acc 0.9375\n",
      "2018-04-11T15:55:36.186461: step 3454, loss 0.437101, acc 0.90625\n",
      "2018-04-11T15:55:36.766733: step 3455, loss 0.120833, acc 0.96875\n",
      "2018-04-11T15:55:37.345167: step 3456, loss 0.160188, acc 0.96875\n",
      "2018-04-11T15:55:37.926826: step 3457, loss 0.120625, acc 0.984375\n",
      "2018-04-11T15:55:38.516093: step 3458, loss 0.07974, acc 0.96875\n",
      "2018-04-11T15:55:39.105041: step 3459, loss 0.155621, acc 0.9375\n",
      "2018-04-11T15:55:39.675762: step 3460, loss 0.170566, acc 0.9375\n",
      "2018-04-11T15:55:40.245553: step 3461, loss 0.225951, acc 0.96875\n",
      "2018-04-11T15:55:40.832045: step 3462, loss 0.197898, acc 0.921875\n",
      "2018-04-11T15:55:41.408645: step 3463, loss 0.147451, acc 0.96875\n",
      "2018-04-11T15:55:41.986388: step 3464, loss 0.141418, acc 0.96875\n",
      "2018-04-11T15:55:42.564893: step 3465, loss 0.131066, acc 0.96875\n",
      "2018-04-11T15:55:43.149326: step 3466, loss 0.186708, acc 0.9375\n",
      "2018-04-11T15:55:43.721655: step 3467, loss 0.243813, acc 0.90625\n",
      "2018-04-11T15:55:44.275754: step 3468, loss 0.180074, acc 0.9375\n",
      "2018-04-11T15:55:44.831314: step 3469, loss 0.242266, acc 0.921875\n",
      "2018-04-11T15:55:45.416633: step 3470, loss 0.117756, acc 0.984375\n",
      "2018-04-11T15:55:45.972235: step 3471, loss 0.0869854, acc 1\n",
      "2018-04-11T15:55:46.526692: step 3472, loss 0.335813, acc 0.875\n",
      "2018-04-11T15:55:47.098262: step 3473, loss 0.0918405, acc 0.96875\n",
      "2018-04-11T15:55:47.681213: step 3474, loss 0.180655, acc 0.9375\n",
      "2018-04-11T15:55:48.260145: step 3475, loss 0.168151, acc 0.96875\n",
      "2018-04-11T15:55:48.859000: step 3476, loss 0.287407, acc 0.90625\n",
      "2018-04-11T15:55:49.408787: step 3477, loss 0.142316, acc 0.953125\n",
      "2018-04-11T15:55:49.987289: step 3478, loss 0.165371, acc 0.9375\n",
      "2018-04-11T15:55:50.574467: step 3479, loss 0.224146, acc 0.90625\n",
      "2018-04-11T15:55:51.133350: step 3480, loss 0.234068, acc 0.9375\n",
      "2018-04-11T15:55:51.710069: step 3481, loss 0.301304, acc 0.921875\n",
      "2018-04-11T15:55:52.301387: step 3482, loss 0.182387, acc 0.9375\n",
      "2018-04-11T15:55:52.854570: step 3483, loss 0.173794, acc 0.953125\n",
      "2018-04-11T15:55:53.415179: step 3484, loss 0.314322, acc 0.921875\n",
      "2018-04-11T15:55:53.990179: step 3485, loss 0.118203, acc 0.984375\n",
      "2018-04-11T15:55:54.568743: step 3486, loss 0.229226, acc 0.921875\n",
      "2018-04-11T15:55:55.123563: step 3487, loss 0.132916, acc 0.96875\n",
      "2018-04-11T15:55:55.689604: step 3488, loss 0.211724, acc 0.90625\n",
      "2018-04-11T15:55:56.267830: step 3489, loss 0.217056, acc 0.96875\n",
      "2018-04-11T15:55:56.856797: step 3490, loss 0.19866, acc 0.9375\n",
      "2018-04-11T15:55:57.444789: step 3491, loss 0.207421, acc 0.9375\n",
      "2018-04-11T15:55:58.031845: step 3492, loss 0.123892, acc 0.96875\n",
      "2018-04-11T15:55:58.601585: step 3493, loss 0.189867, acc 0.953125\n",
      "2018-04-11T15:55:59.175695: step 3494, loss 0.268938, acc 0.921875\n",
      "2018-04-11T15:55:59.757066: step 3495, loss 0.145436, acc 0.96875\n",
      "2018-04-11T15:56:00.341011: step 3496, loss 0.177858, acc 0.96875\n",
      "2018-04-11T15:56:00.937303: step 3497, loss 0.235857, acc 0.921875\n",
      "2018-04-11T15:56:01.508770: step 3498, loss 0.216837, acc 0.921875\n",
      "2018-04-11T15:56:02.074975: step 3499, loss 0.228842, acc 0.921875\n",
      "2018-04-11T15:56:02.664666: step 3500, loss 0.165481, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:56:02.844506: step 3500, loss 0.590308, acc 0.85\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-3500\n",
      "\n",
      "2018-04-11T15:56:03.591139: step 3501, loss 0.114897, acc 1\n",
      "2018-04-11T15:56:04.163066: step 3502, loss 0.185535, acc 0.9375\n",
      "2018-04-11T15:56:04.722050: step 3503, loss 0.211865, acc 0.953125\n",
      "2018-04-11T15:56:05.301223: step 3504, loss 0.172584, acc 0.96875\n",
      "2018-04-11T15:56:05.881941: step 3505, loss 0.138002, acc 0.96875\n",
      "2018-04-11T15:56:06.474246: step 3506, loss 0.206079, acc 0.9375\n",
      "2018-04-11T15:56:07.049444: step 3507, loss 0.165101, acc 0.96875\n",
      "2018-04-11T15:56:07.622221: step 3508, loss 0.178321, acc 0.953125\n",
      "2018-04-11T15:56:08.194705: step 3509, loss 0.21283, acc 0.921875\n",
      "2018-04-11T15:56:08.762911: step 3510, loss 0.150021, acc 0.9375\n",
      "2018-04-11T15:56:09.340482: step 3511, loss 0.276707, acc 0.9375\n",
      "2018-04-11T15:56:09.933920: step 3512, loss 0.24822, acc 0.90625\n",
      "2018-04-11T15:56:10.528882: step 3513, loss 0.168023, acc 0.953125\n",
      "2018-04-11T15:56:11.107777: step 3514, loss 0.16792, acc 0.9375\n",
      "2018-04-11T15:56:11.695675: step 3515, loss 0.242073, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:56:12.280332: step 3516, loss 0.247216, acc 0.921875\n",
      "2018-04-11T15:56:12.862363: step 3517, loss 0.146564, acc 0.96875\n",
      "2018-04-11T15:56:13.460549: step 3518, loss 0.120153, acc 0.96875\n",
      "2018-04-11T15:56:14.036262: step 3519, loss 0.167819, acc 0.953125\n",
      "2018-04-11T15:56:14.618895: step 3520, loss 0.281013, acc 0.90625\n",
      "2018-04-11T15:56:15.211741: step 3521, loss 0.157258, acc 0.953125\n",
      "2018-04-11T15:56:15.808640: step 3522, loss 0.145364, acc 0.953125\n",
      "2018-04-11T15:56:16.394011: step 3523, loss 0.275777, acc 0.890625\n",
      "2018-04-11T15:56:16.953686: step 3524, loss 0.241157, acc 0.90625\n",
      "2018-04-11T15:56:17.549005: step 3525, loss 0.188258, acc 0.921875\n",
      "2018-04-11T15:56:18.119160: step 3526, loss 0.289664, acc 0.90625\n",
      "2018-04-11T15:56:18.712006: step 3527, loss 0.230513, acc 0.953125\n",
      "2018-04-11T15:56:19.299684: step 3528, loss 0.12314, acc 0.96875\n",
      "2018-04-11T15:56:19.870235: step 3529, loss 0.24203, acc 0.90625\n",
      "2018-04-11T15:56:20.468731: step 3530, loss 0.210918, acc 0.90625\n",
      "2018-04-11T15:56:21.036968: step 3531, loss 0.336517, acc 0.875\n",
      "2018-04-11T15:56:21.632033: step 3532, loss 0.307354, acc 0.921875\n",
      "2018-04-11T15:56:22.222003: step 3533, loss 0.202709, acc 0.921875\n",
      "2018-04-11T15:56:22.806483: step 3534, loss 0.133506, acc 0.96875\n",
      "2018-04-11T15:56:23.382230: step 3535, loss 0.1516, acc 0.953125\n",
      "2018-04-11T15:56:23.952245: step 3536, loss 0.146874, acc 0.953125\n",
      "2018-04-11T15:56:24.510998: step 3537, loss 0.205886, acc 0.953125\n",
      "2018-04-11T15:56:25.081320: step 3538, loss 0.175321, acc 0.9375\n",
      "2018-04-11T15:56:25.654833: step 3539, loss 0.117241, acc 0.984375\n",
      "2018-04-11T15:56:26.218374: step 3540, loss 0.169211, acc 0.921875\n",
      "2018-04-11T15:56:26.786735: step 3541, loss 0.367743, acc 0.921875\n",
      "2018-04-11T15:56:27.356801: step 3542, loss 0.233721, acc 0.890625\n",
      "2018-04-11T15:56:27.938567: step 3543, loss 0.347863, acc 0.859375\n",
      "2018-04-11T15:56:28.530372: step 3544, loss 0.162618, acc 0.96875\n",
      "2018-04-11T15:56:29.113644: step 3545, loss 0.113197, acc 1\n",
      "2018-04-11T15:56:29.704652: step 3546, loss 0.162514, acc 0.96875\n",
      "2018-04-11T15:56:30.296519: step 3547, loss 0.155988, acc 0.953125\n",
      "2018-04-11T15:56:30.889046: step 3548, loss 0.142377, acc 0.96875\n",
      "2018-04-11T15:56:31.482602: step 3549, loss 0.168877, acc 0.953125\n",
      "2018-04-11T15:56:32.061694: step 3550, loss 0.223207, acc 0.921875\n",
      "2018-04-11T15:56:32.688504: step 3551, loss 0.10899, acc 0.984375\n",
      "2018-04-11T15:56:33.311859: step 3552, loss 0.251756, acc 0.921875\n",
      "2018-04-11T15:56:33.941613: step 3553, loss 0.164525, acc 0.96875\n",
      "2018-04-11T15:56:34.602570: step 3554, loss 0.23165, acc 0.953125\n",
      "2018-04-11T15:56:35.222827: step 3555, loss 0.170487, acc 0.953125\n",
      "2018-04-11T15:56:35.842428: step 3556, loss 0.163531, acc 0.96875\n",
      "2018-04-11T15:56:36.561993: step 3557, loss 0.276103, acc 0.921875\n",
      "2018-04-11T15:56:37.172166: step 3558, loss 0.233596, acc 0.890625\n",
      "2018-04-11T15:56:37.869460: step 3559, loss 0.256208, acc 0.953125\n",
      "2018-04-11T15:56:38.614771: step 3560, loss 0.229054, acc 0.953125\n",
      "2018-04-11T15:56:39.320794: step 3561, loss 0.104977, acc 0.984375\n",
      "2018-04-11T15:56:40.043612: step 3562, loss 0.170428, acc 0.984375\n",
      "2018-04-11T15:56:40.710144: step 3563, loss 0.184659, acc 0.96875\n",
      "2018-04-11T15:56:41.377917: step 3564, loss 0.308324, acc 0.90625\n",
      "2018-04-11T15:56:42.030006: step 3565, loss 0.195715, acc 0.953125\n",
      "2018-04-11T15:56:42.783136: step 3566, loss 0.168473, acc 0.9375\n",
      "2018-04-11T15:56:43.562752: step 3567, loss 0.193691, acc 0.96875\n",
      "2018-04-11T15:56:44.241757: step 3568, loss 0.131727, acc 0.953125\n",
      "2018-04-11T15:56:45.022098: step 3569, loss 0.26002, acc 0.921875\n",
      "2018-04-11T15:56:45.681480: step 3570, loss 0.270481, acc 0.90625\n",
      "2018-04-11T15:56:46.475702: step 3571, loss 0.212942, acc 0.9375\n",
      "2018-04-11T15:56:47.216387: step 3572, loss 0.204566, acc 0.9375\n",
      "2018-04-11T15:56:47.798503: step 3573, loss 0.210908, acc 0.921875\n",
      "2018-04-11T15:56:48.424699: step 3574, loss 0.192711, acc 0.953125\n",
      "2018-04-11T15:56:49.074523: step 3575, loss 0.236075, acc 0.921875\n",
      "2018-04-11T15:56:49.773554: step 3576, loss 0.226811, acc 0.9375\n",
      "2018-04-11T15:56:50.509418: step 3577, loss 0.259128, acc 0.890625\n",
      "2018-04-11T15:56:51.114074: step 3578, loss 0.250881, acc 0.9375\n",
      "2018-04-11T15:56:51.761982: step 3579, loss 0.156395, acc 0.953125\n",
      "2018-04-11T15:56:52.464394: step 3580, loss 0.216515, acc 0.921875\n",
      "2018-04-11T15:56:53.185946: step 3581, loss 0.182279, acc 0.953125\n",
      "2018-04-11T15:56:53.771283: step 3582, loss 0.130977, acc 0.96875\n",
      "2018-04-11T15:56:54.367716: step 3583, loss 0.160772, acc 0.96875\n",
      "2018-04-11T15:56:55.081040: step 3584, loss 0.143255, acc 0.96875\n",
      "2018-04-11T15:56:55.782968: step 3585, loss 0.187606, acc 0.953125\n",
      "2018-04-11T15:56:56.497778: step 3586, loss 0.145383, acc 0.96875\n",
      "2018-04-11T15:56:57.117151: step 3587, loss 0.195082, acc 0.953125\n",
      "2018-04-11T15:56:57.835305: step 3588, loss 0.229781, acc 0.90625\n",
      "2018-04-11T15:56:58.529447: step 3589, loss 0.196407, acc 0.9375\n",
      "2018-04-11T15:56:59.145337: step 3590, loss 0.178848, acc 0.9375\n",
      "2018-04-11T15:56:59.402489: step 3591, loss 0.0660812, acc 1\n",
      "2018-04-11T15:57:00.089802: step 3592, loss 0.141924, acc 0.96875\n",
      "2018-04-11T15:57:00.787803: step 3593, loss 0.187268, acc 0.9375\n",
      "2018-04-11T15:57:01.487935: step 3594, loss 0.171495, acc 0.96875\n",
      "2018-04-11T15:57:02.194127: step 3595, loss 0.111629, acc 0.984375\n",
      "2018-04-11T15:57:02.896027: step 3596, loss 0.227704, acc 0.890625\n",
      "2018-04-11T15:57:03.507283: step 3597, loss 0.168564, acc 0.921875\n",
      "2018-04-11T15:57:04.150854: step 3598, loss 0.164307, acc 0.953125\n",
      "2018-04-11T15:57:04.820841: step 3599, loss 0.137251, acc 0.96875\n",
      "2018-04-11T15:57:05.481599: step 3600, loss 0.218161, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:57:05.676268: step 3600, loss 0.871328, acc 0.78\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-3600\n",
      "\n",
      "2018-04-11T15:57:06.536117: step 3601, loss 0.0960623, acc 1\n",
      "2018-04-11T15:57:07.114428: step 3602, loss 0.113847, acc 0.984375\n",
      "2018-04-11T15:57:07.789826: step 3603, loss 0.135024, acc 0.984375\n",
      "2018-04-11T15:57:08.456058: step 3604, loss 0.216835, acc 0.921875\n",
      "2018-04-11T15:57:09.064054: step 3605, loss 0.0994905, acc 0.984375\n",
      "2018-04-11T15:57:09.644216: step 3606, loss 0.222101, acc 0.921875\n",
      "2018-04-11T15:57:10.275334: step 3607, loss 0.150696, acc 0.96875\n",
      "2018-04-11T15:57:10.890501: step 3608, loss 0.101973, acc 0.984375\n",
      "2018-04-11T15:57:11.476749: step 3609, loss 0.132641, acc 0.96875\n",
      "2018-04-11T15:57:12.082045: step 3610, loss 0.0684139, acc 1\n",
      "2018-04-11T15:57:12.668802: step 3611, loss 0.101494, acc 0.984375\n",
      "2018-04-11T15:57:13.292837: step 3612, loss 0.145684, acc 0.953125\n",
      "2018-04-11T15:57:13.916071: step 3613, loss 0.106076, acc 0.984375\n",
      "2018-04-11T15:57:14.515115: step 3614, loss 0.132291, acc 0.953125\n",
      "2018-04-11T15:57:15.103826: step 3615, loss 0.124256, acc 0.96875\n",
      "2018-04-11T15:57:15.719781: step 3616, loss 0.170268, acc 0.953125\n",
      "2018-04-11T15:57:16.321764: step 3617, loss 0.150033, acc 0.96875\n",
      "2018-04-11T15:57:16.915002: step 3618, loss 0.166852, acc 0.953125\n",
      "2018-04-11T15:57:17.516588: step 3619, loss 0.162988, acc 0.9375\n",
      "2018-04-11T15:57:18.127144: step 3620, loss 0.195733, acc 0.9375\n",
      "2018-04-11T15:57:18.710451: step 3621, loss 0.183864, acc 0.953125\n",
      "2018-04-11T15:57:19.311677: step 3622, loss 0.11163, acc 0.984375\n",
      "2018-04-11T15:57:19.916960: step 3623, loss 0.195713, acc 0.9375\n",
      "2018-04-11T15:57:20.493014: step 3624, loss 0.151242, acc 0.96875\n",
      "2018-04-11T15:57:21.096626: step 3625, loss 0.123431, acc 0.953125\n",
      "2018-04-11T15:57:21.704037: step 3626, loss 0.203318, acc 0.953125\n",
      "2018-04-11T15:57:22.307158: step 3627, loss 0.107559, acc 0.96875\n",
      "2018-04-11T15:57:22.911361: step 3628, loss 0.19123, acc 0.90625\n",
      "2018-04-11T15:57:23.485032: step 3629, loss 0.132559, acc 0.96875\n",
      "2018-04-11T15:57:24.055160: step 3630, loss 0.224713, acc 0.96875\n",
      "2018-04-11T15:57:24.628897: step 3631, loss 0.183924, acc 0.9375\n",
      "2018-04-11T15:57:25.228554: step 3632, loss 0.120653, acc 0.96875\n",
      "2018-04-11T15:57:25.799165: step 3633, loss 0.141608, acc 0.953125\n",
      "2018-04-11T15:57:26.400720: step 3634, loss 0.0842355, acc 0.984375\n",
      "2018-04-11T15:57:27.000669: step 3635, loss 0.189659, acc 0.953125\n",
      "2018-04-11T15:57:27.598550: step 3636, loss 0.0973705, acc 0.96875\n",
      "2018-04-11T15:57:28.187323: step 3637, loss 0.190148, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:57:28.761603: step 3638, loss 0.232679, acc 0.921875\n",
      "2018-04-11T15:57:29.343492: step 3639, loss 0.220038, acc 0.921875\n",
      "2018-04-11T15:57:29.902937: step 3640, loss 0.0664427, acc 1\n",
      "2018-04-11T15:57:30.480620: step 3641, loss 0.155366, acc 0.984375\n",
      "2018-04-11T15:57:31.064326: step 3642, loss 0.224526, acc 0.953125\n",
      "2018-04-11T15:57:31.630579: step 3643, loss 0.14645, acc 0.96875\n",
      "2018-04-11T15:57:32.198044: step 3644, loss 0.0998682, acc 0.96875\n",
      "2018-04-11T15:57:32.774466: step 3645, loss 0.133125, acc 0.96875\n",
      "2018-04-11T15:57:33.344588: step 3646, loss 0.115406, acc 0.96875\n",
      "2018-04-11T15:57:33.944066: step 3647, loss 0.178389, acc 0.953125\n",
      "2018-04-11T15:57:34.508645: step 3648, loss 0.177731, acc 0.96875\n",
      "2018-04-11T15:57:35.081415: step 3649, loss 0.218751, acc 0.9375\n",
      "2018-04-11T15:57:35.632116: step 3650, loss 0.16758, acc 0.9375\n",
      "2018-04-11T15:57:36.180086: step 3651, loss 0.15621, acc 0.96875\n",
      "2018-04-11T15:57:36.748819: step 3652, loss 0.070625, acc 1\n",
      "2018-04-11T15:57:37.329707: step 3653, loss 0.238243, acc 0.9375\n",
      "2018-04-11T15:57:37.912565: step 3654, loss 0.131567, acc 0.953125\n",
      "2018-04-11T15:57:38.491026: step 3655, loss 0.122634, acc 0.953125\n",
      "2018-04-11T15:57:39.047720: step 3656, loss 0.108154, acc 0.984375\n",
      "2018-04-11T15:57:39.622781: step 3657, loss 0.115141, acc 0.96875\n",
      "2018-04-11T15:57:40.201206: step 3658, loss 0.121935, acc 0.96875\n",
      "2018-04-11T15:57:40.785078: step 3659, loss 0.130737, acc 1\n",
      "2018-04-11T15:57:41.341173: step 3660, loss 0.114332, acc 0.984375\n",
      "2018-04-11T15:57:41.907660: step 3661, loss 0.138312, acc 0.96875\n",
      "2018-04-11T15:57:42.489812: step 3662, loss 0.101716, acc 0.984375\n",
      "2018-04-11T15:57:43.050414: step 3663, loss 0.0769135, acc 0.984375\n",
      "2018-04-11T15:57:43.631373: step 3664, loss 0.132858, acc 0.96875\n",
      "2018-04-11T15:57:44.174810: step 3665, loss 0.0931398, acc 0.984375\n",
      "2018-04-11T15:57:44.723315: step 3666, loss 0.138969, acc 0.984375\n",
      "2018-04-11T15:57:45.266392: step 3667, loss 0.134985, acc 0.96875\n",
      "2018-04-11T15:57:45.871674: step 3668, loss 0.258002, acc 0.90625\n",
      "2018-04-11T15:57:46.449409: step 3669, loss 0.147483, acc 0.984375\n",
      "2018-04-11T15:57:47.019048: step 3670, loss 0.102144, acc 0.984375\n",
      "2018-04-11T15:57:47.585407: step 3671, loss 0.178906, acc 0.953125\n",
      "2018-04-11T15:57:48.155946: step 3672, loss 0.19173, acc 0.96875\n",
      "2018-04-11T15:57:48.738536: step 3673, loss 0.222736, acc 0.90625\n",
      "2018-04-11T15:57:49.308660: step 3674, loss 0.215387, acc 0.9375\n",
      "2018-04-11T15:57:49.862887: step 3675, loss 0.092344, acc 1\n",
      "2018-04-11T15:57:50.435769: step 3676, loss 0.0716756, acc 1\n",
      "2018-04-11T15:57:50.989997: step 3677, loss 0.112361, acc 0.96875\n",
      "2018-04-11T15:57:51.545457: step 3678, loss 0.260696, acc 0.953125\n",
      "2018-04-11T15:57:52.093754: step 3679, loss 0.105741, acc 0.984375\n",
      "2018-04-11T15:57:52.647351: step 3680, loss 0.127721, acc 0.96875\n",
      "2018-04-11T15:57:53.228447: step 3681, loss 0.16409, acc 0.984375\n",
      "2018-04-11T15:57:53.775802: step 3682, loss 0.133293, acc 0.96875\n",
      "2018-04-11T15:57:54.331190: step 3683, loss 0.167989, acc 0.96875\n",
      "2018-04-11T15:57:54.903172: step 3684, loss 0.0975864, acc 0.96875\n",
      "2018-04-11T15:57:55.452656: step 3685, loss 0.0628468, acc 1\n",
      "2018-04-11T15:57:55.998754: step 3686, loss 0.192582, acc 0.96875\n",
      "2018-04-11T15:57:56.542823: step 3687, loss 0.215705, acc 0.953125\n",
      "2018-04-11T15:57:57.110898: step 3688, loss 0.117003, acc 0.984375\n",
      "2018-04-11T15:57:57.654442: step 3689, loss 0.0938353, acc 0.984375\n",
      "2018-04-11T15:57:58.196573: step 3690, loss 0.0896638, acc 0.984375\n",
      "2018-04-11T15:57:58.769844: step 3691, loss 0.111942, acc 0.984375\n",
      "2018-04-11T15:57:59.319665: step 3692, loss 0.127888, acc 0.953125\n",
      "2018-04-11T15:57:59.863188: step 3693, loss 0.124855, acc 0.96875\n",
      "2018-04-11T15:58:00.413927: step 3694, loss 0.126521, acc 0.9375\n",
      "2018-04-11T15:58:00.989668: step 3695, loss 0.0908069, acc 0.984375\n",
      "2018-04-11T15:58:01.572724: step 3696, loss 0.141941, acc 0.953125\n",
      "2018-04-11T15:58:02.112966: step 3697, loss 0.17886, acc 0.953125\n",
      "2018-04-11T15:58:02.695710: step 3698, loss 0.183726, acc 0.9375\n",
      "2018-04-11T15:58:03.243008: step 3699, loss 0.122188, acc 0.953125\n",
      "2018-04-11T15:58:03.806940: step 3700, loss 0.0993821, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:58:03.971019: step 3700, loss 0.485873, acc 0.88\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-3700\n",
      "\n",
      "2018-04-11T15:58:04.646783: step 3701, loss 0.11906, acc 0.96875\n",
      "2018-04-11T15:58:05.197901: step 3702, loss 0.108628, acc 1\n",
      "2018-04-11T15:58:05.749076: step 3703, loss 0.120231, acc 0.96875\n",
      "2018-04-11T15:58:06.294662: step 3704, loss 0.125548, acc 0.96875\n",
      "2018-04-11T15:58:06.859917: step 3705, loss 0.114771, acc 0.96875\n",
      "2018-04-11T15:58:07.436840: step 3706, loss 0.164893, acc 0.9375\n",
      "2018-04-11T15:58:07.990415: step 3707, loss 0.0697398, acc 1\n",
      "2018-04-11T15:58:08.553416: step 3708, loss 0.128076, acc 0.96875\n",
      "2018-04-11T15:58:09.127729: step 3709, loss 0.0945098, acc 0.984375\n",
      "2018-04-11T15:58:09.669726: step 3710, loss 0.310345, acc 0.875\n",
      "2018-04-11T15:58:10.222298: step 3711, loss 0.103416, acc 0.984375\n",
      "2018-04-11T15:58:10.758438: step 3712, loss 0.165233, acc 0.9375\n",
      "2018-04-11T15:58:11.310143: step 3713, loss 0.117647, acc 0.96875\n",
      "2018-04-11T15:58:11.881198: step 3714, loss 0.122773, acc 0.96875\n",
      "2018-04-11T15:58:12.445245: step 3715, loss 0.152597, acc 0.96875\n",
      "2018-04-11T15:58:12.995234: step 3716, loss 0.109629, acc 0.984375\n",
      "2018-04-11T15:58:13.557061: step 3717, loss 0.0751025, acc 1\n",
      "2018-04-11T15:58:14.104186: step 3718, loss 0.130738, acc 0.953125\n",
      "2018-04-11T15:58:14.660078: step 3719, loss 0.292963, acc 0.921875\n",
      "2018-04-11T15:58:15.236774: step 3720, loss 0.246343, acc 0.90625\n",
      "2018-04-11T15:58:15.800956: step 3721, loss 0.0992872, acc 0.96875\n",
      "2018-04-11T15:58:16.354175: step 3722, loss 0.140426, acc 0.96875\n",
      "2018-04-11T15:58:16.916796: step 3723, loss 0.197909, acc 0.953125\n",
      "2018-04-11T15:58:17.462451: step 3724, loss 0.129848, acc 0.953125\n",
      "2018-04-11T15:58:18.003964: step 3725, loss 0.159301, acc 0.9375\n",
      "2018-04-11T15:58:18.564015: step 3726, loss 0.117382, acc 0.984375\n",
      "2018-04-11T15:58:19.127395: step 3727, loss 0.1351, acc 0.984375\n",
      "2018-04-11T15:58:19.702237: step 3728, loss 0.180861, acc 0.921875\n",
      "2018-04-11T15:58:20.277592: step 3729, loss 0.130226, acc 0.984375\n",
      "2018-04-11T15:58:20.845670: step 3730, loss 0.146189, acc 0.96875\n",
      "2018-04-11T15:58:21.395631: step 3731, loss 0.167067, acc 0.953125\n",
      "2018-04-11T15:58:21.940134: step 3732, loss 0.191286, acc 0.953125\n",
      "2018-04-11T15:58:22.482128: step 3733, loss 0.106051, acc 0.96875\n",
      "2018-04-11T15:58:23.028091: step 3734, loss 0.0958092, acc 0.984375\n",
      "2018-04-11T15:58:23.577604: step 3735, loss 0.098524, acc 0.984375\n",
      "2018-04-11T15:58:24.130818: step 3736, loss 0.134278, acc 0.984375\n",
      "2018-04-11T15:58:24.660569: step 3737, loss 0.11891, acc 0.96875\n",
      "2018-04-11T15:58:25.220561: step 3738, loss 0.145398, acc 0.953125\n",
      "2018-04-11T15:58:25.781184: step 3739, loss 0.148267, acc 0.953125\n",
      "2018-04-11T15:58:26.361508: step 3740, loss 0.193698, acc 0.96875\n",
      "2018-04-11T15:58:26.900488: step 3741, loss 0.117563, acc 0.984375\n",
      "2018-04-11T15:58:27.456356: step 3742, loss 0.236055, acc 0.9375\n",
      "2018-04-11T15:58:27.996471: step 3743, loss 0.209042, acc 0.9375\n",
      "2018-04-11T15:58:28.543196: step 3744, loss 0.0838814, acc 1\n",
      "2018-04-11T15:58:29.111085: step 3745, loss 0.095459, acc 0.984375\n",
      "2018-04-11T15:58:29.665518: step 3746, loss 0.167541, acc 0.953125\n",
      "2018-04-11T15:58:30.225282: step 3747, loss 0.0859704, acc 0.984375\n",
      "2018-04-11T15:58:30.768100: step 3748, loss 0.161253, acc 0.9375\n",
      "2018-04-11T15:58:31.321166: step 3749, loss 0.0597401, acc 1\n",
      "2018-04-11T15:58:31.885503: step 3750, loss 0.0825842, acc 1\n",
      "2018-04-11T15:58:32.438825: step 3751, loss 0.11764, acc 0.953125\n",
      "2018-04-11T15:58:32.995334: step 3752, loss 0.255958, acc 0.90625\n",
      "2018-04-11T15:58:33.550051: step 3753, loss 0.238706, acc 0.9375\n",
      "2018-04-11T15:58:34.128631: step 3754, loss 0.15294, acc 0.953125\n",
      "2018-04-11T15:58:34.685430: step 3755, loss 0.0790014, acc 0.984375\n",
      "2018-04-11T15:58:35.234959: step 3756, loss 0.236059, acc 0.90625\n",
      "2018-04-11T15:58:35.795754: step 3757, loss 0.211155, acc 0.953125\n",
      "2018-04-11T15:58:36.358058: step 3758, loss 0.130582, acc 0.984375\n",
      "2018-04-11T15:58:36.911022: step 3759, loss 0.234262, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:58:37.468718: step 3760, loss 0.19435, acc 0.921875\n",
      "2018-04-11T15:58:38.021415: step 3761, loss 0.112896, acc 0.96875\n",
      "2018-04-11T15:58:38.561127: step 3762, loss 0.204791, acc 0.9375\n",
      "2018-04-11T15:58:39.113247: step 3763, loss 0.240426, acc 0.90625\n",
      "2018-04-11T15:58:39.665970: step 3764, loss 0.0778045, acc 0.984375\n",
      "2018-04-11T15:58:40.211570: step 3765, loss 0.183259, acc 0.96875\n",
      "2018-04-11T15:58:40.750478: step 3766, loss 0.0744139, acc 1\n",
      "2018-04-11T15:58:41.310003: step 3767, loss 0.255294, acc 0.921875\n",
      "2018-04-11T15:58:41.859570: step 3768, loss 0.225865, acc 0.96875\n",
      "2018-04-11T15:58:42.405757: step 3769, loss 0.220931, acc 0.953125\n",
      "2018-04-11T15:58:42.954643: step 3770, loss 0.155457, acc 0.984375\n",
      "2018-04-11T15:58:43.522221: step 3771, loss 0.101347, acc 1\n",
      "2018-04-11T15:58:44.062596: step 3772, loss 0.205624, acc 0.96875\n",
      "2018-04-11T15:58:44.588932: step 3773, loss 0.177777, acc 0.96875\n",
      "2018-04-11T15:58:45.136829: step 3774, loss 0.103067, acc 0.984375\n",
      "2018-04-11T15:58:45.686137: step 3775, loss 0.161136, acc 0.9375\n",
      "2018-04-11T15:58:46.234930: step 3776, loss 0.189996, acc 0.921875\n",
      "2018-04-11T15:58:46.799546: step 3777, loss 0.192298, acc 0.953125\n",
      "2018-04-11T15:58:47.366870: step 3778, loss 0.10671, acc 0.984375\n",
      "2018-04-11T15:58:47.935258: step 3779, loss 0.118813, acc 0.96875\n",
      "2018-04-11T15:58:48.489791: step 3780, loss 0.214622, acc 0.9375\n",
      "2018-04-11T15:58:49.040592: step 3781, loss 0.111259, acc 0.96875\n",
      "2018-04-11T15:58:49.579625: step 3782, loss 0.275301, acc 0.9375\n",
      "2018-04-11T15:58:50.147080: step 3783, loss 0.213213, acc 0.921875\n",
      "2018-04-11T15:58:50.696792: step 3784, loss 0.122875, acc 0.984375\n",
      "2018-04-11T15:58:51.238718: step 3785, loss 0.125014, acc 0.984375\n",
      "2018-04-11T15:58:51.802363: step 3786, loss 0.172764, acc 0.953125\n",
      "2018-04-11T15:58:52.352233: step 3787, loss 0.126705, acc 0.96875\n",
      "2018-04-11T15:58:52.901662: step 3788, loss 0.151282, acc 0.953125\n",
      "2018-04-11T15:58:53.446834: step 3789, loss 0.125889, acc 0.96875\n",
      "2018-04-11T15:58:53.988459: step 3790, loss 0.139367, acc 0.984375\n",
      "2018-04-11T15:58:54.544004: step 3791, loss 0.115914, acc 0.96875\n",
      "2018-04-11T15:58:55.100220: step 3792, loss 0.0689789, acc 1\n",
      "2018-04-11T15:58:55.641650: step 3793, loss 0.0758527, acc 0.984375\n",
      "2018-04-11T15:58:56.195507: step 3794, loss 0.136404, acc 0.96875\n",
      "2018-04-11T15:58:56.745817: step 3795, loss 0.20607, acc 0.953125\n",
      "2018-04-11T15:58:57.300413: step 3796, loss 0.237596, acc 0.9375\n",
      "2018-04-11T15:58:57.847263: step 3797, loss 0.22431, acc 0.921875\n",
      "2018-04-11T15:58:58.419282: step 3798, loss 0.184533, acc 0.9375\n",
      "2018-04-11T15:58:58.978475: step 3799, loss 0.191304, acc 0.921875\n",
      "2018-04-11T15:58:59.520069: step 3800, loss 0.245537, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:58:59.684322: step 3800, loss 0.666261, acc 0.78\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-3800\n",
      "\n",
      "2018-04-11T15:59:00.370981: step 3801, loss 0.0534029, acc 1\n",
      "2018-04-11T15:59:00.913247: step 3802, loss 0.121556, acc 0.953125\n",
      "2018-04-11T15:59:01.456621: step 3803, loss 0.18254, acc 0.9375\n",
      "2018-04-11T15:59:02.006980: step 3804, loss 0.110089, acc 0.984375\n",
      "2018-04-11T15:59:02.550685: step 3805, loss 0.220187, acc 0.921875\n",
      "2018-04-11T15:59:03.101959: step 3806, loss 0.121861, acc 0.953125\n",
      "2018-04-11T15:59:03.658446: step 3807, loss 0.12103, acc 0.984375\n",
      "2018-04-11T15:59:04.195337: step 3808, loss 0.120389, acc 1\n",
      "2018-04-11T15:59:04.748760: step 3809, loss 0.18481, acc 0.9375\n",
      "2018-04-11T15:59:05.297017: step 3810, loss 0.169396, acc 0.953125\n",
      "2018-04-11T15:59:05.857156: step 3811, loss 0.190425, acc 0.953125\n",
      "2018-04-11T15:59:06.416038: step 3812, loss 0.27418, acc 0.90625\n",
      "2018-04-11T15:59:06.955775: step 3813, loss 0.089478, acc 1\n",
      "2018-04-11T15:59:07.521103: step 3814, loss 0.123307, acc 0.96875\n",
      "2018-04-11T15:59:08.105496: step 3815, loss 0.280829, acc 0.90625\n",
      "2018-04-11T15:59:08.694408: step 3816, loss 0.203219, acc 0.9375\n",
      "2018-04-11T15:59:09.251426: step 3817, loss 0.218222, acc 0.921875\n",
      "2018-04-11T15:59:09.807949: step 3818, loss 0.0980328, acc 0.984375\n",
      "2018-04-11T15:59:10.385944: step 3819, loss 0.14466, acc 0.96875\n",
      "2018-04-11T15:59:10.931438: step 3820, loss 0.136957, acc 0.96875\n",
      "2018-04-11T15:59:11.492773: step 3821, loss 0.179711, acc 0.953125\n",
      "2018-04-11T15:59:12.034552: step 3822, loss 0.157213, acc 0.9375\n",
      "2018-04-11T15:59:12.591929: step 3823, loss 0.117511, acc 0.953125\n",
      "2018-04-11T15:59:13.162085: step 3824, loss 0.0919286, acc 1\n",
      "2018-04-11T15:59:13.700378: step 3825, loss 0.235772, acc 0.9375\n",
      "2018-04-11T15:59:14.269483: step 3826, loss 0.193733, acc 0.984375\n",
      "2018-04-11T15:59:14.837105: step 3827, loss 0.197248, acc 0.953125\n",
      "2018-04-11T15:59:15.419306: step 3828, loss 0.153963, acc 0.9375\n",
      "2018-04-11T15:59:15.958243: step 3829, loss 0.162613, acc 0.953125\n",
      "2018-04-11T15:59:16.501058: step 3830, loss 0.209421, acc 0.953125\n",
      "2018-04-11T15:59:17.061182: step 3831, loss 0.272532, acc 0.9375\n",
      "2018-04-11T15:59:17.606995: step 3832, loss 0.208942, acc 0.9375\n",
      "2018-04-11T15:59:18.167863: step 3833, loss 0.157287, acc 0.953125\n",
      "2018-04-11T15:59:18.710897: step 3834, loss 0.185947, acc 0.9375\n",
      "2018-04-11T15:59:19.259735: step 3835, loss 0.205456, acc 0.921875\n",
      "2018-04-11T15:59:19.809022: step 3836, loss 0.227816, acc 0.921875\n",
      "2018-04-11T15:59:20.344930: step 3837, loss 0.102658, acc 1\n",
      "2018-04-11T15:59:20.914393: step 3838, loss 0.0781811, acc 0.984375\n",
      "2018-04-11T15:59:21.460411: step 3839, loss 0.0911983, acc 0.96875\n",
      "2018-04-11T15:59:22.013761: step 3840, loss 0.261427, acc 0.921875\n",
      "2018-04-11T15:59:22.584092: step 3841, loss 0.226658, acc 0.921875\n",
      "2018-04-11T15:59:23.129187: step 3842, loss 0.107771, acc 0.984375\n",
      "2018-04-11T15:59:23.690577: step 3843, loss 0.236155, acc 0.921875\n",
      "2018-04-11T15:59:24.249758: step 3844, loss 0.176513, acc 0.9375\n",
      "2018-04-11T15:59:24.803904: step 3845, loss 0.172378, acc 0.96875\n",
      "2018-04-11T15:59:25.365742: step 3846, loss 0.247748, acc 0.921875\n",
      "2018-04-11T15:59:25.909877: step 3847, loss 0.158137, acc 0.953125\n",
      "2018-04-11T15:59:26.454487: step 3848, loss 0.144432, acc 0.953125\n",
      "2018-04-11T15:59:26.999717: step 3849, loss 0.0691719, acc 1\n",
      "2018-04-11T15:59:27.553913: step 3850, loss 0.0708933, acc 1\n",
      "2018-04-11T15:59:28.113373: step 3851, loss 0.125111, acc 0.984375\n",
      "2018-04-11T15:59:28.667193: step 3852, loss 0.195183, acc 0.9375\n",
      "2018-04-11T15:59:29.213708: step 3853, loss 0.270014, acc 0.921875\n",
      "2018-04-11T15:59:29.753114: step 3854, loss 0.160962, acc 0.984375\n",
      "2018-04-11T15:59:30.296011: step 3855, loss 0.142868, acc 0.96875\n",
      "2018-04-11T15:59:30.857848: step 3856, loss 0.0886118, acc 0.984375\n",
      "2018-04-11T15:59:31.414642: step 3857, loss 0.24876, acc 0.9375\n",
      "2018-04-11T15:59:31.963854: step 3858, loss 0.162023, acc 0.921875\n",
      "2018-04-11T15:59:32.519265: step 3859, loss 0.210232, acc 0.921875\n",
      "2018-04-11T15:59:33.056624: step 3860, loss 0.162129, acc 0.96875\n",
      "2018-04-11T15:59:33.620173: step 3861, loss 0.142586, acc 0.984375\n",
      "2018-04-11T15:59:34.184468: step 3862, loss 0.191551, acc 0.953125\n",
      "2018-04-11T15:59:34.730846: step 3863, loss 0.0661811, acc 1\n",
      "2018-04-11T15:59:35.275640: step 3864, loss 0.121231, acc 0.953125\n",
      "2018-04-11T15:59:35.815953: step 3865, loss 0.146799, acc 0.96875\n",
      "2018-04-11T15:59:36.381082: step 3866, loss 0.141892, acc 0.96875\n",
      "2018-04-11T15:59:36.931722: step 3867, loss 0.253314, acc 0.921875\n",
      "2018-04-11T15:59:37.485192: step 3868, loss 0.11612, acc 0.984375\n",
      "2018-04-11T15:59:38.025729: step 3869, loss 0.164944, acc 0.9375\n",
      "2018-04-11T15:59:38.554920: step 3870, loss 0.167919, acc 0.96875\n",
      "2018-04-11T15:59:39.106161: step 3871, loss 0.191297, acc 0.9375\n",
      "2018-04-11T15:59:39.658963: step 3872, loss 0.153373, acc 0.921875\n",
      "2018-04-11T15:59:40.215704: step 3873, loss 0.183612, acc 0.953125\n",
      "2018-04-11T15:59:40.759651: step 3874, loss 0.165062, acc 0.953125\n",
      "2018-04-11T15:59:41.302740: step 3875, loss 0.113607, acc 0.984375\n",
      "2018-04-11T15:59:41.860922: step 3876, loss 0.122469, acc 0.96875\n",
      "2018-04-11T15:59:42.410656: step 3877, loss 0.116837, acc 0.96875\n",
      "2018-04-11T15:59:42.970154: step 3878, loss 0.121269, acc 0.984375\n",
      "2018-04-11T15:59:43.541029: step 3879, loss 0.223203, acc 0.9375\n",
      "2018-04-11T15:59:44.097899: step 3880, loss 0.158435, acc 0.96875\n",
      "2018-04-11T15:59:44.634311: step 3881, loss 0.13555, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T15:59:45.221380: step 3882, loss 0.099534, acc 0.9375\n",
      "2018-04-11T15:59:45.778121: step 3883, loss 0.205175, acc 0.90625\n",
      "2018-04-11T15:59:46.331338: step 3884, loss 0.0757163, acc 1\n",
      "2018-04-11T15:59:46.878881: step 3885, loss 0.132258, acc 0.953125\n",
      "2018-04-11T15:59:47.431876: step 3886, loss 0.149232, acc 0.9375\n",
      "2018-04-11T15:59:47.983283: step 3887, loss 0.168007, acc 0.953125\n",
      "2018-04-11T15:59:48.528332: step 3888, loss 0.312053, acc 0.90625\n",
      "2018-04-11T15:59:49.064383: step 3889, loss 0.24275, acc 0.953125\n",
      "2018-04-11T15:59:49.605004: step 3890, loss 0.105274, acc 0.984375\n",
      "2018-04-11T15:59:50.159896: step 3891, loss 0.139363, acc 0.953125\n",
      "2018-04-11T15:59:50.709128: step 3892, loss 0.208035, acc 0.96875\n",
      "2018-04-11T15:59:51.259762: step 3893, loss 0.255187, acc 0.90625\n",
      "2018-04-11T15:59:51.810286: step 3894, loss 0.183766, acc 0.953125\n",
      "2018-04-11T15:59:52.363544: step 3895, loss 0.154848, acc 0.953125\n",
      "2018-04-11T15:59:52.918253: step 3896, loss 0.193801, acc 0.921875\n",
      "2018-04-11T15:59:53.458753: step 3897, loss 0.138866, acc 0.953125\n",
      "2018-04-11T15:59:53.995748: step 3898, loss 0.103239, acc 1\n",
      "2018-04-11T15:59:54.537271: step 3899, loss 0.0883512, acc 0.984375\n",
      "2018-04-11T15:59:55.068376: step 3900, loss 0.138279, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T15:59:55.231198: step 3900, loss 0.496279, acc 0.82\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-3900\n",
      "\n",
      "2018-04-11T15:59:55.906271: step 3901, loss 0.135695, acc 0.953125\n",
      "2018-04-11T15:59:56.463113: step 3902, loss 0.110783, acc 0.984375\n",
      "2018-04-11T15:59:56.984302: step 3903, loss 0.195708, acc 0.96875\n",
      "2018-04-11T15:59:57.542571: step 3904, loss 0.179815, acc 0.9375\n",
      "2018-04-11T15:59:58.105735: step 3905, loss 0.102401, acc 0.96875\n",
      "2018-04-11T15:59:58.639152: step 3906, loss 0.16525, acc 0.9375\n",
      "2018-04-11T15:59:59.177704: step 3907, loss 0.156484, acc 0.96875\n",
      "2018-04-11T15:59:59.716919: step 3908, loss 0.16271, acc 0.953125\n",
      "2018-04-11T16:00:00.252249: step 3909, loss 0.237892, acc 0.9375\n",
      "2018-04-11T16:00:00.782311: step 3910, loss 0.117024, acc 0.984375\n",
      "2018-04-11T16:00:01.333440: step 3911, loss 0.0974309, acc 0.96875\n",
      "2018-04-11T16:00:01.893595: step 3912, loss 0.148236, acc 0.953125\n",
      "2018-04-11T16:00:02.450450: step 3913, loss 0.0917646, acc 0.984375\n",
      "2018-04-11T16:00:02.989648: step 3914, loss 0.209001, acc 0.953125\n",
      "2018-04-11T16:00:03.530255: step 3915, loss 0.280975, acc 0.953125\n",
      "2018-04-11T16:00:04.078985: step 3916, loss 0.153896, acc 0.9375\n",
      "2018-04-11T16:00:04.607211: step 3917, loss 0.232481, acc 0.9375\n",
      "2018-04-11T16:00:05.157912: step 3918, loss 0.137371, acc 0.953125\n",
      "2018-04-11T16:00:05.706385: step 3919, loss 0.249901, acc 0.9375\n",
      "2018-04-11T16:00:06.257308: step 3920, loss 0.273306, acc 0.890625\n",
      "2018-04-11T16:00:06.799737: step 3921, loss 0.189392, acc 0.9375\n",
      "2018-04-11T16:00:07.358569: step 3922, loss 0.179739, acc 0.953125\n",
      "2018-04-11T16:00:07.913972: step 3923, loss 0.170851, acc 0.9375\n",
      "2018-04-11T16:00:08.455568: step 3924, loss 0.212206, acc 0.953125\n",
      "2018-04-11T16:00:08.991859: step 3925, loss 0.166977, acc 0.953125\n",
      "2018-04-11T16:00:09.534276: step 3926, loss 0.168572, acc 0.9375\n",
      "2018-04-11T16:00:10.078803: step 3927, loss 0.171823, acc 0.96875\n",
      "2018-04-11T16:00:10.609132: step 3928, loss 0.182385, acc 0.96875\n",
      "2018-04-11T16:00:11.147795: step 3929, loss 0.141001, acc 0.9375\n",
      "2018-04-11T16:00:11.690692: step 3930, loss 0.0674651, acc 1\n",
      "2018-04-11T16:00:12.251441: step 3931, loss 0.125937, acc 0.984375\n",
      "2018-04-11T16:00:12.783188: step 3932, loss 0.214605, acc 0.9375\n",
      "2018-04-11T16:00:13.335174: step 3933, loss 0.170418, acc 0.953125\n",
      "2018-04-11T16:00:13.873684: step 3934, loss 0.292315, acc 0.9375\n",
      "2018-04-11T16:00:14.408952: step 3935, loss 0.149706, acc 0.953125\n",
      "2018-04-11T16:00:14.963699: step 3936, loss 0.166312, acc 0.96875\n",
      "2018-04-11T16:00:15.531286: step 3937, loss 0.0628873, acc 1\n",
      "2018-04-11T16:00:16.078019: step 3938, loss 0.154999, acc 0.9375\n",
      "2018-04-11T16:00:16.636139: step 3939, loss 0.139137, acc 0.96875\n",
      "2018-04-11T16:00:17.189457: step 3940, loss 0.179229, acc 0.9375\n",
      "2018-04-11T16:00:17.738846: step 3941, loss 0.107878, acc 0.984375\n",
      "2018-04-11T16:00:18.303710: step 3942, loss 0.130295, acc 0.96875\n",
      "2018-04-11T16:00:18.852041: step 3943, loss 0.13607, acc 0.96875\n",
      "2018-04-11T16:00:19.387514: step 3944, loss 0.0840444, acc 0.984375\n",
      "2018-04-11T16:00:19.909128: step 3945, loss 0.2446, acc 0.9375\n",
      "2018-04-11T16:00:20.471247: step 3946, loss 0.158756, acc 0.953125\n",
      "2018-04-11T16:00:21.016774: step 3947, loss 0.0901564, acc 0.984375\n",
      "2018-04-11T16:00:21.556882: step 3948, loss 0.120239, acc 0.984375\n",
      "2018-04-11T16:00:22.115385: step 3949, loss 0.150401, acc 0.953125\n",
      "2018-04-11T16:00:22.643243: step 3950, loss 0.187243, acc 0.921875\n",
      "2018-04-11T16:00:23.199376: step 3951, loss 0.115525, acc 0.96875\n",
      "2018-04-11T16:00:23.733685: step 3952, loss 0.150838, acc 0.953125\n",
      "2018-04-11T16:00:24.283742: step 3953, loss 0.238225, acc 0.921875\n",
      "2018-04-11T16:00:24.817753: step 3954, loss 0.129557, acc 0.96875\n",
      "2018-04-11T16:00:25.351709: step 3955, loss 0.16143, acc 0.953125\n",
      "2018-04-11T16:00:25.910835: step 3956, loss 0.247813, acc 0.9375\n",
      "2018-04-11T16:00:26.458565: step 3957, loss 0.381598, acc 0.90625\n",
      "2018-04-11T16:00:27.014594: step 3958, loss 0.145433, acc 0.953125\n",
      "2018-04-11T16:00:27.559595: step 3959, loss 0.173583, acc 0.953125\n",
      "2018-04-11T16:00:28.113880: step 3960, loss 0.126069, acc 0.984375\n",
      "2018-04-11T16:00:28.655757: step 3961, loss 0.154558, acc 0.953125\n",
      "2018-04-11T16:00:29.210581: step 3962, loss 0.14494, acc 0.96875\n",
      "2018-04-11T16:00:29.764664: step 3963, loss 0.167067, acc 0.9375\n",
      "2018-04-11T16:00:30.301140: step 3964, loss 0.088059, acc 1\n",
      "2018-04-11T16:00:30.855500: step 3965, loss 0.133423, acc 0.984375\n",
      "2018-04-11T16:00:31.411710: step 3966, loss 0.174683, acc 0.90625\n",
      "2018-04-11T16:00:31.958920: step 3967, loss 0.136297, acc 0.984375\n",
      "2018-04-11T16:00:32.503866: step 3968, loss 0.115829, acc 0.96875\n",
      "2018-04-11T16:00:33.046519: step 3969, loss 0.064843, acc 1\n",
      "2018-04-11T16:00:33.607240: step 3970, loss 0.138871, acc 0.953125\n",
      "2018-04-11T16:00:34.162560: step 3971, loss 0.0865202, acc 0.96875\n",
      "2018-04-11T16:00:34.702973: step 3972, loss 0.149918, acc 0.9375\n",
      "2018-04-11T16:00:35.242297: step 3973, loss 0.15231, acc 0.953125\n",
      "2018-04-11T16:00:35.774117: step 3974, loss 0.150107, acc 0.953125\n",
      "2018-04-11T16:00:36.309322: step 3975, loss 0.164386, acc 0.953125\n",
      "2018-04-11T16:00:36.861589: step 3976, loss 0.206084, acc 0.90625\n",
      "2018-04-11T16:00:37.421985: step 3977, loss 0.176633, acc 0.9375\n",
      "2018-04-11T16:00:37.972565: step 3978, loss 0.209714, acc 0.921875\n",
      "2018-04-11T16:00:38.509754: step 3979, loss 0.0689957, acc 1\n",
      "2018-04-11T16:00:39.040812: step 3980, loss 0.117039, acc 0.96875\n",
      "2018-04-11T16:00:39.577375: step 3981, loss 0.236281, acc 0.90625\n",
      "2018-04-11T16:00:40.129183: step 3982, loss 0.280927, acc 0.90625\n",
      "2018-04-11T16:00:40.686217: step 3983, loss 0.165916, acc 0.953125\n",
      "2018-04-11T16:00:41.237976: step 3984, loss 0.194661, acc 0.953125\n",
      "2018-04-11T16:00:41.789931: step 3985, loss 0.181144, acc 0.96875\n",
      "2018-04-11T16:00:42.322538: step 3986, loss 0.142862, acc 0.96875\n",
      "2018-04-11T16:00:42.884570: step 3987, loss 0.151649, acc 0.953125\n",
      "2018-04-11T16:00:43.445455: step 3988, loss 0.0996086, acc 0.984375\n",
      "2018-04-11T16:00:43.997542: step 3989, loss 0.095024, acc 0.984375\n",
      "2018-04-11T16:00:44.537416: step 3990, loss 0.157464, acc 0.984375\n",
      "2018-04-11T16:00:45.099540: step 3991, loss 0.240301, acc 0.921875\n",
      "2018-04-11T16:00:45.675529: step 3992, loss 0.186047, acc 0.96875\n",
      "2018-04-11T16:00:46.233364: step 3993, loss 0.0885281, acc 0.984375\n",
      "2018-04-11T16:00:46.795980: step 3994, loss 0.10069, acc 0.984375\n",
      "2018-04-11T16:00:47.351164: step 3995, loss 0.260753, acc 0.890625\n",
      "2018-04-11T16:00:47.898561: step 3996, loss 0.135384, acc 0.984375\n",
      "2018-04-11T16:00:48.445475: step 3997, loss 0.126029, acc 0.984375\n",
      "2018-04-11T16:00:48.984074: step 3998, loss 0.125977, acc 0.984375\n",
      "2018-04-11T16:00:49.523572: step 3999, loss 0.192543, acc 0.9375\n",
      "2018-04-11T16:00:50.064028: step 4000, loss 0.205499, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T16:00:50.228758: step 4000, loss 0.537309, acc 0.86\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-4000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T16:00:50.893514: step 4001, loss 0.164602, acc 0.96875\n",
      "2018-04-11T16:00:51.444454: step 4002, loss 0.110273, acc 0.96875\n",
      "2018-04-11T16:00:51.995335: step 4003, loss 0.135873, acc 0.953125\n",
      "2018-04-11T16:00:52.556329: step 4004, loss 0.130563, acc 0.96875\n",
      "2018-04-11T16:00:53.092497: step 4005, loss 0.241689, acc 0.921875\n",
      "2018-04-11T16:00:53.734006: step 4006, loss 0.226796, acc 0.921875\n",
      "2018-04-11T16:00:54.397875: step 4007, loss 0.0536189, acc 1\n",
      "2018-04-11T16:00:55.091408: step 4008, loss 0.174693, acc 0.96875\n",
      "2018-04-11T16:00:55.801842: step 4009, loss 0.158849, acc 0.953125\n",
      "2018-04-11T16:00:56.447007: step 4010, loss 0.211409, acc 0.9375\n",
      "2018-04-11T16:00:57.107643: step 4011, loss 0.18731, acc 0.953125\n",
      "2018-04-11T16:00:57.668164: step 4012, loss 0.124757, acc 0.96875\n",
      "2018-04-11T16:00:58.289932: step 4013, loss 0.114992, acc 0.96875\n",
      "2018-04-11T16:00:58.964934: step 4014, loss 0.158936, acc 0.953125\n",
      "2018-04-11T16:00:59.616072: step 4015, loss 0.123169, acc 0.953125\n",
      "2018-04-11T16:01:00.328596: step 4016, loss 0.173353, acc 0.9375\n",
      "2018-04-11T16:01:01.020832: step 4017, loss 0.187442, acc 0.953125\n",
      "2018-04-11T16:01:01.676382: step 4018, loss 0.0929745, acc 1\n",
      "2018-04-11T16:01:02.341680: step 4019, loss 0.16033, acc 0.96875\n",
      "2018-04-11T16:01:03.033075: step 4020, loss 0.199361, acc 0.921875\n",
      "2018-04-11T16:01:03.648550: step 4021, loss 0.101491, acc 1\n",
      "2018-04-11T16:01:04.256261: step 4022, loss 0.174249, acc 0.90625\n",
      "2018-04-11T16:01:04.788935: step 4023, loss 0.196689, acc 0.9375\n",
      "2018-04-11T16:01:05.340575: step 4024, loss 0.116795, acc 0.984375\n",
      "2018-04-11T16:01:05.889093: step 4025, loss 0.104596, acc 1\n",
      "2018-04-11T16:01:06.595246: step 4026, loss 0.147634, acc 0.9375\n",
      "2018-04-11T16:01:07.159458: step 4027, loss 0.132943, acc 0.984375\n",
      "2018-04-11T16:01:07.769755: step 4028, loss 0.11939, acc 0.96875\n",
      "2018-04-11T16:01:08.440220: step 4029, loss 0.16637, acc 0.953125\n",
      "2018-04-11T16:01:09.036123: step 4030, loss 0.150323, acc 0.953125\n",
      "2018-04-11T16:01:09.658252: step 4031, loss 0.177061, acc 0.9375\n",
      "2018-04-11T16:01:10.318202: step 4032, loss 0.139559, acc 0.953125\n",
      "2018-04-11T16:01:10.949968: step 4033, loss 0.105561, acc 0.984375\n",
      "2018-04-11T16:01:11.566191: step 4034, loss 0.129372, acc 0.96875\n",
      "2018-04-11T16:01:12.144604: step 4035, loss 0.287111, acc 0.90625\n",
      "2018-04-11T16:01:12.802271: step 4036, loss 0.206571, acc 0.90625\n",
      "2018-04-11T16:01:13.419755: step 4037, loss 0.135447, acc 0.984375\n",
      "2018-04-11T16:01:14.053051: step 4038, loss 0.167084, acc 0.96875\n",
      "2018-04-11T16:01:14.709255: step 4039, loss 0.166276, acc 0.953125\n",
      "2018-04-11T16:01:15.303513: step 4040, loss 0.187972, acc 0.9375\n",
      "2018-04-11T16:01:15.976533: step 4041, loss 0.147802, acc 0.984375\n",
      "2018-04-11T16:01:16.549506: step 4042, loss 0.163026, acc 0.953125\n",
      "2018-04-11T16:01:17.227875: step 4043, loss 0.11537, acc 0.984375\n",
      "2018-04-11T16:01:17.790903: step 4044, loss 0.0973984, acc 0.984375\n",
      "2018-04-11T16:01:18.464996: step 4045, loss 0.219931, acc 0.90625\n",
      "2018-04-11T16:01:19.112179: step 4046, loss 0.185012, acc 0.96875\n",
      "2018-04-11T16:01:19.732744: step 4047, loss 0.132031, acc 0.96875\n",
      "2018-04-11T16:01:20.291331: step 4048, loss 0.150588, acc 0.96875\n",
      "2018-04-11T16:01:20.861515: step 4049, loss 0.099617, acc 0.96875\n",
      "2018-04-11T16:01:21.435185: step 4050, loss 0.326409, acc 0.890625\n",
      "2018-04-11T16:01:22.009756: step 4051, loss 0.217313, acc 0.9375\n",
      "2018-04-11T16:01:22.604071: step 4052, loss 0.173188, acc 0.953125\n",
      "2018-04-11T16:01:23.181392: step 4053, loss 0.0919139, acc 0.96875\n",
      "2018-04-11T16:01:23.745006: step 4054, loss 0.132061, acc 0.9375\n",
      "2018-04-11T16:01:24.327126: step 4055, loss 0.105356, acc 1\n",
      "2018-04-11T16:01:24.924120: step 4056, loss 0.123193, acc 0.953125\n",
      "2018-04-11T16:01:25.527039: step 4057, loss 0.222282, acc 0.921875\n",
      "2018-04-11T16:01:26.119671: step 4058, loss 0.22995, acc 0.953125\n",
      "2018-04-11T16:01:26.707787: step 4059, loss 0.194991, acc 0.96875\n",
      "2018-04-11T16:01:27.321995: step 4060, loss 0.192357, acc 0.9375\n",
      "2018-04-11T16:01:27.956169: step 4061, loss 0.174949, acc 0.953125\n",
      "2018-04-11T16:01:28.547278: step 4062, loss 0.257776, acc 0.890625\n",
      "2018-04-11T16:01:29.140968: step 4063, loss 0.130688, acc 0.953125\n",
      "2018-04-11T16:01:29.745382: step 4064, loss 0.101892, acc 0.984375\n",
      "2018-04-11T16:01:30.351244: step 4065, loss 0.263291, acc 0.9375\n",
      "2018-04-11T16:01:30.945482: step 4066, loss 0.097026, acc 0.984375\n",
      "2018-04-11T16:01:31.557631: step 4067, loss 0.145673, acc 0.984375\n",
      "2018-04-11T16:01:32.160028: step 4068, loss 0.168917, acc 0.953125\n",
      "2018-04-11T16:01:32.765865: step 4069, loss 0.157999, acc 0.953125\n",
      "2018-04-11T16:01:33.374376: step 4070, loss 0.159332, acc 0.953125\n",
      "2018-04-11T16:01:33.986468: step 4071, loss 0.134529, acc 0.984375\n",
      "2018-04-11T16:01:34.596773: step 4072, loss 0.191149, acc 0.9375\n",
      "2018-04-11T16:01:35.189642: step 4073, loss 0.261669, acc 0.90625\n",
      "2018-04-11T16:01:35.793576: step 4074, loss 0.102606, acc 0.984375\n",
      "2018-04-11T16:01:36.379973: step 4075, loss 0.167657, acc 0.90625\n",
      "2018-04-11T16:01:36.978633: step 4076, loss 0.238564, acc 0.90625\n",
      "2018-04-11T16:01:37.590576: step 4077, loss 0.142063, acc 0.953125\n",
      "2018-04-11T16:01:38.204483: step 4078, loss 0.211042, acc 0.90625\n",
      "2018-04-11T16:01:38.787122: step 4079, loss 0.161235, acc 0.953125\n",
      "2018-04-11T16:01:39.370094: step 4080, loss 0.235293, acc 0.890625\n",
      "2018-04-11T16:01:39.978058: step 4081, loss 0.121632, acc 0.96875\n",
      "2018-04-11T16:01:40.570311: step 4082, loss 0.143521, acc 0.984375\n",
      "2018-04-11T16:01:41.143479: step 4083, loss 0.097735, acc 0.984375\n",
      "2018-04-11T16:01:41.747245: step 4084, loss 0.0970053, acc 0.96875\n",
      "2018-04-11T16:01:42.348942: step 4085, loss 0.118293, acc 0.984375\n",
      "2018-04-11T16:01:42.941468: step 4086, loss 0.103752, acc 0.96875\n",
      "2018-04-11T16:01:43.569714: step 4087, loss 0.172519, acc 0.953125\n",
      "2018-04-11T16:01:44.166853: step 4088, loss 0.136206, acc 0.96875\n",
      "2018-04-11T16:01:44.756677: step 4089, loss 0.12934, acc 0.96875\n",
      "2018-04-11T16:01:45.351124: step 4090, loss 0.154985, acc 0.953125\n",
      "2018-04-11T16:01:46.005681: step 4091, loss 0.0885684, acc 1\n",
      "2018-04-11T16:01:46.622860: step 4092, loss 0.256455, acc 0.90625\n",
      "2018-04-11T16:01:47.211422: step 4093, loss 0.296148, acc 0.90625\n",
      "2018-04-11T16:01:47.814534: step 4094, loss 0.138398, acc 0.953125\n",
      "2018-04-11T16:01:48.429139: step 4095, loss 0.213853, acc 0.96875\n",
      "2018-04-11T16:01:49.028808: step 4096, loss 0.0774998, acc 0.984375\n",
      "2018-04-11T16:01:49.639088: step 4097, loss 0.303476, acc 0.90625\n",
      "2018-04-11T16:01:50.230192: step 4098, loss 0.187352, acc 0.9375\n",
      "2018-04-11T16:01:50.813312: step 4099, loss 0.168121, acc 0.9375\n",
      "2018-04-11T16:01:51.411656: step 4100, loss 0.10068, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T16:01:51.604257: step 4100, loss 0.871815, acc 0.8\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-4100\n",
      "\n",
      "2018-04-11T16:01:52.361908: step 4101, loss 0.117867, acc 0.984375\n",
      "2018-04-11T16:01:52.954481: step 4102, loss 0.161492, acc 0.96875\n",
      "2018-04-11T16:01:53.572767: step 4103, loss 0.138463, acc 0.953125\n",
      "2018-04-11T16:01:53.862944: step 4104, loss 0.0393878, acc 1\n",
      "2018-04-11T16:01:54.487131: step 4105, loss 0.180536, acc 0.953125\n",
      "2018-04-11T16:01:55.144893: step 4106, loss 0.189977, acc 0.953125\n",
      "2018-04-11T16:01:55.774627: step 4107, loss 0.158438, acc 0.96875\n",
      "2018-04-11T16:01:56.367224: step 4108, loss 0.257991, acc 0.953125\n",
      "2018-04-11T16:01:56.975404: step 4109, loss 0.10353, acc 0.96875\n",
      "2018-04-11T16:01:57.566547: step 4110, loss 0.193331, acc 0.953125\n",
      "2018-04-11T16:01:58.174211: step 4111, loss 0.125581, acc 0.984375\n",
      "2018-04-11T16:01:58.775816: step 4112, loss 0.0734431, acc 1\n",
      "2018-04-11T16:01:59.357404: step 4113, loss 0.123771, acc 0.96875\n",
      "2018-04-11T16:01:59.940840: step 4114, loss 0.0831296, acc 0.984375\n",
      "2018-04-11T16:02:00.537090: step 4115, loss 0.191245, acc 0.953125\n",
      "2018-04-11T16:02:01.148021: step 4116, loss 0.120923, acc 0.984375\n",
      "2018-04-11T16:02:01.748028: step 4117, loss 0.167556, acc 0.9375\n",
      "2018-04-11T16:02:02.336297: step 4118, loss 0.0630689, acc 1\n",
      "2018-04-11T16:02:02.911892: step 4119, loss 0.134957, acc 0.984375\n",
      "2018-04-11T16:02:03.494168: step 4120, loss 0.152636, acc 0.9375\n",
      "2018-04-11T16:02:04.064203: step 4121, loss 0.115256, acc 0.984375\n",
      "2018-04-11T16:02:04.651268: step 4122, loss 0.117208, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T16:02:05.253203: step 4123, loss 0.0919539, acc 0.96875\n",
      "2018-04-11T16:02:05.812791: step 4124, loss 0.146425, acc 0.953125\n",
      "2018-04-11T16:02:06.385599: step 4125, loss 0.126193, acc 0.96875\n",
      "2018-04-11T16:02:06.984234: step 4126, loss 0.0824849, acc 0.984375\n",
      "2018-04-11T16:02:07.571483: step 4127, loss 0.0887505, acc 0.984375\n",
      "2018-04-11T16:02:08.156677: step 4128, loss 0.0709996, acc 1\n",
      "2018-04-11T16:02:08.728417: step 4129, loss 0.16722, acc 0.953125\n",
      "2018-04-11T16:02:09.307552: step 4130, loss 0.173185, acc 0.9375\n",
      "2018-04-11T16:02:09.886856: step 4131, loss 0.12608, acc 0.96875\n",
      "2018-04-11T16:02:10.451809: step 4132, loss 0.073954, acc 0.984375\n",
      "2018-04-11T16:02:11.031486: step 4133, loss 0.16418, acc 0.953125\n",
      "2018-04-11T16:02:11.610525: step 4134, loss 0.178608, acc 0.953125\n",
      "2018-04-11T16:02:12.185799: step 4135, loss 0.122936, acc 0.96875\n",
      "2018-04-11T16:02:12.751654: step 4136, loss 0.100002, acc 0.984375\n",
      "2018-04-11T16:02:13.318457: step 4137, loss 0.0896793, acc 0.984375\n",
      "2018-04-11T16:02:13.876856: step 4138, loss 0.162474, acc 0.953125\n",
      "2018-04-11T16:02:14.430704: step 4139, loss 0.105738, acc 0.96875\n",
      "2018-04-11T16:02:14.998788: step 4140, loss 0.107292, acc 1\n",
      "2018-04-11T16:02:15.556020: step 4141, loss 0.113959, acc 0.984375\n",
      "2018-04-11T16:02:16.131924: step 4142, loss 0.0920962, acc 0.984375\n",
      "2018-04-11T16:02:16.706748: step 4143, loss 0.122748, acc 0.953125\n",
      "2018-04-11T16:02:17.268725: step 4144, loss 0.113527, acc 0.984375\n",
      "2018-04-11T16:02:17.841113: step 4145, loss 0.161592, acc 0.984375\n",
      "2018-04-11T16:02:18.409174: step 4146, loss 0.122633, acc 0.984375\n",
      "2018-04-11T16:02:18.950221: step 4147, loss 0.116013, acc 0.96875\n",
      "2018-04-11T16:02:19.506598: step 4148, loss 0.127323, acc 0.953125\n",
      "2018-04-11T16:02:20.061254: step 4149, loss 0.102007, acc 1\n",
      "2018-04-11T16:02:20.636714: step 4150, loss 0.119553, acc 0.96875\n",
      "2018-04-11T16:02:21.259245: step 4151, loss 0.102399, acc 0.984375\n",
      "2018-04-11T16:02:21.885411: step 4152, loss 0.118794, acc 0.984375\n",
      "2018-04-11T16:02:22.546536: step 4153, loss 0.165727, acc 0.953125\n",
      "2018-04-11T16:02:23.160783: step 4154, loss 0.0701371, acc 1\n",
      "2018-04-11T16:02:23.889208: step 4155, loss 0.116341, acc 0.984375\n",
      "2018-04-11T16:02:24.609938: step 4156, loss 0.0845942, acc 1\n",
      "2018-04-11T16:02:25.285482: step 4157, loss 0.135751, acc 0.96875\n",
      "2018-04-11T16:02:25.963840: step 4158, loss 0.132532, acc 0.96875\n",
      "2018-04-11T16:02:26.708235: step 4159, loss 0.121248, acc 0.984375\n",
      "2018-04-11T16:02:27.450792: step 4160, loss 0.109775, acc 0.984375\n",
      "2018-04-11T16:02:28.078385: step 4161, loss 0.078309, acc 0.984375\n",
      "2018-04-11T16:02:28.730929: step 4162, loss 0.121256, acc 0.96875\n",
      "2018-04-11T16:02:29.397813: step 4163, loss 0.141861, acc 0.953125\n",
      "2018-04-11T16:02:30.031829: step 4164, loss 0.0709229, acc 1\n",
      "2018-04-11T16:02:30.766753: step 4165, loss 0.17154, acc 0.984375\n",
      "2018-04-11T16:02:31.443404: step 4166, loss 0.123982, acc 0.96875\n",
      "2018-04-11T16:02:32.105147: step 4167, loss 0.092189, acc 0.984375\n",
      "2018-04-11T16:02:32.819128: step 4168, loss 0.114184, acc 0.953125\n",
      "2018-04-11T16:02:33.537654: step 4169, loss 0.0753657, acc 1\n",
      "2018-04-11T16:02:34.220251: step 4170, loss 0.0742432, acc 1\n",
      "2018-04-11T16:02:35.043928: step 4171, loss 0.0850396, acc 0.984375\n",
      "2018-04-11T16:02:35.750096: step 4172, loss 0.0945044, acc 0.984375\n",
      "2018-04-11T16:02:36.400508: step 4173, loss 0.200131, acc 0.96875\n",
      "2018-04-11T16:02:37.034866: step 4174, loss 0.166708, acc 0.953125\n",
      "2018-04-11T16:02:37.668078: step 4175, loss 0.0616641, acc 1\n",
      "2018-04-11T16:02:38.261372: step 4176, loss 0.0600636, acc 1\n",
      "2018-04-11T16:02:38.881991: step 4177, loss 0.191687, acc 0.9375\n",
      "2018-04-11T16:02:39.515548: step 4178, loss 0.0837747, acc 1\n",
      "2018-04-11T16:02:40.163477: step 4179, loss 0.0814468, acc 1\n",
      "2018-04-11T16:02:40.852273: step 4180, loss 0.117226, acc 0.953125\n",
      "2018-04-11T16:02:41.626384: step 4181, loss 0.113218, acc 0.984375\n",
      "2018-04-11T16:02:42.263543: step 4182, loss 0.0807718, acc 1\n",
      "2018-04-11T16:02:42.942635: step 4183, loss 0.107946, acc 0.984375\n",
      "2018-04-11T16:02:43.625789: step 4184, loss 0.108087, acc 0.984375\n",
      "2018-04-11T16:02:44.461509: step 4185, loss 0.131561, acc 0.96875\n",
      "2018-04-11T16:02:45.178322: step 4186, loss 0.0596099, acc 1\n",
      "2018-04-11T16:02:45.843889: step 4187, loss 0.110405, acc 0.96875\n",
      "2018-04-11T16:02:46.476331: step 4188, loss 0.0932476, acc 0.96875\n",
      "2018-04-11T16:02:47.185600: step 4189, loss 0.125141, acc 0.9375\n",
      "2018-04-11T16:02:47.879911: step 4190, loss 0.139737, acc 0.953125\n",
      "2018-04-11T16:02:48.532685: step 4191, loss 0.192186, acc 0.9375\n",
      "2018-04-11T16:02:49.217994: step 4192, loss 0.213419, acc 0.9375\n",
      "2018-04-11T16:02:49.899600: step 4193, loss 0.108527, acc 0.96875\n",
      "2018-04-11T16:02:50.576087: step 4194, loss 0.154147, acc 0.9375\n",
      "2018-04-11T16:02:51.226635: step 4195, loss 0.288428, acc 0.859375\n",
      "2018-04-11T16:02:51.895787: step 4196, loss 0.149331, acc 0.96875\n",
      "2018-04-11T16:02:52.623361: step 4197, loss 0.138554, acc 0.9375\n",
      "2018-04-11T16:02:53.333278: step 4198, loss 0.130496, acc 0.96875\n",
      "2018-04-11T16:02:54.022332: step 4199, loss 0.0995751, acc 0.96875\n",
      "2018-04-11T16:02:54.686298: step 4200, loss 0.158875, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T16:02:54.898019: step 4200, loss 0.758619, acc 0.81\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-4200\n",
      "\n",
      "2018-04-11T16:02:55.761823: step 4201, loss 0.114219, acc 0.96875\n",
      "2018-04-11T16:02:56.405660: step 4202, loss 0.0634532, acc 1\n",
      "2018-04-11T16:02:57.079201: step 4203, loss 0.107768, acc 0.953125\n",
      "2018-04-11T16:02:57.731188: step 4204, loss 0.183034, acc 0.953125\n",
      "2018-04-11T16:02:58.369230: step 4205, loss 0.174887, acc 0.96875\n",
      "2018-04-11T16:02:59.026077: step 4206, loss 0.0961029, acc 0.984375\n",
      "2018-04-11T16:02:59.695396: step 4207, loss 0.13685, acc 0.96875\n",
      "2018-04-11T16:03:00.350905: step 4208, loss 0.110322, acc 0.96875\n",
      "2018-04-11T16:03:01.013930: step 4209, loss 0.117022, acc 0.96875\n",
      "2018-04-11T16:03:01.662452: step 4210, loss 0.12497, acc 0.984375\n",
      "2018-04-11T16:03:02.295717: step 4211, loss 0.095599, acc 1\n",
      "2018-04-11T16:03:02.930945: step 4212, loss 0.181672, acc 0.9375\n",
      "2018-04-11T16:03:03.579292: step 4213, loss 0.139827, acc 0.96875\n",
      "2018-04-11T16:03:04.256059: step 4214, loss 0.135482, acc 0.953125\n",
      "2018-04-11T16:03:04.925106: step 4215, loss 0.0991601, acc 0.984375\n",
      "2018-04-11T16:03:05.583773: step 4216, loss 0.0912931, acc 0.984375\n",
      "2018-04-11T16:03:06.244635: step 4217, loss 0.131064, acc 0.984375\n",
      "2018-04-11T16:03:06.920531: step 4218, loss 0.0819899, acc 0.984375\n",
      "2018-04-11T16:03:07.565864: step 4219, loss 0.154124, acc 0.953125\n",
      "2018-04-11T16:03:08.265282: step 4220, loss 0.195705, acc 0.96875\n",
      "2018-04-11T16:03:08.924338: step 4221, loss 0.0660751, acc 1\n",
      "2018-04-11T16:03:09.576891: step 4222, loss 0.183325, acc 0.953125\n",
      "2018-04-11T16:03:10.248099: step 4223, loss 0.09559, acc 0.984375\n",
      "2018-04-11T16:03:10.934451: step 4224, loss 0.111814, acc 0.96875\n",
      "2018-04-11T16:03:11.599694: step 4225, loss 0.164557, acc 0.953125\n",
      "2018-04-11T16:03:12.221941: step 4226, loss 0.168398, acc 0.953125\n",
      "2018-04-11T16:03:12.882658: step 4227, loss 0.0959949, acc 0.96875\n",
      "2018-04-11T16:03:13.551852: step 4228, loss 0.0981846, acc 1\n",
      "2018-04-11T16:03:14.224236: step 4229, loss 0.140635, acc 0.953125\n",
      "2018-04-11T16:03:14.867820: step 4230, loss 0.133284, acc 0.953125\n",
      "2018-04-11T16:03:15.511127: step 4231, loss 0.079394, acc 0.984375\n",
      "2018-04-11T16:03:16.175476: step 4232, loss 0.113327, acc 0.953125\n",
      "2018-04-11T16:03:16.818306: step 4233, loss 0.157375, acc 0.9375\n",
      "2018-04-11T16:03:17.504184: step 4234, loss 0.139514, acc 0.96875\n",
      "2018-04-11T16:03:18.148932: step 4235, loss 0.0723213, acc 1\n",
      "2018-04-11T16:03:18.808862: step 4236, loss 0.0990963, acc 1\n",
      "2018-04-11T16:03:19.478126: step 4237, loss 0.0814571, acc 0.984375\n",
      "2018-04-11T16:03:20.129259: step 4238, loss 0.0621287, acc 1\n",
      "2018-04-11T16:03:20.780930: step 4239, loss 0.064, acc 1\n",
      "2018-04-11T16:03:21.413460: step 4240, loss 0.108622, acc 0.953125\n",
      "2018-04-11T16:03:22.084453: step 4241, loss 0.0889355, acc 0.984375\n",
      "2018-04-11T16:03:22.738130: step 4242, loss 0.0933334, acc 0.96875\n",
      "2018-04-11T16:03:23.412330: step 4243, loss 0.0937746, acc 0.984375\n",
      "2018-04-11T16:03:24.074091: step 4244, loss 0.136427, acc 0.9375\n",
      "2018-04-11T16:03:24.738871: step 4245, loss 0.0714257, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T16:03:25.422816: step 4246, loss 0.154698, acc 0.9375\n",
      "2018-04-11T16:03:26.070625: step 4247, loss 0.211706, acc 0.9375\n",
      "2018-04-11T16:03:26.724093: step 4248, loss 0.103874, acc 0.984375\n",
      "2018-04-11T16:03:27.375218: step 4249, loss 0.0975456, acc 0.984375\n",
      "2018-04-11T16:03:28.006439: step 4250, loss 0.0628617, acc 1\n",
      "2018-04-11T16:03:28.676504: step 4251, loss 0.157578, acc 0.9375\n",
      "2018-04-11T16:03:29.330374: step 4252, loss 0.104841, acc 0.984375\n",
      "2018-04-11T16:03:29.959724: step 4253, loss 0.224903, acc 0.921875\n",
      "2018-04-11T16:03:30.638409: step 4254, loss 0.0749393, acc 0.984375\n",
      "2018-04-11T16:03:31.305981: step 4255, loss 0.108407, acc 0.984375\n",
      "2018-04-11T16:03:31.974245: step 4256, loss 0.204401, acc 0.9375\n",
      "2018-04-11T16:03:32.628312: step 4257, loss 0.0739703, acc 0.984375\n",
      "2018-04-11T16:03:33.297558: step 4258, loss 0.159773, acc 0.9375\n",
      "2018-04-11T16:03:33.995354: step 4259, loss 0.144067, acc 0.953125\n",
      "2018-04-11T16:03:34.633507: step 4260, loss 0.15836, acc 0.96875\n",
      "2018-04-11T16:03:35.272349: step 4261, loss 0.0635915, acc 1\n",
      "2018-04-11T16:03:35.914831: step 4262, loss 0.125936, acc 0.953125\n",
      "2018-04-11T16:03:36.582024: step 4263, loss 0.118499, acc 0.96875\n",
      "2018-04-11T16:03:37.244000: step 4264, loss 0.0966206, acc 1\n",
      "2018-04-11T16:03:37.884513: step 4265, loss 0.0838951, acc 0.96875\n",
      "2018-04-11T16:03:38.516785: step 4266, loss 0.0900216, acc 0.984375\n",
      "2018-04-11T16:03:39.155826: step 4267, loss 0.0882538, acc 0.984375\n",
      "2018-04-11T16:03:39.806479: step 4268, loss 0.105449, acc 0.953125\n",
      "2018-04-11T16:03:40.457712: step 4269, loss 0.134214, acc 0.953125\n",
      "2018-04-11T16:03:41.128243: step 4270, loss 0.0735277, acc 0.984375\n",
      "2018-04-11T16:03:41.806852: step 4271, loss 0.179948, acc 0.953125\n",
      "2018-04-11T16:03:42.471446: step 4272, loss 0.116476, acc 0.96875\n",
      "2018-04-11T16:03:43.131723: step 4273, loss 0.156986, acc 0.96875\n",
      "2018-04-11T16:03:43.797302: step 4274, loss 0.0814147, acc 0.984375\n",
      "2018-04-11T16:03:44.451709: step 4275, loss 0.105574, acc 1\n",
      "2018-04-11T16:03:45.166298: step 4276, loss 0.148516, acc 0.953125\n",
      "2018-04-11T16:03:45.810569: step 4277, loss 0.214725, acc 0.921875\n",
      "2018-04-11T16:03:46.513239: step 4278, loss 0.116558, acc 0.96875\n",
      "2018-04-11T16:03:47.159987: step 4279, loss 0.116517, acc 0.96875\n",
      "2018-04-11T16:03:47.799424: step 4280, loss 0.114045, acc 0.984375\n",
      "2018-04-11T16:03:48.456662: step 4281, loss 0.0753135, acc 1\n",
      "2018-04-11T16:03:49.098337: step 4282, loss 0.213757, acc 0.96875\n",
      "2018-04-11T16:03:49.736424: step 4283, loss 0.0755558, acc 1\n",
      "2018-04-11T16:03:50.379448: step 4284, loss 0.128656, acc 0.953125\n",
      "2018-04-11T16:03:51.072841: step 4285, loss 0.169463, acc 0.953125\n",
      "2018-04-11T16:03:51.732012: step 4286, loss 0.0827564, acc 0.984375\n",
      "2018-04-11T16:03:52.408298: step 4287, loss 0.136948, acc 0.96875\n",
      "2018-04-11T16:03:53.093532: step 4288, loss 0.0966543, acc 0.984375\n",
      "2018-04-11T16:03:53.727088: step 4289, loss 0.0993343, acc 1\n",
      "2018-04-11T16:03:54.443881: step 4290, loss 0.122424, acc 0.984375\n",
      "2018-04-11T16:03:55.119337: step 4291, loss 0.0624629, acc 1\n",
      "2018-04-11T16:03:55.746992: step 4292, loss 0.141715, acc 0.953125\n",
      "2018-04-11T16:03:56.396086: step 4293, loss 0.0576633, acc 1\n",
      "2018-04-11T16:03:57.071138: step 4294, loss 0.141603, acc 0.96875\n",
      "2018-04-11T16:03:57.774803: step 4295, loss 0.121043, acc 0.96875\n",
      "2018-04-11T16:03:58.432309: step 4296, loss 0.0893547, acc 0.984375\n",
      "2018-04-11T16:03:59.086291: step 4297, loss 0.174336, acc 0.9375\n",
      "2018-04-11T16:03:59.705397: step 4298, loss 0.0548501, acc 1\n",
      "2018-04-11T16:04:00.310872: step 4299, loss 0.141486, acc 0.9375\n",
      "2018-04-11T16:04:00.954142: step 4300, loss 0.0910532, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T16:04:01.153676: step 4300, loss 1.05213, acc 0.72\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-4300\n",
      "\n",
      "2018-04-11T16:04:01.920091: step 4301, loss 0.0924551, acc 0.96875\n",
      "2018-04-11T16:04:02.549118: step 4302, loss 0.179612, acc 0.921875\n",
      "2018-04-11T16:04:03.166636: step 4303, loss 0.0821236, acc 1\n",
      "2018-04-11T16:04:03.796058: step 4304, loss 0.097191, acc 0.984375\n",
      "2018-04-11T16:04:04.380601: step 4305, loss 0.183672, acc 0.953125\n",
      "2018-04-11T16:04:04.967836: step 4306, loss 0.0712913, acc 1\n",
      "2018-04-11T16:04:05.573930: step 4307, loss 0.186981, acc 0.953125\n",
      "2018-04-11T16:04:06.210807: step 4308, loss 0.224079, acc 0.953125\n",
      "2018-04-11T16:04:06.831050: step 4309, loss 0.192065, acc 0.96875\n",
      "2018-04-11T16:04:07.456404: step 4310, loss 0.161995, acc 0.9375\n",
      "2018-04-11T16:04:08.054911: step 4311, loss 0.14218, acc 0.953125\n",
      "2018-04-11T16:04:08.660802: step 4312, loss 0.0888658, acc 0.984375\n",
      "2018-04-11T16:04:09.294678: step 4313, loss 0.129246, acc 0.96875\n",
      "2018-04-11T16:04:09.893100: step 4314, loss 0.100976, acc 0.984375\n",
      "2018-04-11T16:04:10.505941: step 4315, loss 0.0713343, acc 1\n",
      "2018-04-11T16:04:11.168019: step 4316, loss 0.0749102, acc 1\n",
      "2018-04-11T16:04:11.784658: step 4317, loss 0.0745717, acc 0.984375\n",
      "2018-04-11T16:04:12.425479: step 4318, loss 0.0931994, acc 0.984375\n",
      "2018-04-11T16:04:13.067781: step 4319, loss 0.12263, acc 0.96875\n",
      "2018-04-11T16:04:13.814227: step 4320, loss 0.096544, acc 0.984375\n",
      "2018-04-11T16:04:14.436565: step 4321, loss 0.202874, acc 0.921875\n",
      "2018-04-11T16:04:15.036741: step 4322, loss 0.192702, acc 0.953125\n",
      "2018-04-11T16:04:15.751374: step 4323, loss 0.15435, acc 0.96875\n",
      "2018-04-11T16:04:16.365238: step 4324, loss 0.20241, acc 0.921875\n",
      "2018-04-11T16:04:17.009658: step 4325, loss 0.188447, acc 0.9375\n",
      "2018-04-11T16:04:17.764091: step 4326, loss 0.0845084, acc 1\n",
      "2018-04-11T16:04:18.599312: step 4327, loss 0.0994993, acc 0.96875\n",
      "2018-04-11T16:04:19.195122: step 4328, loss 0.118631, acc 0.984375\n",
      "2018-04-11T16:04:19.817066: step 4329, loss 0.131576, acc 0.96875\n",
      "2018-04-11T16:04:20.484899: step 4330, loss 0.0464194, acc 1\n",
      "2018-04-11T16:04:21.125539: step 4331, loss 0.209926, acc 0.96875\n",
      "2018-04-11T16:04:21.764838: step 4332, loss 0.171978, acc 0.9375\n",
      "2018-04-11T16:04:22.444575: step 4333, loss 0.202012, acc 0.921875\n",
      "2018-04-11T16:04:23.249811: step 4334, loss 0.0909107, acc 1\n",
      "2018-04-11T16:04:23.875674: step 4335, loss 0.13273, acc 0.953125\n",
      "2018-04-11T16:04:24.538570: step 4336, loss 0.161545, acc 0.953125\n",
      "2018-04-11T16:04:25.180654: step 4337, loss 0.218196, acc 0.953125\n",
      "2018-04-11T16:04:25.846261: step 4338, loss 0.212194, acc 0.9375\n",
      "2018-04-11T16:04:26.577763: step 4339, loss 0.0677922, acc 1\n",
      "2018-04-11T16:04:27.233258: step 4340, loss 0.145927, acc 0.96875\n",
      "2018-04-11T16:04:27.912289: step 4341, loss 0.127374, acc 0.96875\n",
      "2018-04-11T16:04:28.584773: step 4342, loss 0.0628705, acc 0.984375\n",
      "2018-04-11T16:04:29.234071: step 4343, loss 0.154252, acc 0.921875\n",
      "2018-04-11T16:04:29.878173: step 4344, loss 0.119581, acc 0.984375\n",
      "2018-04-11T16:04:30.459148: step 4345, loss 0.0838133, acc 1\n",
      "2018-04-11T16:04:31.044811: step 4346, loss 0.0673266, acc 1\n",
      "2018-04-11T16:04:31.647198: step 4347, loss 0.17111, acc 0.953125\n",
      "2018-04-11T16:04:32.323071: step 4348, loss 0.0807832, acc 0.96875\n",
      "2018-04-11T16:04:32.950999: step 4349, loss 0.166829, acc 0.953125\n",
      "2018-04-11T16:04:33.650348: step 4350, loss 0.107635, acc 0.984375\n",
      "2018-04-11T16:04:34.301371: step 4351, loss 0.195233, acc 0.9375\n",
      "2018-04-11T16:04:34.890184: step 4352, loss 0.0839151, acc 0.984375\n",
      "2018-04-11T16:04:35.539008: step 4353, loss 0.202862, acc 0.90625\n",
      "2018-04-11T16:04:36.195554: step 4354, loss 0.122693, acc 0.953125\n",
      "2018-04-11T16:04:36.852162: step 4355, loss 0.244616, acc 0.9375\n",
      "2018-04-11T16:04:37.442373: step 4356, loss 0.148091, acc 0.96875\n",
      "2018-04-11T16:04:38.043867: step 4357, loss 0.100103, acc 0.984375\n",
      "2018-04-11T16:04:38.654458: step 4358, loss 0.101964, acc 1\n",
      "2018-04-11T16:04:39.220830: step 4359, loss 0.194757, acc 0.953125\n",
      "2018-04-11T16:04:39.978121: step 4360, loss 0.215879, acc 0.96875\n",
      "2018-04-11T16:04:41.024150: step 4361, loss 0.164802, acc 0.96875\n",
      "2018-04-11T16:04:42.037992: step 4362, loss 0.101861, acc 0.984375\n",
      "2018-04-11T16:04:42.716746: step 4363, loss 0.134764, acc 0.953125\n",
      "2018-04-11T16:04:43.363120: step 4364, loss 0.120869, acc 0.96875\n",
      "2018-04-11T16:04:44.010223: step 4365, loss 0.120451, acc 0.96875\n",
      "2018-04-11T16:04:44.636132: step 4366, loss 0.182079, acc 0.9375\n",
      "2018-04-11T16:04:45.279919: step 4367, loss 0.104053, acc 0.984375\n",
      "2018-04-11T16:04:45.977359: step 4368, loss 0.156494, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T16:04:46.654708: step 4369, loss 0.174634, acc 0.96875\n",
      "2018-04-11T16:04:47.303733: step 4370, loss 0.165561, acc 0.953125\n",
      "2018-04-11T16:04:47.963365: step 4371, loss 0.0764395, acc 1\n",
      "2018-04-11T16:04:48.591009: step 4372, loss 0.150603, acc 0.984375\n",
      "2018-04-11T16:04:49.212488: step 4373, loss 0.218521, acc 0.953125\n",
      "2018-04-11T16:04:49.811758: step 4374, loss 0.144291, acc 0.984375\n",
      "2018-04-11T16:04:50.429079: step 4375, loss 0.149097, acc 0.96875\n",
      "2018-04-11T16:04:51.059194: step 4376, loss 0.193033, acc 0.953125\n",
      "2018-04-11T16:04:51.688460: step 4377, loss 0.148205, acc 0.984375\n",
      "2018-04-11T16:04:52.397257: step 4378, loss 0.112362, acc 0.96875\n",
      "2018-04-11T16:04:53.021447: step 4379, loss 0.117223, acc 0.96875\n",
      "2018-04-11T16:04:53.629664: step 4380, loss 0.0922593, acc 0.984375\n",
      "2018-04-11T16:04:54.229659: step 4381, loss 0.138792, acc 0.96875\n",
      "2018-04-11T16:04:54.841197: step 4382, loss 0.111354, acc 0.96875\n",
      "2018-04-11T16:04:55.438521: step 4383, loss 0.107507, acc 0.96875\n",
      "2018-04-11T16:04:56.058723: step 4384, loss 0.161509, acc 0.953125\n",
      "2018-04-11T16:04:56.620679: step 4385, loss 0.123732, acc 0.984375\n",
      "2018-04-11T16:04:57.195087: step 4386, loss 0.0646028, acc 1\n",
      "2018-04-11T16:04:57.801085: step 4387, loss 0.136776, acc 0.9375\n",
      "2018-04-11T16:04:58.432701: step 4388, loss 0.111652, acc 0.96875\n",
      "2018-04-11T16:04:59.059720: step 4389, loss 0.0961065, acc 0.984375\n",
      "2018-04-11T16:04:59.658077: step 4390, loss 0.105241, acc 0.984375\n",
      "2018-04-11T16:05:00.258252: step 4391, loss 0.127346, acc 0.96875\n",
      "2018-04-11T16:05:00.890197: step 4392, loss 0.0773529, acc 1\n",
      "2018-04-11T16:05:01.554247: step 4393, loss 0.0786409, acc 0.984375\n",
      "2018-04-11T16:05:02.184353: step 4394, loss 0.257916, acc 0.90625\n",
      "2018-04-11T16:05:02.768718: step 4395, loss 0.242356, acc 0.921875\n",
      "2018-04-11T16:05:03.333517: step 4396, loss 0.0738872, acc 0.984375\n",
      "2018-04-11T16:05:03.940955: step 4397, loss 0.107975, acc 0.96875\n",
      "2018-04-11T16:05:04.529655: step 4398, loss 0.220115, acc 0.921875\n",
      "2018-04-11T16:05:05.146560: step 4399, loss 0.169833, acc 0.953125\n",
      "2018-04-11T16:05:05.781582: step 4400, loss 0.108929, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T16:05:05.991470: step 4400, loss 0.909131, acc 0.81\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-4400\n",
      "\n",
      "2018-04-11T16:05:06.744027: step 4401, loss 0.141596, acc 0.96875\n",
      "2018-04-11T16:05:07.546498: step 4402, loss 0.105278, acc 0.984375\n",
      "2018-04-11T16:05:08.186619: step 4403, loss 0.106776, acc 0.96875\n",
      "2018-04-11T16:05:08.792335: step 4404, loss 0.149847, acc 0.953125\n",
      "2018-04-11T16:05:09.402021: step 4405, loss 0.146237, acc 0.96875\n",
      "2018-04-11T16:05:10.000713: step 4406, loss 0.132136, acc 0.96875\n",
      "2018-04-11T16:05:10.677957: step 4407, loss 0.238591, acc 0.890625\n",
      "2018-04-11T16:05:11.317021: step 4408, loss 0.0944464, acc 0.984375\n",
      "2018-04-11T16:05:11.954979: step 4409, loss 0.246344, acc 0.96875\n",
      "2018-04-11T16:05:12.558043: step 4410, loss 0.126778, acc 0.96875\n",
      "2018-04-11T16:05:13.171116: step 4411, loss 0.169344, acc 0.9375\n",
      "2018-04-11T16:05:13.808410: step 4412, loss 0.0805001, acc 0.984375\n",
      "2018-04-11T16:05:14.486355: step 4413, loss 0.211666, acc 0.90625\n",
      "2018-04-11T16:05:15.130145: step 4414, loss 0.223528, acc 0.9375\n",
      "2018-04-11T16:05:15.760463: step 4415, loss 0.169171, acc 0.9375\n",
      "2018-04-11T16:05:16.403858: step 4416, loss 0.110112, acc 0.96875\n",
      "2018-04-11T16:05:17.100175: step 4417, loss 0.105585, acc 0.984375\n",
      "2018-04-11T16:05:17.668919: step 4418, loss 0.251516, acc 0.953125\n",
      "2018-04-11T16:05:18.231934: step 4419, loss 0.109965, acc 0.96875\n",
      "2018-04-11T16:05:18.807427: step 4420, loss 0.12839, acc 0.96875\n",
      "2018-04-11T16:05:19.397805: step 4421, loss 0.173726, acc 0.96875\n",
      "2018-04-11T16:05:19.980120: step 4422, loss 0.126698, acc 0.96875\n",
      "2018-04-11T16:05:20.575418: step 4423, loss 0.142997, acc 0.96875\n",
      "2018-04-11T16:05:21.183085: step 4424, loss 0.0727337, acc 0.984375\n",
      "2018-04-11T16:05:21.830481: step 4425, loss 0.138504, acc 0.984375\n",
      "2018-04-11T16:05:22.445623: step 4426, loss 0.18903, acc 0.953125\n",
      "2018-04-11T16:05:23.125054: step 4427, loss 0.197121, acc 0.9375\n",
      "2018-04-11T16:05:23.778989: step 4428, loss 0.10035, acc 0.96875\n",
      "2018-04-11T16:05:24.355922: step 4429, loss 0.0743023, acc 0.984375\n",
      "2018-04-11T16:05:24.939399: step 4430, loss 0.090612, acc 1\n",
      "2018-04-11T16:05:25.537846: step 4431, loss 0.175615, acc 0.9375\n",
      "2018-04-11T16:05:26.164510: step 4432, loss 0.161077, acc 0.9375\n",
      "2018-04-11T16:05:26.811990: step 4433, loss 0.127677, acc 0.9375\n",
      "2018-04-11T16:05:27.439508: step 4434, loss 0.0955597, acc 0.984375\n",
      "2018-04-11T16:05:28.237830: step 4435, loss 0.135095, acc 0.984375\n",
      "2018-04-11T16:05:29.036931: step 4436, loss 0.118618, acc 0.96875\n",
      "2018-04-11T16:05:29.946548: step 4437, loss 0.11386, acc 0.96875\n",
      "2018-04-11T16:05:30.891830: step 4438, loss 0.15724, acc 0.9375\n",
      "2018-04-11T16:05:31.561150: step 4439, loss 0.180343, acc 0.9375\n",
      "2018-04-11T16:05:32.148034: step 4440, loss 0.219255, acc 0.953125\n",
      "2018-04-11T16:05:32.782013: step 4441, loss 0.126214, acc 0.96875\n",
      "2018-04-11T16:05:33.371298: step 4442, loss 0.162794, acc 0.9375\n",
      "2018-04-11T16:05:34.001960: step 4443, loss 0.113206, acc 0.96875\n",
      "2018-04-11T16:05:34.611620: step 4444, loss 0.128191, acc 0.96875\n",
      "2018-04-11T16:05:35.260717: step 4445, loss 0.132416, acc 0.984375\n",
      "2018-04-11T16:05:35.893420: step 4446, loss 0.0906088, acc 0.984375\n",
      "2018-04-11T16:05:36.489727: step 4447, loss 0.121798, acc 0.96875\n",
      "2018-04-11T16:05:37.201289: step 4448, loss 0.10191, acc 1\n",
      "2018-04-11T16:05:37.807219: step 4449, loss 0.113959, acc 0.984375\n",
      "2018-04-11T16:05:38.432245: step 4450, loss 0.133537, acc 0.984375\n",
      "2018-04-11T16:05:39.030892: step 4451, loss 0.176424, acc 0.921875\n",
      "2018-04-11T16:05:39.655011: step 4452, loss 0.0646719, acc 1\n",
      "2018-04-11T16:05:40.244554: step 4453, loss 0.0858052, acc 1\n",
      "2018-04-11T16:05:40.864804: step 4454, loss 0.25844, acc 0.859375\n",
      "2018-04-11T16:05:41.461066: step 4455, loss 0.106496, acc 0.984375\n",
      "2018-04-11T16:05:42.027546: step 4456, loss 0.135394, acc 0.953125\n",
      "2018-04-11T16:05:42.686267: step 4457, loss 0.0979816, acc 0.96875\n",
      "2018-04-11T16:05:43.303087: step 4458, loss 0.106352, acc 0.96875\n",
      "2018-04-11T16:05:43.893368: step 4459, loss 0.186067, acc 0.9375\n",
      "2018-04-11T16:05:44.519293: step 4460, loss 0.18909, acc 0.953125\n",
      "2018-04-11T16:05:45.122014: step 4461, loss 0.101244, acc 0.96875\n",
      "2018-04-11T16:05:45.683967: step 4462, loss 0.11722, acc 0.96875\n",
      "2018-04-11T16:05:46.357929: step 4463, loss 0.112671, acc 0.984375\n",
      "2018-04-11T16:05:46.944258: step 4464, loss 0.127304, acc 0.953125\n",
      "2018-04-11T16:05:47.535625: step 4465, loss 0.17567, acc 0.9375\n",
      "2018-04-11T16:05:48.136357: step 4466, loss 0.283179, acc 0.921875\n",
      "2018-04-11T16:05:48.716644: step 4467, loss 0.13104, acc 0.9375\n",
      "2018-04-11T16:05:49.306214: step 4468, loss 0.0758969, acc 1\n",
      "2018-04-11T16:05:49.912454: step 4469, loss 0.0555987, acc 1\n",
      "2018-04-11T16:05:50.515508: step 4470, loss 0.121709, acc 0.96875\n",
      "2018-04-11T16:05:51.112431: step 4471, loss 0.230872, acc 0.9375\n",
      "2018-04-11T16:05:51.731657: step 4472, loss 0.260898, acc 0.875\n",
      "2018-04-11T16:05:52.329774: step 4473, loss 0.192618, acc 0.96875\n",
      "2018-04-11T16:05:52.947808: step 4474, loss 0.210944, acc 0.9375\n",
      "2018-04-11T16:05:53.543700: step 4475, loss 0.124553, acc 0.96875\n",
      "2018-04-11T16:05:54.131145: step 4476, loss 0.144911, acc 0.9375\n",
      "2018-04-11T16:05:54.727585: step 4477, loss 0.102778, acc 0.984375\n",
      "2018-04-11T16:05:55.287211: step 4478, loss 0.103556, acc 0.96875\n",
      "2018-04-11T16:05:55.843020: step 4479, loss 0.180222, acc 0.953125\n",
      "2018-04-11T16:05:56.450276: step 4480, loss 0.0618654, acc 1\n",
      "2018-04-11T16:05:57.080072: step 4481, loss 0.0809901, acc 0.96875\n",
      "2018-04-11T16:05:57.658960: step 4482, loss 0.241572, acc 0.9375\n",
      "2018-04-11T16:05:58.262288: step 4483, loss 0.11953, acc 0.96875\n",
      "2018-04-11T16:05:58.861111: step 4484, loss 0.171234, acc 0.953125\n",
      "2018-04-11T16:05:59.482283: step 4485, loss 0.071193, acc 0.984375\n",
      "2018-04-11T16:06:00.078783: step 4486, loss 0.168964, acc 0.953125\n",
      "2018-04-11T16:06:00.686103: step 4487, loss 0.12094, acc 0.953125\n",
      "2018-04-11T16:06:01.264741: step 4488, loss 0.226567, acc 0.90625\n",
      "2018-04-11T16:06:01.853550: step 4489, loss 0.0706243, acc 1\n",
      "2018-04-11T16:06:02.482606: step 4490, loss 0.151277, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T16:06:03.138855: step 4491, loss 0.101194, acc 0.96875\n",
      "2018-04-11T16:06:03.753697: step 4492, loss 0.118903, acc 0.984375\n",
      "2018-04-11T16:06:04.327500: step 4493, loss 0.0926014, acc 0.96875\n",
      "2018-04-11T16:06:04.915087: step 4494, loss 0.130392, acc 0.96875\n",
      "2018-04-11T16:06:05.514377: step 4495, loss 0.14665, acc 0.96875\n",
      "2018-04-11T16:06:06.100474: step 4496, loss 0.0989614, acc 1\n",
      "2018-04-11T16:06:06.660109: step 4497, loss 0.132387, acc 0.96875\n",
      "2018-04-11T16:06:07.235535: step 4498, loss 0.203231, acc 0.9375\n",
      "2018-04-11T16:06:07.828264: step 4499, loss 0.0891519, acc 0.984375\n",
      "2018-04-11T16:06:08.402887: step 4500, loss 0.100165, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T16:06:08.576819: step 4500, loss 0.686244, acc 0.81\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-4500\n",
      "\n",
      "2018-04-11T16:06:09.292236: step 4501, loss 0.0926337, acc 0.984375\n",
      "2018-04-11T16:06:09.909753: step 4502, loss 0.0811002, acc 1\n",
      "2018-04-11T16:06:10.500456: step 4503, loss 0.1311, acc 0.953125\n",
      "2018-04-11T16:06:11.087481: step 4504, loss 0.173008, acc 0.953125\n",
      "2018-04-11T16:06:11.671600: step 4505, loss 0.104701, acc 0.984375\n",
      "2018-04-11T16:06:12.274608: step 4506, loss 0.0807856, acc 0.984375\n",
      "2018-04-11T16:06:12.915282: step 4507, loss 0.155886, acc 0.953125\n",
      "2018-04-11T16:06:13.468886: step 4508, loss 0.0724668, acc 0.984375\n",
      "2018-04-11T16:06:14.080124: step 4509, loss 0.13629, acc 0.96875\n",
      "2018-04-11T16:06:14.647614: step 4510, loss 0.0656932, acc 0.984375\n",
      "2018-04-11T16:06:15.219407: step 4511, loss 0.101371, acc 0.984375\n",
      "2018-04-11T16:06:15.787467: step 4512, loss 0.11456, acc 0.984375\n",
      "2018-04-11T16:06:16.360915: step 4513, loss 0.115907, acc 0.96875\n",
      "2018-04-11T16:06:16.944197: step 4514, loss 0.185244, acc 0.9375\n",
      "2018-04-11T16:06:17.519963: step 4515, loss 0.141757, acc 0.96875\n",
      "2018-04-11T16:06:18.104431: step 4516, loss 0.141243, acc 0.96875\n",
      "2018-04-11T16:06:18.689942: step 4517, loss 0.0866334, acc 0.984375\n",
      "2018-04-11T16:06:19.253860: step 4518, loss 0.297172, acc 0.9375\n",
      "2018-04-11T16:06:19.859383: step 4519, loss 0.164624, acc 0.953125\n",
      "2018-04-11T16:06:20.431830: step 4520, loss 0.208698, acc 0.953125\n",
      "2018-04-11T16:06:20.998871: step 4521, loss 0.0903985, acc 0.984375\n",
      "2018-04-11T16:06:21.599293: step 4522, loss 0.109678, acc 0.96875\n",
      "2018-04-11T16:06:22.183336: step 4523, loss 0.108221, acc 0.953125\n",
      "2018-04-11T16:06:22.772318: step 4524, loss 0.13405, acc 0.953125\n",
      "2018-04-11T16:06:23.348045: step 4525, loss 0.208754, acc 0.921875\n",
      "2018-04-11T16:06:23.920272: step 4526, loss 0.0739163, acc 1\n",
      "2018-04-11T16:06:24.500925: step 4527, loss 0.106911, acc 0.984375\n",
      "2018-04-11T16:06:25.064519: step 4528, loss 0.0890602, acc 0.984375\n",
      "2018-04-11T16:06:25.642615: step 4529, loss 0.155791, acc 0.96875\n",
      "2018-04-11T16:06:26.210044: step 4530, loss 0.179241, acc 0.96875\n",
      "2018-04-11T16:06:26.807803: step 4531, loss 0.155466, acc 0.96875\n",
      "2018-04-11T16:06:27.388099: step 4532, loss 0.0799373, acc 0.984375\n",
      "2018-04-11T16:06:27.954954: step 4533, loss 0.154794, acc 0.953125\n",
      "2018-04-11T16:06:28.528256: step 4534, loss 0.205343, acc 0.953125\n",
      "2018-04-11T16:06:29.084429: step 4535, loss 0.124238, acc 0.96875\n",
      "2018-04-11T16:06:29.654600: step 4536, loss 0.106972, acc 0.96875\n",
      "2018-04-11T16:06:30.268047: step 4537, loss 0.114775, acc 0.984375\n",
      "2018-04-11T16:06:30.857609: step 4538, loss 0.14239, acc 0.96875\n",
      "2018-04-11T16:06:31.459017: step 4539, loss 0.169851, acc 0.96875\n",
      "2018-04-11T16:06:32.033254: step 4540, loss 0.0861008, acc 0.984375\n",
      "2018-04-11T16:06:32.606765: step 4541, loss 0.114744, acc 0.96875\n",
      "2018-04-11T16:06:33.194492: step 4542, loss 0.242131, acc 0.921875\n",
      "2018-04-11T16:06:33.788890: step 4543, loss 0.0899785, acc 0.984375\n",
      "2018-04-11T16:06:34.347712: step 4544, loss 0.142635, acc 0.953125\n",
      "2018-04-11T16:06:34.924074: step 4545, loss 0.0985674, acc 0.984375\n",
      "2018-04-11T16:06:35.478690: step 4546, loss 0.108479, acc 0.96875\n",
      "2018-04-11T16:06:36.050727: step 4547, loss 0.14663, acc 0.953125\n",
      "2018-04-11T16:06:36.601857: step 4548, loss 0.20731, acc 0.921875\n",
      "2018-04-11T16:06:37.224953: step 4549, loss 0.274593, acc 0.921875\n",
      "2018-04-11T16:06:37.913912: step 4550, loss 0.103414, acc 0.984375\n",
      "2018-04-11T16:06:38.518028: step 4551, loss 0.146239, acc 0.953125\n",
      "2018-04-11T16:06:39.107407: step 4552, loss 0.186423, acc 0.953125\n",
      "2018-04-11T16:06:39.679324: step 4553, loss 0.216353, acc 0.953125\n",
      "2018-04-11T16:06:40.353585: step 4554, loss 0.109102, acc 0.984375\n",
      "2018-04-11T16:06:40.928468: step 4555, loss 0.129543, acc 0.96875\n",
      "2018-04-11T16:06:41.499510: step 4556, loss 0.106304, acc 0.953125\n",
      "2018-04-11T16:06:42.056283: step 4557, loss 0.16863, acc 0.921875\n",
      "2018-04-11T16:06:42.621816: step 4558, loss 0.24017, acc 0.90625\n",
      "2018-04-11T16:06:43.360924: step 4559, loss 0.108626, acc 0.96875\n",
      "2018-04-11T16:06:43.916027: step 4560, loss 0.130581, acc 0.953125\n",
      "2018-04-11T16:06:44.594989: step 4561, loss 0.0821667, acc 1\n",
      "2018-04-11T16:06:45.371089: step 4562, loss 0.148675, acc 0.953125\n",
      "2018-04-11T16:06:45.985471: step 4563, loss 0.0841291, acc 1\n",
      "2018-04-11T16:06:46.565423: step 4564, loss 0.140905, acc 0.9375\n",
      "2018-04-11T16:06:47.202370: step 4565, loss 0.196481, acc 0.90625\n",
      "2018-04-11T16:06:47.887661: step 4566, loss 0.0894526, acc 0.96875\n",
      "2018-04-11T16:06:48.527196: step 4567, loss 0.143731, acc 0.96875\n",
      "2018-04-11T16:06:49.186478: step 4568, loss 0.14632, acc 0.96875\n",
      "2018-04-11T16:06:49.915055: step 4569, loss 0.124979, acc 0.96875\n",
      "2018-04-11T16:06:50.484971: step 4570, loss 0.18331, acc 0.9375\n",
      "2018-04-11T16:06:51.120496: step 4571, loss 0.124777, acc 0.984375\n",
      "2018-04-11T16:06:51.704827: step 4572, loss 0.130785, acc 0.953125\n",
      "2018-04-11T16:06:52.299812: step 4573, loss 0.106423, acc 0.96875\n",
      "2018-04-11T16:06:52.913706: step 4574, loss 0.102141, acc 0.984375\n",
      "2018-04-11T16:06:53.513751: step 4575, loss 0.102309, acc 1\n",
      "2018-04-11T16:06:54.108683: step 4576, loss 0.150217, acc 0.9375\n",
      "2018-04-11T16:06:54.683793: step 4577, loss 0.183512, acc 0.921875\n",
      "2018-04-11T16:06:55.319264: step 4578, loss 0.141469, acc 0.984375\n",
      "2018-04-11T16:06:55.961850: step 4579, loss 0.140964, acc 0.96875\n",
      "2018-04-11T16:06:56.613748: step 4580, loss 0.23438, acc 0.90625\n",
      "2018-04-11T16:06:57.215640: step 4581, loss 0.0969155, acc 0.984375\n",
      "2018-04-11T16:06:57.862008: step 4582, loss 0.119708, acc 0.953125\n",
      "2018-04-11T16:06:58.536250: step 4583, loss 0.0606005, acc 1\n",
      "2018-04-11T16:06:59.117325: step 4584, loss 0.150914, acc 0.9375\n",
      "2018-04-11T16:06:59.706000: step 4585, loss 0.107611, acc 0.96875\n",
      "2018-04-11T16:07:00.392500: step 4586, loss 0.123117, acc 0.96875\n",
      "2018-04-11T16:07:01.010973: step 4587, loss 0.177614, acc 0.953125\n",
      "2018-04-11T16:07:01.599379: step 4588, loss 0.150031, acc 0.953125\n",
      "2018-04-11T16:07:02.186663: step 4589, loss 0.202966, acc 0.921875\n",
      "2018-04-11T16:07:02.769633: step 4590, loss 0.144801, acc 0.953125\n",
      "2018-04-11T16:07:03.351377: step 4591, loss 0.115336, acc 0.984375\n",
      "2018-04-11T16:07:03.905404: step 4592, loss 0.177802, acc 0.9375\n",
      "2018-04-11T16:07:04.489342: step 4593, loss 0.13826, acc 0.96875\n",
      "2018-04-11T16:07:05.077562: step 4594, loss 0.197618, acc 0.90625\n",
      "2018-04-11T16:07:05.643407: step 4595, loss 0.154281, acc 0.9375\n",
      "2018-04-11T16:07:06.217028: step 4596, loss 0.096548, acc 1\n",
      "2018-04-11T16:07:06.776023: step 4597, loss 0.142947, acc 0.953125\n",
      "2018-04-11T16:07:07.340412: step 4598, loss 0.105376, acc 0.984375\n",
      "2018-04-11T16:07:07.902703: step 4599, loss 0.186336, acc 0.953125\n",
      "2018-04-11T16:07:08.534741: step 4600, loss 0.142533, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T16:07:08.716304: step 4600, loss 0.895621, acc 0.81\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-4600\n",
      "\n",
      "2018-04-11T16:07:09.430183: step 4601, loss 0.081313, acc 1\n",
      "2018-04-11T16:07:10.005999: step 4602, loss 0.120525, acc 0.96875\n",
      "2018-04-11T16:07:10.571255: step 4603, loss 0.157649, acc 0.953125\n",
      "2018-04-11T16:07:11.131172: step 4604, loss 0.144598, acc 0.984375\n",
      "2018-04-11T16:07:11.691708: step 4605, loss 0.103995, acc 0.984375\n",
      "2018-04-11T16:07:12.268791: step 4606, loss 0.2171, acc 0.90625\n",
      "2018-04-11T16:07:12.816998: step 4607, loss 0.0816435, acc 1\n",
      "2018-04-11T16:07:13.390289: step 4608, loss 0.072672, acc 0.984375\n",
      "2018-04-11T16:07:13.951940: step 4609, loss 0.0825436, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T16:07:14.501232: step 4610, loss 0.100926, acc 0.984375\n",
      "2018-04-11T16:07:15.082205: step 4611, loss 0.127439, acc 0.953125\n",
      "2018-04-11T16:07:15.658045: step 4612, loss 0.242356, acc 0.96875\n",
      "2018-04-11T16:07:16.236693: step 4613, loss 0.247598, acc 0.921875\n",
      "2018-04-11T16:07:16.803678: step 4614, loss 0.14129, acc 0.96875\n",
      "2018-04-11T16:07:17.369560: step 4615, loss 0.18344, acc 0.96875\n",
      "2018-04-11T16:07:17.958335: step 4616, loss 0.267002, acc 0.921875\n",
      "2018-04-11T16:07:18.180803: step 4617, loss 0.176341, acc 0.916667\n",
      "2018-04-11T16:07:18.767861: step 4618, loss 0.0939993, acc 0.984375\n",
      "2018-04-11T16:07:19.349007: step 4619, loss 0.132832, acc 0.984375\n",
      "2018-04-11T16:07:19.903329: step 4620, loss 0.092603, acc 0.96875\n",
      "2018-04-11T16:07:20.451788: step 4621, loss 0.204318, acc 0.9375\n",
      "2018-04-11T16:07:21.011896: step 4622, loss 0.133427, acc 0.984375\n",
      "2018-04-11T16:07:21.574690: step 4623, loss 0.0759372, acc 0.984375\n",
      "2018-04-11T16:07:22.139704: step 4624, loss 0.0751762, acc 0.984375\n",
      "2018-04-11T16:07:22.708228: step 4625, loss 0.12372, acc 0.96875\n",
      "2018-04-11T16:07:23.272536: step 4626, loss 0.104479, acc 1\n",
      "2018-04-11T16:07:23.836971: step 4627, loss 0.108592, acc 0.96875\n",
      "2018-04-11T16:07:24.422234: step 4628, loss 0.0742188, acc 1\n",
      "2018-04-11T16:07:24.990747: step 4629, loss 0.0910043, acc 1\n",
      "2018-04-11T16:07:25.547257: step 4630, loss 0.0617453, acc 0.984375\n",
      "2018-04-11T16:07:26.113576: step 4631, loss 0.0474545, acc 1\n",
      "2018-04-11T16:07:26.657696: step 4632, loss 0.125091, acc 0.96875\n",
      "2018-04-11T16:07:27.228080: step 4633, loss 0.152395, acc 0.96875\n",
      "2018-04-11T16:07:27.805753: step 4634, loss 0.133234, acc 0.953125\n",
      "2018-04-11T16:07:28.351062: step 4635, loss 0.112642, acc 0.96875\n",
      "2018-04-11T16:07:28.896973: step 4636, loss 0.112121, acc 0.953125\n",
      "2018-04-11T16:07:29.442350: step 4637, loss 0.173979, acc 0.953125\n",
      "2018-04-11T16:07:30.094950: step 4638, loss 0.137195, acc 0.9375\n",
      "2018-04-11T16:07:30.662446: step 4639, loss 0.0719465, acc 1\n",
      "2018-04-11T16:07:31.233255: step 4640, loss 0.0715998, acc 0.96875\n",
      "2018-04-11T16:07:31.805791: step 4641, loss 0.146435, acc 0.921875\n",
      "2018-04-11T16:07:32.358741: step 4642, loss 0.15288, acc 0.953125\n",
      "2018-04-11T16:07:32.920090: step 4643, loss 0.139447, acc 0.953125\n",
      "2018-04-11T16:07:33.479536: step 4644, loss 0.0868287, acc 0.984375\n",
      "2018-04-11T16:07:34.054103: step 4645, loss 0.0655253, acc 1\n",
      "2018-04-11T16:07:34.608379: step 4646, loss 0.0936462, acc 0.96875\n",
      "2018-04-11T16:07:35.160664: step 4647, loss 0.115176, acc 0.96875\n",
      "2018-04-11T16:07:35.736760: step 4648, loss 0.108211, acc 0.984375\n",
      "2018-04-11T16:07:36.297280: step 4649, loss 0.164353, acc 0.953125\n",
      "2018-04-11T16:07:36.859254: step 4650, loss 0.109775, acc 0.984375\n",
      "2018-04-11T16:07:37.439893: step 4651, loss 0.0791091, acc 1\n",
      "2018-04-11T16:07:38.016966: step 4652, loss 0.0840795, acc 1\n",
      "2018-04-11T16:07:38.547814: step 4653, loss 0.129273, acc 0.984375\n",
      "2018-04-11T16:07:39.116286: step 4654, loss 0.0764898, acc 1\n",
      "2018-04-11T16:07:39.673859: step 4655, loss 0.129809, acc 0.953125\n",
      "2018-04-11T16:07:40.256702: step 4656, loss 0.154891, acc 0.921875\n",
      "2018-04-11T16:07:40.833949: step 4657, loss 0.0894059, acc 0.984375\n",
      "2018-04-11T16:07:41.384300: step 4658, loss 0.065186, acc 1\n",
      "2018-04-11T16:07:41.967099: step 4659, loss 0.116992, acc 0.96875\n",
      "2018-04-11T16:07:42.514800: step 4660, loss 0.0625146, acc 1\n",
      "2018-04-11T16:07:43.086723: step 4661, loss 0.107839, acc 0.984375\n",
      "2018-04-11T16:07:43.694519: step 4662, loss 0.0571772, acc 1\n",
      "2018-04-11T16:07:44.266216: step 4663, loss 0.0463996, acc 1\n",
      "2018-04-11T16:07:44.836804: step 4664, loss 0.0981749, acc 0.984375\n",
      "2018-04-11T16:07:45.421341: step 4665, loss 0.158303, acc 0.953125\n",
      "2018-04-11T16:07:45.997547: step 4666, loss 0.0731507, acc 1\n",
      "2018-04-11T16:07:46.589745: step 4667, loss 0.0859667, acc 0.984375\n",
      "2018-04-11T16:07:47.151546: step 4668, loss 0.0616879, acc 1\n",
      "2018-04-11T16:07:47.727511: step 4669, loss 0.0904222, acc 0.984375\n",
      "2018-04-11T16:07:48.305585: step 4670, loss 0.051349, acc 1\n",
      "2018-04-11T16:07:48.877448: step 4671, loss 0.152237, acc 0.953125\n",
      "2018-04-11T16:07:49.427003: step 4672, loss 0.195849, acc 0.953125\n",
      "2018-04-11T16:07:49.997507: step 4673, loss 0.10761, acc 0.96875\n",
      "2018-04-11T16:07:50.556400: step 4674, loss 0.121392, acc 0.96875\n",
      "2018-04-11T16:07:51.140039: step 4675, loss 0.126683, acc 0.984375\n",
      "2018-04-11T16:07:51.702643: step 4676, loss 0.141384, acc 0.96875\n",
      "2018-04-11T16:07:52.249821: step 4677, loss 0.109467, acc 0.984375\n",
      "2018-04-11T16:07:52.803317: step 4678, loss 0.16705, acc 0.9375\n",
      "2018-04-11T16:07:53.345718: step 4679, loss 0.106042, acc 0.984375\n",
      "2018-04-11T16:07:53.898614: step 4680, loss 0.0658167, acc 1\n",
      "2018-04-11T16:07:54.454022: step 4681, loss 0.160341, acc 0.953125\n",
      "2018-04-11T16:07:55.037768: step 4682, loss 0.103099, acc 0.984375\n",
      "2018-04-11T16:07:55.592937: step 4683, loss 0.100798, acc 0.96875\n",
      "2018-04-11T16:07:56.159987: step 4684, loss 0.168005, acc 0.953125\n",
      "2018-04-11T16:07:56.725533: step 4685, loss 0.0733491, acc 1\n",
      "2018-04-11T16:07:57.285651: step 4686, loss 0.135234, acc 0.96875\n",
      "2018-04-11T16:07:57.848051: step 4687, loss 0.0543543, acc 1\n",
      "2018-04-11T16:07:58.400739: step 4688, loss 0.235709, acc 0.90625\n",
      "2018-04-11T16:07:58.981840: step 4689, loss 0.0970004, acc 0.984375\n",
      "2018-04-11T16:07:59.550278: step 4690, loss 0.161791, acc 0.953125\n",
      "2018-04-11T16:08:00.129749: step 4691, loss 0.156303, acc 0.96875\n",
      "2018-04-11T16:08:00.685551: step 4692, loss 0.0857728, acc 0.984375\n",
      "2018-04-11T16:08:01.264530: step 4693, loss 0.124069, acc 0.984375\n",
      "2018-04-11T16:08:01.829504: step 4694, loss 0.0944716, acc 1\n",
      "2018-04-11T16:08:02.405916: step 4695, loss 0.0516339, acc 1\n",
      "2018-04-11T16:08:02.976831: step 4696, loss 0.204414, acc 0.953125\n",
      "2018-04-11T16:08:03.536957: step 4697, loss 0.192383, acc 0.921875\n",
      "2018-04-11T16:08:04.101893: step 4698, loss 0.077308, acc 1\n",
      "2018-04-11T16:08:04.681788: step 4699, loss 0.10643, acc 0.984375\n",
      "2018-04-11T16:08:05.269296: step 4700, loss 0.126927, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T16:08:05.451491: step 4700, loss 0.826361, acc 0.8\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-4700\n",
      "\n",
      "2018-04-11T16:08:06.127253: step 4701, loss 0.132624, acc 0.953125\n",
      "2018-04-11T16:08:06.701452: step 4702, loss 0.115972, acc 0.953125\n",
      "2018-04-11T16:08:07.248034: step 4703, loss 0.165535, acc 0.96875\n",
      "2018-04-11T16:08:07.822046: step 4704, loss 0.16783, acc 0.953125\n",
      "2018-04-11T16:08:08.403358: step 4705, loss 0.16548, acc 0.96875\n",
      "2018-04-11T16:08:08.987778: step 4706, loss 0.13645, acc 0.953125\n",
      "2018-04-11T16:08:09.564188: step 4707, loss 0.101987, acc 0.984375\n",
      "2018-04-11T16:08:10.134863: step 4708, loss 0.103223, acc 0.984375\n",
      "2018-04-11T16:08:10.683719: step 4709, loss 0.0715083, acc 1\n",
      "2018-04-11T16:08:11.255401: step 4710, loss 0.108621, acc 0.96875\n",
      "2018-04-11T16:08:11.833981: step 4711, loss 0.130684, acc 0.96875\n",
      "2018-04-11T16:08:12.387440: step 4712, loss 0.069355, acc 0.984375\n",
      "2018-04-11T16:08:12.965596: step 4713, loss 0.101962, acc 0.984375\n",
      "2018-04-11T16:08:13.514915: step 4714, loss 0.137264, acc 0.984375\n",
      "2018-04-11T16:08:14.090760: step 4715, loss 0.150017, acc 0.96875\n",
      "2018-04-11T16:08:14.664764: step 4716, loss 0.0829467, acc 0.984375\n",
      "2018-04-11T16:08:15.216148: step 4717, loss 0.211611, acc 0.921875\n",
      "2018-04-11T16:08:15.768655: step 4718, loss 0.0844734, acc 0.984375\n",
      "2018-04-11T16:08:16.322810: step 4719, loss 0.19935, acc 0.921875\n",
      "2018-04-11T16:08:16.870105: step 4720, loss 0.0643362, acc 1\n",
      "2018-04-11T16:08:17.414080: step 4721, loss 0.0898796, acc 0.984375\n",
      "2018-04-11T16:08:17.982079: step 4722, loss 0.173886, acc 0.921875\n",
      "2018-04-11T16:08:18.529590: step 4723, loss 0.0723406, acc 0.984375\n",
      "2018-04-11T16:08:19.077462: step 4724, loss 0.0923753, acc 0.984375\n",
      "2018-04-11T16:08:19.656650: step 4725, loss 0.0705724, acc 1\n",
      "2018-04-11T16:08:20.207956: step 4726, loss 0.113513, acc 0.96875\n",
      "2018-04-11T16:08:20.757602: step 4727, loss 0.172929, acc 0.9375\n",
      "2018-04-11T16:08:21.308600: step 4728, loss 0.0871164, acc 0.96875\n",
      "2018-04-11T16:08:21.882573: step 4729, loss 0.129477, acc 0.96875\n",
      "2018-04-11T16:08:22.454290: step 4730, loss 0.138519, acc 0.953125\n",
      "2018-04-11T16:08:23.018064: step 4731, loss 0.0572188, acc 1\n",
      "2018-04-11T16:08:23.576621: step 4732, loss 0.127022, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T16:08:24.124224: step 4733, loss 0.102253, acc 0.953125\n",
      "2018-04-11T16:08:24.691443: step 4734, loss 0.0692007, acc 1\n",
      "2018-04-11T16:08:25.236202: step 4735, loss 0.154805, acc 0.96875\n",
      "2018-04-11T16:08:25.786970: step 4736, loss 0.18051, acc 0.9375\n",
      "2018-04-11T16:08:26.392583: step 4737, loss 0.0994747, acc 0.96875\n",
      "2018-04-11T16:08:26.963827: step 4738, loss 0.0890088, acc 0.984375\n",
      "2018-04-11T16:08:27.579074: step 4739, loss 0.0675684, acc 1\n",
      "2018-04-11T16:08:28.129942: step 4740, loss 0.0844927, acc 0.984375\n",
      "2018-04-11T16:08:28.674416: step 4741, loss 0.0864071, acc 0.984375\n",
      "2018-04-11T16:08:29.226379: step 4742, loss 0.239143, acc 0.9375\n",
      "2018-04-11T16:08:29.849110: step 4743, loss 0.105813, acc 0.984375\n",
      "2018-04-11T16:08:30.473822: step 4744, loss 0.0635528, acc 0.984375\n",
      "2018-04-11T16:08:31.174338: step 4745, loss 0.0751867, acc 0.984375\n",
      "2018-04-11T16:08:31.745093: step 4746, loss 0.068537, acc 0.984375\n",
      "2018-04-11T16:08:32.351148: step 4747, loss 0.122305, acc 0.984375\n",
      "2018-04-11T16:08:32.957533: step 4748, loss 0.111089, acc 0.96875\n",
      "2018-04-11T16:08:33.557743: step 4749, loss 0.115772, acc 0.96875\n",
      "2018-04-11T16:08:34.159529: step 4750, loss 0.114412, acc 0.984375\n",
      "2018-04-11T16:08:34.711405: step 4751, loss 0.175247, acc 0.96875\n",
      "2018-04-11T16:08:35.341819: step 4752, loss 0.114749, acc 0.96875\n",
      "2018-04-11T16:08:36.012404: step 4753, loss 0.0859638, acc 0.984375\n",
      "2018-04-11T16:08:36.728864: step 4754, loss 0.0933531, acc 0.984375\n",
      "2018-04-11T16:08:37.401867: step 4755, loss 0.0707162, acc 0.984375\n",
      "2018-04-11T16:08:37.987429: step 4756, loss 0.108674, acc 0.984375\n",
      "2018-04-11T16:08:38.602290: step 4757, loss 0.16002, acc 0.984375\n",
      "2018-04-11T16:08:39.245315: step 4758, loss 0.132263, acc 0.953125\n",
      "2018-04-11T16:08:39.856300: step 4759, loss 0.0784156, acc 0.984375\n",
      "2018-04-11T16:08:40.459952: step 4760, loss 0.10162, acc 0.96875\n",
      "2018-04-11T16:08:41.034917: step 4761, loss 0.123792, acc 0.96875\n",
      "2018-04-11T16:08:41.598443: step 4762, loss 0.120027, acc 0.953125\n",
      "2018-04-11T16:08:42.219508: step 4763, loss 0.0602176, acc 1\n",
      "2018-04-11T16:08:42.824681: step 4764, loss 0.129731, acc 0.96875\n",
      "2018-04-11T16:08:43.464826: step 4765, loss 0.15825, acc 0.984375\n",
      "2018-04-11T16:08:44.101867: step 4766, loss 0.122532, acc 0.96875\n",
      "2018-04-11T16:08:44.716042: step 4767, loss 0.149129, acc 0.96875\n",
      "2018-04-11T16:08:45.345351: step 4768, loss 0.18256, acc 0.953125\n",
      "2018-04-11T16:08:45.960145: step 4769, loss 0.155278, acc 0.9375\n",
      "2018-04-11T16:08:46.559540: step 4770, loss 0.0923074, acc 0.96875\n",
      "2018-04-11T16:08:47.281963: step 4771, loss 0.0844765, acc 1\n",
      "2018-04-11T16:08:47.906680: step 4772, loss 0.104054, acc 0.984375\n",
      "2018-04-11T16:08:48.534694: step 4773, loss 0.0743971, acc 1\n",
      "2018-04-11T16:08:49.102966: step 4774, loss 0.157691, acc 0.953125\n",
      "2018-04-11T16:08:49.699659: step 4775, loss 0.086914, acc 0.984375\n",
      "2018-04-11T16:08:50.309233: step 4776, loss 0.103533, acc 1\n",
      "2018-04-11T16:08:50.935859: step 4777, loss 0.0541506, acc 1\n",
      "2018-04-11T16:08:51.517748: step 4778, loss 0.128704, acc 0.984375\n",
      "2018-04-11T16:08:52.107691: step 4779, loss 0.0758527, acc 1\n",
      "2018-04-11T16:08:52.685805: step 4780, loss 0.139757, acc 0.953125\n",
      "2018-04-11T16:08:53.280255: step 4781, loss 0.101929, acc 0.953125\n",
      "2018-04-11T16:08:53.876641: step 4782, loss 0.0850876, acc 0.984375\n",
      "2018-04-11T16:08:54.503477: step 4783, loss 0.0689551, acc 0.984375\n",
      "2018-04-11T16:08:55.098082: step 4784, loss 0.188761, acc 0.96875\n",
      "2018-04-11T16:08:55.669491: step 4785, loss 0.0813231, acc 0.984375\n",
      "2018-04-11T16:08:56.252345: step 4786, loss 0.118897, acc 0.96875\n",
      "2018-04-11T16:08:56.797756: step 4787, loss 0.0496121, acc 1\n",
      "2018-04-11T16:08:57.357524: step 4788, loss 0.0601932, acc 1\n",
      "2018-04-11T16:08:57.934465: step 4789, loss 0.0637462, acc 0.984375\n",
      "2018-04-11T16:08:58.512248: step 4790, loss 0.0708796, acc 0.984375\n",
      "2018-04-11T16:08:59.072934: step 4791, loss 0.10586, acc 0.96875\n",
      "2018-04-11T16:08:59.631969: step 4792, loss 0.13586, acc 0.96875\n",
      "2018-04-11T16:09:00.194004: step 4793, loss 0.0832307, acc 0.96875\n",
      "2018-04-11T16:09:00.837049: step 4794, loss 0.147594, acc 0.953125\n",
      "2018-04-11T16:09:01.468664: step 4795, loss 0.14499, acc 0.96875\n",
      "2018-04-11T16:09:02.061099: step 4796, loss 0.102575, acc 0.96875\n",
      "2018-04-11T16:09:02.647891: step 4797, loss 0.0835745, acc 0.984375\n",
      "2018-04-11T16:09:03.216442: step 4798, loss 0.127389, acc 0.984375\n",
      "2018-04-11T16:09:03.798682: step 4799, loss 0.0911423, acc 0.984375\n",
      "2018-04-11T16:09:04.364290: step 4800, loss 0.162817, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T16:09:04.553989: step 4800, loss 0.621493, acc 0.82\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-4800\n",
      "\n",
      "2018-04-11T16:09:05.279896: step 4801, loss 0.103794, acc 0.984375\n",
      "2018-04-11T16:09:05.867596: step 4802, loss 0.089276, acc 0.984375\n",
      "2018-04-11T16:09:06.466097: step 4803, loss 0.0726967, acc 1\n",
      "2018-04-11T16:09:07.068842: step 4804, loss 0.0908081, acc 0.984375\n",
      "2018-04-11T16:09:07.667523: step 4805, loss 0.188389, acc 0.9375\n",
      "2018-04-11T16:09:08.258855: step 4806, loss 0.0877862, acc 0.984375\n",
      "2018-04-11T16:09:08.821686: step 4807, loss 0.113689, acc 0.96875\n",
      "2018-04-11T16:09:09.445907: step 4808, loss 0.172586, acc 0.953125\n",
      "2018-04-11T16:09:10.045863: step 4809, loss 0.129072, acc 0.953125\n",
      "2018-04-11T16:09:10.667379: step 4810, loss 0.0482936, acc 1\n",
      "2018-04-11T16:09:11.293234: step 4811, loss 0.0824195, acc 1\n",
      "2018-04-11T16:09:11.976476: step 4812, loss 0.0670529, acc 0.984375\n",
      "2018-04-11T16:09:12.536461: step 4813, loss 0.085115, acc 0.984375\n",
      "2018-04-11T16:09:13.118856: step 4814, loss 0.121324, acc 0.96875\n",
      "2018-04-11T16:09:13.700657: step 4815, loss 0.08822, acc 1\n",
      "2018-04-11T16:09:14.268291: step 4816, loss 0.0887731, acc 0.96875\n",
      "2018-04-11T16:09:14.849053: step 4817, loss 0.106558, acc 0.984375\n",
      "2018-04-11T16:09:15.414364: step 4818, loss 0.0719134, acc 0.984375\n",
      "2018-04-11T16:09:15.988120: step 4819, loss 0.190801, acc 0.9375\n",
      "2018-04-11T16:09:16.577072: step 4820, loss 0.122542, acc 0.96875\n",
      "2018-04-11T16:09:17.134012: step 4821, loss 0.064302, acc 0.984375\n",
      "2018-04-11T16:09:17.721888: step 4822, loss 0.111967, acc 0.953125\n",
      "2018-04-11T16:09:18.303804: step 4823, loss 0.0550561, acc 1\n",
      "2018-04-11T16:09:18.904828: step 4824, loss 0.0926723, acc 0.984375\n",
      "2018-04-11T16:09:19.514520: step 4825, loss 0.0844797, acc 0.984375\n",
      "2018-04-11T16:09:20.168888: step 4826, loss 0.152601, acc 0.96875\n",
      "2018-04-11T16:09:20.784637: step 4827, loss 0.0835675, acc 0.984375\n",
      "2018-04-11T16:09:21.365872: step 4828, loss 0.0893211, acc 0.984375\n",
      "2018-04-11T16:09:21.985789: step 4829, loss 0.0815515, acc 0.984375\n",
      "2018-04-11T16:09:22.627453: step 4830, loss 0.229699, acc 0.921875\n",
      "2018-04-11T16:09:23.318091: step 4831, loss 0.172668, acc 0.921875\n",
      "2018-04-11T16:09:23.874008: step 4832, loss 0.143613, acc 0.953125\n",
      "2018-04-11T16:09:24.438153: step 4833, loss 0.113396, acc 0.96875\n",
      "2018-04-11T16:09:25.020682: step 4834, loss 0.0452517, acc 1\n",
      "2018-04-11T16:09:25.628789: step 4835, loss 0.0485204, acc 1\n",
      "2018-04-11T16:09:26.259406: step 4836, loss 0.118937, acc 0.953125\n",
      "2018-04-11T16:09:26.936566: step 4837, loss 0.257371, acc 0.953125\n",
      "2018-04-11T16:09:27.599189: step 4838, loss 0.0918846, acc 0.984375\n",
      "2018-04-11T16:09:28.300798: step 4839, loss 0.153578, acc 0.953125\n",
      "2018-04-11T16:09:28.998526: step 4840, loss 0.0979209, acc 0.984375\n",
      "2018-04-11T16:09:29.610875: step 4841, loss 0.0788826, acc 0.984375\n",
      "2018-04-11T16:09:30.233523: step 4842, loss 0.135915, acc 0.9375\n",
      "2018-04-11T16:09:30.813566: step 4843, loss 0.191539, acc 0.9375\n",
      "2018-04-11T16:09:31.402415: step 4844, loss 0.071751, acc 0.984375\n",
      "2018-04-11T16:09:32.003267: step 4845, loss 0.0863805, acc 0.96875\n",
      "2018-04-11T16:09:32.683596: step 4846, loss 0.0909961, acc 0.984375\n",
      "2018-04-11T16:09:33.281445: step 4847, loss 0.100109, acc 0.96875\n",
      "2018-04-11T16:09:33.930141: step 4848, loss 0.147013, acc 0.9375\n",
      "2018-04-11T16:09:34.632297: step 4849, loss 0.1085, acc 0.96875\n",
      "2018-04-11T16:09:35.206264: step 4850, loss 0.0910529, acc 0.984375\n",
      "2018-04-11T16:09:35.788853: step 4851, loss 0.115221, acc 0.96875\n",
      "2018-04-11T16:09:36.446754: step 4852, loss 0.154839, acc 0.953125\n",
      "2018-04-11T16:09:37.069247: step 4853, loss 0.128411, acc 0.953125\n",
      "2018-04-11T16:09:37.671945: step 4854, loss 0.0457797, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T16:09:38.250416: step 4855, loss 0.0994415, acc 0.96875\n",
      "2018-04-11T16:09:38.839977: step 4856, loss 0.0660897, acc 1\n",
      "2018-04-11T16:09:39.407461: step 4857, loss 0.0909621, acc 0.96875\n",
      "2018-04-11T16:09:40.066835: step 4858, loss 0.20203, acc 0.921875\n",
      "2018-04-11T16:09:40.665524: step 4859, loss 0.0736216, acc 1\n",
      "2018-04-11T16:09:41.268313: step 4860, loss 0.126566, acc 0.96875\n",
      "2018-04-11T16:09:41.837363: step 4861, loss 0.0759136, acc 0.984375\n",
      "2018-04-11T16:09:42.449905: step 4862, loss 0.119846, acc 0.9375\n",
      "2018-04-11T16:09:43.075140: step 4863, loss 0.0601849, acc 1\n",
      "2018-04-11T16:09:43.712726: step 4864, loss 0.0616174, acc 1\n",
      "2018-04-11T16:09:44.298469: step 4865, loss 0.117069, acc 0.953125\n",
      "2018-04-11T16:09:44.886452: step 4866, loss 0.186422, acc 0.96875\n",
      "2018-04-11T16:09:45.472369: step 4867, loss 0.0571362, acc 1\n",
      "2018-04-11T16:09:46.111663: step 4868, loss 0.0840809, acc 0.984375\n",
      "2018-04-11T16:09:46.776072: step 4869, loss 0.0751607, acc 1\n",
      "2018-04-11T16:09:47.332423: step 4870, loss 0.0523238, acc 1\n",
      "2018-04-11T16:09:47.962405: step 4871, loss 0.177133, acc 0.921875\n",
      "2018-04-11T16:09:48.543628: step 4872, loss 0.208937, acc 0.9375\n",
      "2018-04-11T16:09:49.120244: step 4873, loss 0.183958, acc 0.953125\n",
      "2018-04-11T16:09:49.733517: step 4874, loss 0.0771764, acc 0.984375\n",
      "2018-04-11T16:09:50.404691: step 4875, loss 0.140899, acc 0.9375\n",
      "2018-04-11T16:09:51.017495: step 4876, loss 0.0942704, acc 0.984375\n",
      "2018-04-11T16:09:51.599010: step 4877, loss 0.0902746, acc 1\n",
      "2018-04-11T16:09:52.226208: step 4878, loss 0.170772, acc 0.96875\n",
      "2018-04-11T16:09:52.835014: step 4879, loss 0.0675219, acc 1\n",
      "2018-04-11T16:09:53.407575: step 4880, loss 0.101063, acc 0.96875\n",
      "2018-04-11T16:09:53.985869: step 4881, loss 0.0942435, acc 0.96875\n",
      "2018-04-11T16:09:54.656454: step 4882, loss 0.101159, acc 0.984375\n",
      "2018-04-11T16:09:55.244056: step 4883, loss 0.129239, acc 0.96875\n",
      "2018-04-11T16:09:55.922125: step 4884, loss 0.160174, acc 0.953125\n",
      "2018-04-11T16:09:56.578645: step 4885, loss 0.0528148, acc 1\n",
      "2018-04-11T16:09:57.162991: step 4886, loss 0.0829114, acc 0.984375\n",
      "2018-04-11T16:09:57.758597: step 4887, loss 0.0844248, acc 0.984375\n",
      "2018-04-11T16:09:58.366122: step 4888, loss 0.0959589, acc 0.984375\n",
      "2018-04-11T16:09:58.978486: step 4889, loss 0.097702, acc 0.96875\n",
      "2018-04-11T16:09:59.569162: step 4890, loss 0.219763, acc 0.9375\n",
      "2018-04-11T16:10:00.206089: step 4891, loss 0.0590411, acc 1\n",
      "2018-04-11T16:10:00.926408: step 4892, loss 0.204718, acc 0.96875\n",
      "2018-04-11T16:10:01.551518: step 4893, loss 0.120559, acc 0.984375\n",
      "2018-04-11T16:10:02.141964: step 4894, loss 0.175834, acc 0.96875\n",
      "2018-04-11T16:10:02.717988: step 4895, loss 0.190351, acc 0.90625\n",
      "2018-04-11T16:10:03.269670: step 4896, loss 0.120279, acc 0.9375\n",
      "2018-04-11T16:10:03.865051: step 4897, loss 0.0587555, acc 1\n",
      "2018-04-11T16:10:04.456646: step 4898, loss 0.0764844, acc 1\n",
      "2018-04-11T16:10:05.056203: step 4899, loss 0.109729, acc 0.984375\n",
      "2018-04-11T16:10:05.653058: step 4900, loss 0.126192, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T16:10:05.838017: step 4900, loss 0.641175, acc 0.84\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-4900\n",
      "\n",
      "2018-04-11T16:10:06.592465: step 4901, loss 0.172912, acc 0.96875\n",
      "2018-04-11T16:10:07.183069: step 4902, loss 0.0805529, acc 0.984375\n",
      "2018-04-11T16:10:07.740577: step 4903, loss 0.096631, acc 0.96875\n",
      "2018-04-11T16:10:08.328844: step 4904, loss 0.193607, acc 0.9375\n",
      "2018-04-11T16:10:09.001784: step 4905, loss 0.07129, acc 1\n",
      "2018-04-11T16:10:09.613141: step 4906, loss 0.093729, acc 0.984375\n",
      "2018-04-11T16:10:10.312597: step 4907, loss 0.115454, acc 0.96875\n",
      "2018-04-11T16:10:10.899972: step 4908, loss 0.101776, acc 0.96875\n",
      "2018-04-11T16:10:11.511354: step 4909, loss 0.0756069, acc 1\n",
      "2018-04-11T16:10:12.098070: step 4910, loss 0.0464579, acc 1\n",
      "2018-04-11T16:10:12.703161: step 4911, loss 0.137901, acc 0.953125\n",
      "2018-04-11T16:10:13.312719: step 4912, loss 0.217566, acc 0.921875\n",
      "2018-04-11T16:10:13.986155: step 4913, loss 0.205512, acc 0.90625\n",
      "2018-04-11T16:10:14.564264: step 4914, loss 0.195063, acc 0.96875\n",
      "2018-04-11T16:10:15.168733: step 4915, loss 0.116335, acc 0.984375\n",
      "2018-04-11T16:10:15.751062: step 4916, loss 0.0607486, acc 1\n",
      "2018-04-11T16:10:16.333386: step 4917, loss 0.114782, acc 0.96875\n",
      "2018-04-11T16:10:16.909034: step 4918, loss 0.199934, acc 0.9375\n",
      "2018-04-11T16:10:17.463660: step 4919, loss 0.107179, acc 0.96875\n",
      "2018-04-11T16:10:18.024706: step 4920, loss 0.130519, acc 0.96875\n",
      "2018-04-11T16:10:18.601182: step 4921, loss 0.0455511, acc 1\n",
      "2018-04-11T16:10:19.176611: step 4922, loss 0.139149, acc 0.96875\n",
      "2018-04-11T16:10:19.725526: step 4923, loss 0.175428, acc 0.9375\n",
      "2018-04-11T16:10:20.326948: step 4924, loss 0.0798242, acc 0.984375\n",
      "2018-04-11T16:10:21.003356: step 4925, loss 0.0868331, acc 0.984375\n",
      "2018-04-11T16:10:21.575263: step 4926, loss 0.0887298, acc 0.984375\n",
      "2018-04-11T16:10:22.298630: step 4927, loss 0.0744384, acc 1\n",
      "2018-04-11T16:10:22.914207: step 4928, loss 0.123295, acc 0.984375\n",
      "2018-04-11T16:10:23.555648: step 4929, loss 0.0757082, acc 0.984375\n",
      "2018-04-11T16:10:24.154996: step 4930, loss 0.100594, acc 0.984375\n",
      "2018-04-11T16:10:24.749507: step 4931, loss 0.103986, acc 0.96875\n",
      "2018-04-11T16:10:25.351344: step 4932, loss 0.121343, acc 0.953125\n",
      "2018-04-11T16:10:25.958501: step 4933, loss 0.173254, acc 0.9375\n",
      "2018-04-11T16:10:26.558882: step 4934, loss 0.0847759, acc 0.984375\n",
      "2018-04-11T16:10:27.169447: step 4935, loss 0.116764, acc 0.96875\n",
      "2018-04-11T16:10:27.759595: step 4936, loss 0.118111, acc 0.953125\n",
      "2018-04-11T16:10:28.329820: step 4937, loss 0.148189, acc 0.953125\n",
      "2018-04-11T16:10:28.927243: step 4938, loss 0.0897677, acc 1\n",
      "2018-04-11T16:10:29.494972: step 4939, loss 0.0971883, acc 0.984375\n",
      "2018-04-11T16:10:30.034106: step 4940, loss 0.0894175, acc 0.984375\n",
      "2018-04-11T16:10:30.617781: step 4941, loss 0.102329, acc 0.96875\n",
      "2018-04-11T16:10:31.213096: step 4942, loss 0.064763, acc 1\n",
      "2018-04-11T16:10:31.787055: step 4943, loss 0.0750902, acc 1\n",
      "2018-04-11T16:10:32.441305: step 4944, loss 0.0450167, acc 1\n",
      "2018-04-11T16:10:33.048483: step 4945, loss 0.186498, acc 0.953125\n",
      "2018-04-11T16:10:33.638263: step 4946, loss 0.138453, acc 0.984375\n",
      "2018-04-11T16:10:34.248506: step 4947, loss 0.0935359, acc 0.984375\n",
      "2018-04-11T16:10:34.894360: step 4948, loss 0.0972778, acc 0.96875\n",
      "2018-04-11T16:10:35.596248: step 4949, loss 0.0726791, acc 1\n",
      "2018-04-11T16:10:36.412471: step 4950, loss 0.0834709, acc 1\n",
      "2018-04-11T16:10:37.070725: step 4951, loss 0.178746, acc 0.953125\n",
      "2018-04-11T16:10:37.675331: step 4952, loss 0.0767643, acc 0.984375\n",
      "2018-04-11T16:10:38.364444: step 4953, loss 0.109082, acc 0.96875\n",
      "2018-04-11T16:10:39.050532: step 4954, loss 0.174638, acc 0.96875\n",
      "2018-04-11T16:10:39.644579: step 4955, loss 0.081566, acc 0.984375\n",
      "2018-04-11T16:10:40.289207: step 4956, loss 0.0797923, acc 1\n",
      "2018-04-11T16:10:40.971837: step 4957, loss 0.233772, acc 0.90625\n",
      "2018-04-11T16:10:41.602001: step 4958, loss 0.139707, acc 0.921875\n",
      "2018-04-11T16:10:42.197198: step 4959, loss 0.198926, acc 0.9375\n",
      "2018-04-11T16:10:42.793329: step 4960, loss 0.159668, acc 0.96875\n",
      "2018-04-11T16:10:43.409204: step 4961, loss 0.0690681, acc 0.984375\n",
      "2018-04-11T16:10:44.059142: step 4962, loss 0.0852929, acc 0.984375\n",
      "2018-04-11T16:10:44.663142: step 4963, loss 0.121116, acc 0.984375\n",
      "2018-04-11T16:10:45.285786: step 4964, loss 0.103838, acc 0.96875\n",
      "2018-04-11T16:10:45.948279: step 4965, loss 0.149738, acc 0.9375\n",
      "2018-04-11T16:10:46.511126: step 4966, loss 0.134087, acc 0.984375\n",
      "2018-04-11T16:10:47.096157: step 4967, loss 0.0988456, acc 0.984375\n",
      "2018-04-11T16:10:47.731432: step 4968, loss 0.156915, acc 0.96875\n",
      "2018-04-11T16:10:48.339638: step 4969, loss 0.115184, acc 0.984375\n",
      "2018-04-11T16:10:48.962507: step 4970, loss 0.0424218, acc 1\n",
      "2018-04-11T16:10:49.685768: step 4971, loss 0.0648457, acc 0.984375\n",
      "2018-04-11T16:10:50.284798: step 4972, loss 0.0521674, acc 1\n",
      "2018-04-11T16:10:50.963128: step 4973, loss 0.108771, acc 0.984375\n",
      "2018-04-11T16:10:51.579364: step 4974, loss 0.0604119, acc 0.984375\n",
      "2018-04-11T16:10:52.275208: step 4975, loss 0.172201, acc 0.9375\n",
      "2018-04-11T16:10:52.899006: step 4976, loss 0.0749149, acc 0.984375\n",
      "2018-04-11T16:10:53.503178: step 4977, loss 0.120477, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11T16:10:54.117703: step 4978, loss 0.0549405, acc 1\n",
      "2018-04-11T16:10:54.748783: step 4979, loss 0.180028, acc 0.953125\n",
      "2018-04-11T16:10:55.330260: step 4980, loss 0.126546, acc 0.984375\n",
      "2018-04-11T16:10:55.924443: step 4981, loss 0.103206, acc 0.984375\n",
      "2018-04-11T16:10:56.567637: step 4982, loss 0.108849, acc 0.984375\n",
      "2018-04-11T16:10:57.126432: step 4983, loss 0.100995, acc 0.984375\n",
      "2018-04-11T16:10:57.699474: step 4984, loss 0.180641, acc 0.921875\n",
      "2018-04-11T16:10:58.266445: step 4985, loss 0.153685, acc 0.9375\n",
      "2018-04-11T16:10:58.868904: step 4986, loss 0.12034, acc 0.984375\n",
      "2018-04-11T16:10:59.514385: step 4987, loss 0.120198, acc 0.96875\n",
      "2018-04-11T16:11:00.112505: step 4988, loss 0.0607797, acc 1\n",
      "2018-04-11T16:11:00.698426: step 4989, loss 0.0883618, acc 1\n",
      "2018-04-11T16:11:01.262009: step 4990, loss 0.142351, acc 0.96875\n",
      "2018-04-11T16:11:01.845422: step 4991, loss 0.0926365, acc 0.984375\n",
      "2018-04-11T16:11:02.418022: step 4992, loss 0.122135, acc 0.96875\n",
      "2018-04-11T16:11:03.093057: step 4993, loss 0.157295, acc 0.984375\n",
      "2018-04-11T16:11:03.714075: step 4994, loss 0.139252, acc 0.96875\n",
      "2018-04-11T16:11:04.272880: step 4995, loss 0.111235, acc 0.953125\n",
      "2018-04-11T16:11:04.846297: step 4996, loss 0.160677, acc 0.96875\n",
      "2018-04-11T16:11:05.496540: step 4997, loss 0.0656019, acc 1\n",
      "2018-04-11T16:11:06.109710: step 4998, loss 0.110034, acc 0.984375\n",
      "2018-04-11T16:11:06.743905: step 4999, loss 0.0996739, acc 0.96875\n",
      "2018-04-11T16:11:07.358868: step 5000, loss 0.119582, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T16:11:07.545825: step 5000, loss 0.807965, acc 0.71\n",
      "\n",
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-5000\n",
      "\n",
      "2018-04-11T16:11:08.394101: step 5001, loss 0.0754896, acc 1\n",
      "2018-04-11T16:11:09.093420: step 5002, loss 0.0760968, acc 1\n",
      "2018-04-11T16:11:09.722226: step 5003, loss 0.174636, acc 0.9375\n",
      "2018-04-11T16:11:10.341858: step 5004, loss 0.0514414, acc 1\n",
      "2018-04-11T16:11:10.916149: step 5005, loss 0.112553, acc 0.96875\n",
      "2018-04-11T16:11:11.536802: step 5006, loss 0.0755566, acc 0.96875\n",
      "2018-04-11T16:11:12.151889: step 5007, loss 0.128056, acc 0.96875\n",
      "2018-04-11T16:11:12.800403: step 5008, loss 0.16747, acc 0.953125\n",
      "2018-04-11T16:11:13.390481: step 5009, loss 0.119176, acc 0.953125\n",
      "2018-04-11T16:11:14.044395: step 5010, loss 0.124651, acc 0.984375\n",
      "2018-04-11T16:11:14.632923: step 5011, loss 0.0654799, acc 1\n",
      "2018-04-11T16:11:15.232524: step 5012, loss 0.125358, acc 0.96875\n",
      "2018-04-11T16:11:15.896514: step 5013, loss 0.0992781, acc 0.984375\n",
      "2018-04-11T16:11:16.561499: step 5014, loss 0.120565, acc 0.96875\n",
      "2018-04-11T16:11:17.235282: step 5015, loss 0.10014, acc 0.984375\n",
      "2018-04-11T16:11:17.871492: step 5016, loss 0.135514, acc 0.953125\n",
      "2018-04-11T16:11:18.432803: step 5017, loss 0.0768236, acc 1\n",
      "2018-04-11T16:11:19.003271: step 5018, loss 0.111619, acc 0.96875\n",
      "2018-04-11T16:11:19.576936: step 5019, loss 0.185535, acc 0.96875\n",
      "2018-04-11T16:11:20.182560: step 5020, loss 0.143372, acc 0.953125\n",
      "2018-04-11T16:11:20.828628: step 5021, loss 0.155269, acc 0.96875\n",
      "2018-04-11T16:11:21.405207: step 5022, loss 0.0900539, acc 0.96875\n",
      "2018-04-11T16:11:21.971382: step 5023, loss 0.115275, acc 0.96875\n",
      "2018-04-11T16:11:22.542567: step 5024, loss 0.213154, acc 0.921875\n",
      "2018-04-11T16:11:23.132304: step 5025, loss 0.0971342, acc 0.96875\n",
      "2018-04-11T16:11:23.694122: step 5026, loss 0.171445, acc 0.953125\n",
      "2018-04-11T16:11:24.282208: step 5027, loss 0.132118, acc 0.96875\n",
      "2018-04-11T16:11:24.847483: step 5028, loss 0.164351, acc 0.953125\n",
      "2018-04-11T16:11:25.436280: step 5029, loss 0.107099, acc 0.96875\n",
      "2018-04-11T16:11:26.004830: step 5030, loss 0.0761515, acc 1\n",
      "2018-04-11T16:11:26.650073: step 5031, loss 0.135004, acc 0.96875\n",
      "2018-04-11T16:11:27.296565: step 5032, loss 0.167169, acc 0.96875\n",
      "2018-04-11T16:11:27.921929: step 5033, loss 0.060473, acc 1\n",
      "2018-04-11T16:11:28.518854: step 5034, loss 0.065487, acc 1\n",
      "2018-04-11T16:11:29.124719: step 5035, loss 0.237633, acc 0.953125\n",
      "2018-04-11T16:11:29.766792: step 5036, loss 0.0606215, acc 1\n",
      "2018-04-11T16:11:30.428575: step 5037, loss 0.154045, acc 0.953125\n",
      "2018-04-11T16:11:31.109964: step 5038, loss 0.091657, acc 0.96875\n",
      "2018-04-11T16:11:31.738960: step 5039, loss 0.0790752, acc 0.984375\n",
      "2018-04-11T16:11:32.332500: step 5040, loss 0.0874169, acc 1\n",
      "2018-04-11T16:11:32.954648: step 5041, loss 0.118211, acc 0.984375\n",
      "2018-04-11T16:11:33.603275: step 5042, loss 0.10805, acc 0.953125\n",
      "2018-04-11T16:11:34.234133: step 5043, loss 0.0825566, acc 1\n",
      "2018-04-11T16:11:34.890793: step 5044, loss 0.146767, acc 0.96875\n",
      "2018-04-11T16:11:35.549614: step 5045, loss 0.117318, acc 0.96875\n",
      "2018-04-11T16:11:36.201874: step 5046, loss 0.0622807, acc 1\n",
      "2018-04-11T16:11:36.873382: step 5047, loss 0.177652, acc 0.953125\n",
      "2018-04-11T16:11:37.453888: step 5048, loss 0.170319, acc 0.953125\n",
      "2018-04-11T16:11:38.037131: step 5049, loss 0.180355, acc 0.9375\n",
      "2018-04-11T16:11:38.618133: step 5050, loss 0.0900751, acc 0.984375\n",
      "2018-04-11T16:11:39.299316: step 5051, loss 0.0744035, acc 0.984375\n",
      "2018-04-11T16:11:39.884217: step 5052, loss 0.0564751, acc 1\n",
      "2018-04-11T16:11:40.457615: step 5053, loss 0.107466, acc 0.96875\n",
      "2018-04-11T16:11:41.036437: step 5054, loss 0.162708, acc 0.953125\n",
      "2018-04-11T16:11:41.702150: step 5055, loss 0.091578, acc 0.984375\n",
      "2018-04-11T16:11:42.325814: step 5056, loss 0.0860801, acc 1\n",
      "2018-04-11T16:11:42.903034: step 5057, loss 0.149972, acc 0.96875\n",
      "2018-04-11T16:11:43.493882: step 5058, loss 0.0622828, acc 1\n",
      "2018-04-11T16:11:44.115249: step 5059, loss 0.073171, acc 1\n",
      "2018-04-11T16:11:44.773267: step 5060, loss 0.179308, acc 0.90625\n",
      "2018-04-11T16:11:45.379208: step 5061, loss 0.100542, acc 0.984375\n",
      "2018-04-11T16:11:45.943165: step 5062, loss 0.189048, acc 0.921875\n",
      "2018-04-11T16:11:46.509316: step 5063, loss 0.060623, acc 1\n",
      "2018-04-11T16:11:47.242824: step 5064, loss 0.101641, acc 0.984375\n",
      "2018-04-11T16:11:47.861262: step 5065, loss 0.0579559, acc 1\n",
      "2018-04-11T16:11:48.416209: step 5066, loss 0.0854562, acc 0.953125\n",
      "2018-04-11T16:11:49.093477: step 5067, loss 0.116724, acc 0.96875\n",
      "2018-04-11T16:11:49.670991: step 5068, loss 0.245965, acc 0.921875\n",
      "2018-04-11T16:11:50.271472: step 5069, loss 0.147123, acc 0.953125\n",
      "2018-04-11T16:11:50.862761: step 5070, loss 0.16768, acc 0.953125\n",
      "2018-04-11T16:11:51.424236: step 5071, loss 0.164097, acc 0.96875\n",
      "2018-04-11T16:11:51.977457: step 5072, loss 0.0782919, acc 0.984375\n",
      "2018-04-11T16:11:52.572895: step 5073, loss 0.0695877, acc 1\n",
      "2018-04-11T16:11:53.284213: step 5074, loss 0.126321, acc 0.96875\n",
      "2018-04-11T16:11:53.901855: step 5075, loss 0.145454, acc 0.96875\n",
      "2018-04-11T16:11:54.504437: step 5076, loss 0.0589482, acc 1\n",
      "2018-04-11T16:11:55.080338: step 5077, loss 0.0872978, acc 0.984375\n",
      "2018-04-11T16:11:55.719808: step 5078, loss 0.0510212, acc 1\n",
      "2018-04-11T16:11:56.384617: step 5079, loss 0.0687678, acc 0.984375\n",
      "2018-04-11T16:11:56.952847: step 5080, loss 0.0840543, acc 0.96875\n",
      "2018-04-11T16:11:57.543988: step 5081, loss 0.107509, acc 0.96875\n",
      "2018-04-11T16:11:58.128090: step 5082, loss 0.152557, acc 0.953125\n",
      "2018-04-11T16:11:58.712526: step 5083, loss 0.141785, acc 0.953125\n",
      "2018-04-11T16:11:59.301730: step 5084, loss 0.166553, acc 0.953125\n",
      "2018-04-11T16:11:59.894336: step 5085, loss 0.231492, acc 0.9375\n",
      "2018-04-11T16:12:00.475982: step 5086, loss 0.166574, acc 0.953125\n",
      "2018-04-11T16:12:01.117670: step 5087, loss 0.0720116, acc 1\n",
      "2018-04-11T16:12:01.743437: step 5088, loss 0.203084, acc 0.953125\n",
      "2018-04-11T16:12:02.339224: step 5089, loss 0.162226, acc 0.96875\n",
      "2018-04-11T16:12:02.989998: step 5090, loss 0.0948627, acc 0.96875\n",
      "2018-04-11T16:12:03.562693: step 5091, loss 0.0854574, acc 0.984375\n",
      "2018-04-11T16:12:04.130916: step 5092, loss 0.127132, acc 0.96875\n",
      "2018-04-11T16:12:04.725142: step 5093, loss 0.0744748, acc 1\n",
      "2018-04-11T16:12:05.383487: step 5094, loss 0.117576, acc 0.96875\n",
      "2018-04-11T16:12:05.966573: step 5095, loss 0.111053, acc 0.96875\n",
      "2018-04-11T16:12:06.551915: step 5096, loss 0.14126, acc 0.984375\n",
      "2018-04-11T16:12:07.119582: step 5097, loss 0.076993, acc 0.984375\n",
      "2018-04-11T16:12:07.744205: step 5098, loss 0.0760707, acc 0.984375\n",
      "2018-04-11T16:12:08.432723: step 5099, loss 0.26078, acc 0.9375\n",
      "2018-04-11T16:12:09.025848: step 5100, loss 0.197459, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-04-11T16:12:09.203344: step 5100, loss 0.672084, acc 0.82\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /home/yejinlee/emotionTest/runs/1523427711/checkpoints/model-5100\n",
      "\n",
      "2018-04-11T16:12:10.008586: step 5101, loss 0.112985, acc 0.984375\n",
      "2018-04-11T16:12:10.588598: step 5102, loss 0.0907483, acc 1\n",
      "2018-04-11T16:12:11.171145: step 5103, loss 0.109841, acc 0.96875\n",
      "2018-04-11T16:12:11.752827: step 5104, loss 0.159045, acc 0.953125\n",
      "2018-04-11T16:12:12.355817: step 5105, loss 0.0915214, acc 0.984375\n",
      "2018-04-11T16:12:12.986210: step 5106, loss 0.0954492, acc 0.984375\n",
      "2018-04-11T16:12:13.566224: step 5107, loss 0.101628, acc 0.96875\n",
      "2018-04-11T16:12:14.139500: step 5108, loss 0.107492, acc 0.984375\n",
      "2018-04-11T16:12:14.780631: step 5109, loss 0.0802585, acc 0.984375\n",
      "2018-04-11T16:12:15.403677: step 5110, loss 0.0736634, acc 1\n",
      "2018-04-11T16:12:15.984854: step 5111, loss 0.132454, acc 0.96875\n",
      "2018-04-11T16:12:16.555124: step 5112, loss 0.090781, acc 0.984375\n",
      "2018-04-11T16:12:17.187597: step 5113, loss 0.119971, acc 0.984375\n",
      "2018-04-11T16:12:17.825667: step 5114, loss 0.0781206, acc 0.984375\n",
      "2018-04-11T16:12:18.428451: step 5115, loss 0.0972052, acc 0.96875\n",
      "2018-04-11T16:12:19.013825: step 5116, loss 0.121178, acc 0.96875\n",
      "2018-04-11T16:12:19.602145: step 5117, loss 0.0967216, acc 0.96875\n",
      "2018-04-11T16:12:20.183120: step 5118, loss 0.196295, acc 0.921875\n",
      "2018-04-11T16:12:20.753554: step 5119, loss 0.100635, acc 0.96875\n",
      "2018-04-11T16:12:21.338986: step 5120, loss 0.0965286, acc 0.984375\n",
      "2018-04-11T16:12:21.932167: step 5121, loss 0.111637, acc 0.984375\n",
      "2018-04-11T16:12:22.521693: step 5122, loss 0.14282, acc 0.953125\n",
      "2018-04-11T16:12:23.087990: step 5123, loss 0.0935425, acc 0.984375\n",
      "2018-04-11T16:12:23.662400: step 5124, loss 0.0628978, acc 1\n",
      "2018-04-11T16:12:24.237302: step 5125, loss 0.168303, acc 0.96875\n",
      "2018-04-11T16:12:24.880565: step 5126, loss 0.149653, acc 0.953125\n",
      "2018-04-11T16:12:25.563968: step 5127, loss 0.11064, acc 0.984375\n",
      "2018-04-11T16:12:26.129749: step 5128, loss 0.102013, acc 0.984375\n",
      "2018-04-11T16:12:26.698031: step 5129, loss 0.0960976, acc 0.96875\n",
      "2018-04-11T16:12:26.928298: step 5130, loss 0.0423746, acc 1\n"
     ]
    }
   ],
   "source": [
    "# 3. train the model and test\n",
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(sequence_length=x_train.shape[1],\n",
    "                      num_classes=y_train.shape[1],\n",
    "                      vocab_size=vocab_size,\n",
    "                      embedding_size=FLAGS['embedding_dim'].value,\n",
    "                      filter_sizes=list(map(int, FLAGS['filter_sizes'].value.split(\",\"))),\n",
    "                      num_filters=FLAGS['num_filters'].value,\n",
    "                      l2_reg_lambda=FLAGS['l2_reg_lambda'].value)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS['num_checkpoints'].value)\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x_batch,\n",
    "                cnn.input_y: y_batch,\n",
    "                cnn.dropout_keep_prob: FLAGS['dropout_keep_prob'].value\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x_batch,\n",
    "                cnn.input_y: y_batch,\n",
    "                cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "\n",
    "        def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "            \"\"\"\n",
    "            Generates a batch iterator for a dataset.\n",
    "            \"\"\"\n",
    "            data = np.array(data)\n",
    "            data_size = len(data)\n",
    "            num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "            for epoch in range(num_epochs):\n",
    "                # Shuffle the data at each epoch\n",
    "                if shuffle:\n",
    "                    shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                    shuffled_data = data[shuffle_indices]\n",
    "                else:\n",
    "                    shuffled_data = data\n",
    "                for batch_num in range(num_batches_per_epoch):\n",
    "                    start_index = batch_num * batch_size\n",
    "                    end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                    yield shuffled_data[start_index:end_index]\n",
    "\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS['batch_size'].value, FLAGS['num_epochs'].value)\n",
    "\n",
    "        testpoint = 0\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS['evaluate_every'].value == 0:\n",
    "                if testpoint + 100 < len(x_test):\n",
    "                    testpoint += 100\n",
    "                else:\n",
    "                    testpoint = 0\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_test[testpoint:testpoint+100], y_test[testpoint:testpoint+100], writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS['checkpoint_every'].value == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
