{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yejinlee\\anaconda3\\envs\\emotion\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from scipy.sparse import find\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import MeCab\n",
    "\n",
    "from jamo import h2j, j2hcj\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = MeCab.Tagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_w2v_dic (path, flag, w2v_size,filetype):\n",
    "    allFileNames = os.listdir(path)\n",
    "    modelNames = [fn for fn in allFileNames if fn.find(flag+'.'+filetype) > -1 and fn.find(str(w2v_size)) > -1]\n",
    "    \n",
    "    model = gensim.models.Word2Vec.load(path+'/'+modelNames[0])\n",
    "    print(modelNames[0]+' is loaded')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data별로 처리가 다름\n",
    "dataDir = 'data/uplusInput/train'\n",
    "allFileNames = os.listdir(dataDir)\n",
    "\n",
    "filePaths = []\n",
    "\n",
    "for fname in allFileNames:\n",
    "    if fname[0] != '.':\n",
    "        filePaths.append(os.path.join(dataDir, fname))\n",
    "\n",
    "\n",
    "for fpn in filePaths:\n",
    "    if fpn[-4:] == '.csv':\n",
    "        datas = pd.read_csv(fpn)\n",
    "        input_x = datas['content']\n",
    "        y_train = datas[['neu','pos','neg']]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = 'data/uplusInput/test'\n",
    "allFileNames = os.listdir(dataDir)\n",
    "\n",
    "filePaths = []\n",
    "\n",
    "for fname in allFileNames:\n",
    "    if fname[0] != '.':\n",
    "        filePaths.append(os.path.join(dataDir, fname))\n",
    "\n",
    "\n",
    "for fpn in filePaths:\n",
    "    if fpn[-4:] == '.csv':\n",
    "        datas = pd.read_csv(fpn)\n",
    "        test_input_x = datas['content']\n",
    "        y_test = datas[['neu','pos','neg']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# divide train/test set function                   #\n",
    "####################################################\n",
    "def divide(x, y, train_prop):\n",
    "    random.seed(1234)\n",
    "#     x = np.array(x)\n",
    "#     y = np.array(y)\n",
    "    tmp = np.random.permutation(np.arange(len(x)))\n",
    "    return x[tmp][:round(train_prop * len(x))],  x[tmp][-(len(x)-round(train_prop * len(x))):], y[tmp][:round(train_prop * len(x))], y[tmp][-(len(x)-round(train_prop * len(x))):]\n",
    "#     x_tr = x[tmp][:round(train_prop * len(x))]\n",
    "#     y_tr = y[tmp][:round(train_prop * len(x))]\n",
    "#     x_te = x[tmp][-(len(x)-round(train_prop * len(x))):]\n",
    "#     y_te = y[tmp][-(len(x)-round(train_prop * len(x))):]\n",
    "#     return x_tr, x_te, y_tr, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_vector(word_array, word_vectors, word2vector_size=300):\n",
    "    embedding_vector = []\n",
    "    for w in word_array:\n",
    "        if w not in word_vectors.vocab:\n",
    "            embedding_vector.append(np.random.normal(scale=1e-2, size=word2vector_size))#[np.zeros(shape=300)] #\n",
    "        else:\n",
    "            embedding_vector.append(word_vectors[w])\n",
    "    return embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(x_train, x_train2, y_train, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data_size = len(x_train)\n",
    "    num_batches_per_epoch = int((data_size - 1) / batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_x_data = x_train[shuffle_indices]\n",
    "            shuffled_x_data2 = x_train2[shuffle_indices]\n",
    "            shuffled_y_data = y_train[shuffle_indices]\n",
    "        else:\n",
    "            suffled_x_data = x_train\n",
    "            shuffled_x_data2 = x_train2\n",
    "            shuffled_y_data = y_train\n",
    "            \n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield zip(shuffled_x_data[start_index:end_index], shuffled_x_data2[start_index:end_index], shuffled_y_data[start_index:end_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201804231620_150_WT.bin is loaded\n"
     ]
    }
   ],
   "source": [
    "word2vector_size = 150\n",
    "window = 5  #현재 word를 계산하는데 사용하는 maximum distance ?\n",
    "min_count = 2 #단어가 해당 개수 이하로 나오면 무시\n",
    "SAVE_PATH = './wordDic/'\n",
    "flag = 'WT'\n",
    "\n",
    "model1 = load_w2v_dic(path=SAVE_PATH, flag=flag, w2v_size = word2vector_size,filetype='bin')\n",
    "word_vectors_WT = model1.wv\n",
    "\n",
    "flag = 'JM'\n",
    "model2 = load_w2v_dic(path=SAVE_PATH, flag=flag, w2v_size = word2vector_size,filetype='bin')\n",
    "word_vectors_JM = model2.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flag define\n",
    "max_seq_size = 150\n",
    "filter_sizes = [2, 3]\n",
    "num_filters = [128, 128]\n",
    "\n",
    "input_weights = [1,1]\n",
    "dropout_keep_prob = 0.7\n",
    "l2_reg_lambda = 0.1\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "evaluate_every = 200\n",
    "checkpoint_every = 200\n",
    "num_checkpoints = 5\n",
    "\n",
    "allow_soft_placement = True\n",
    "log_device_placement = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x_vec1 = []\n",
    "input_x_vec2 = []\n",
    "\n",
    "# w2v = Word2Vec(word_vec_size=word2vector_size, window=window, min_count=min_count, flag=flag)\n",
    "\n",
    "for x in input_x:\n",
    "    parse_x = w2v.make_input([x]) #jamo기준 parsing\n",
    "    embedding_vec1 = find_vector(parse_x, word_vectors_WT, word2vector_size)\n",
    "    embedding_vec2 = find_vector(parse_x, word_vectors_JM, word2vector_size)\n",
    "    \n",
    "    temp_len1 = max_seq_size-len(embedding_vec1)\n",
    "    temp_len2 = max_seq_size-len(embedding_vec2)\n",
    "    \n",
    "    if temp_len1 > 0:\n",
    "        for i in range(0,temp_len1):\n",
    "            embedding_vec1 += [np.zeros(word2vector_size)]    \n",
    "    else:\n",
    "        embedding_vec1 = embedding_vec1[:max_seq_size]\n",
    "        \n",
    "    if temp_len2 > 0:\n",
    "        for i in range(0,temp_len2):\n",
    "            embedding_vec2 += [np.zeros(word2vector_size)]    \n",
    "    else:\n",
    "        embedding_vec2 = embedding_vec2[:max_seq_size]    \n",
    "    \n",
    "    input_x_vec1.append(embedding_vec1)\n",
    "    input_x_vec2.append(embedding_vec2)\n",
    "\n",
    "del input_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_x_vec1 = []\n",
    "test_input_x_vec2 = []\n",
    "\n",
    "# w2v = Word2Vec(word_vec_size=word2vector_size, window=window, min_count=min_count, flag=flag)\n",
    "\n",
    "for x in test_input_x:\n",
    "    parse_x = w2v.make_input([x]) #jamo기준 parsing\n",
    "    embedding_vec1 = find_vector(parse_x, word_vectors_WT, word2vector_size)\n",
    "    embedding_vec2 = find_vector(parse_x, word_vectors_JM, word2vector_size)\n",
    "    \n",
    "    temp_len1 = max_seq_size-len(embedding_vec1)\n",
    "    temp_len2 = max_seq_size-len(embedding_vec2)\n",
    "    \n",
    "    if temp_len1 > 0:\n",
    "        for i in range(0,temp_len1):\n",
    "            embedding_vec1 += [np.zeros(word2vector_size)]    \n",
    "    else:\n",
    "        embedding_vec1 = embedding_vec1[:max_seq_size]\n",
    "        \n",
    "    if temp_len2 > 0:\n",
    "        for i in range(0,temp_len2):\n",
    "            embedding_vec2 += [np.zeros(word2vector_size)]    \n",
    "    else:\n",
    "        embedding_vec2 = embedding_vec2[:max_seq_size]    \n",
    "    \n",
    "    test_input_x_vec1.append(embedding_vec1)\n",
    "    test_input_x_vec2.append(embedding_vec2)\n",
    "\n",
    "del test_input_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_train1 = np.array(input_x_vec1)\n",
    "x_train2 = np.array(input_x_vec2)\n",
    "del input_x_vec1\n",
    "del input_x_vec2\n",
    "\n",
    "x_test1 = np.array(test_input_x_vec1)\n",
    "x_test2 = np.array(test_input_x_vec2)\n",
    "del test_input_x_vec1\n",
    "del test_input_x_vec2\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "del model1\n",
    "del model2\n",
    "del word_vectors_WT\n",
    "del word_vectors_JM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del input_x_vec\n",
    "# del test_input_x_vec \n",
    "# del input_x\n",
    "# del test_input_x\n",
    "# del w2v\n",
    "# del model\n",
    "# del word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide dataset into train/test set - 하나의 문서를 나눠서 사용할때\n",
    "# x_train, x_test, y_train, y_test = divide(np.array(input_x_vec),np.array(input_y),train_prop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    <Parameters>\n",
    "        - sequence_length: 최대 문장 길이\n",
    "        - num_classes: 클래스 개수\n",
    "        - embedding_size: 각 단어에 해당되는 임베디드 벡터의 차원\n",
    "        - filter_sizes: convolutional filter들의 사이즈 (= 각 filter가 몇 개의 단어를 볼 것인가?) (예: \"3, 4, 5\")\n",
    "        - num_filters: 각 filter size 별 filter 수\n",
    "        - l2_reg_lambda: 각 weights, biases에 대한 l2 regularization 정도\n",
    "    \"\"\"\n",
    "    def make_filter_set(self, filter_size, num_filters):\n",
    "        if type(filter_size) == int:\n",
    "            filter_size = [filter_size]\n",
    "        if type(num_filters) == int:\n",
    "            num_filters = [num_filters]\n",
    "\n",
    "        filter_first = True\n",
    "        if len(filter_size) >= len(num_filters):\n",
    "            t1 = filter_size\n",
    "            t2 = num_filters\n",
    "        else:\n",
    "            t1 = num_filters\n",
    "            t2 = filter_size\n",
    "            filter_first = False\n",
    "\n",
    "        result_arr = []\n",
    "\n",
    "        for i, t in enumerate(t1):\n",
    "            j = i\n",
    "\n",
    "            if i >= len(t2):\n",
    "                j = i%len(t2)\n",
    "\n",
    "            if filter_first:\n",
    "                c = (t , t2[j])\n",
    "            else:\n",
    "                c = (t2[j], t)\n",
    "\n",
    "            result_arr.append(c)\n",
    "\n",
    "        return result_arr\n",
    "        \n",
    "    def __init__(\n",
    "            self, sequence_length, num_classes, \n",
    "            embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0, input_weights=[1.0]):\n",
    "       \n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = []\n",
    "        n = len(input_weights)\n",
    "        for i in range(n):\n",
    "            name = \"input_x\" + str(i)\n",
    "            self.input_x.append(tf.expand_dims(tf.placeholder(tf.float32, [None, sequence_length, embedding_size], name=name),-1))\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "#         self.expended_input_x = tf.expand_dims(self.input_x, -1) #conv2를 위한 차원 expend?\n",
    "        test_set = self.make_filter_set(filter_size=filter_sizes,num_filters=num_filters)\n",
    "        \n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        num_filters_total = 0\n",
    "        pooled_outputs = []\n",
    "        \n",
    "        for idx, filter_set in enumerate(test_set):\n",
    "            filter_size, num_filter = filter_set\n",
    "            with tf.name_scope(\"conv-%s\" % filter_size):\n",
    "                input_vec = self.input_x[idx]\n",
    "                \n",
    "                sentence_length = int(input_vec.shape[1])\n",
    "                word_vector_size = int(input_vec.shape[2])\n",
    "                \n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, word_vector_size, 1, num_filter]\n",
    "                print(\"filter shape : \"+str(filter_shape))\n",
    "\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape), name=\"W\") #변경 가능 영역 - stddev / truncated_normal<->random_normal\n",
    "                b = tf.Variable(tf.random_normal(shape=[num_filter]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    input_vec,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                print(\"conv result shape : \"+str(h.shape))\n",
    "                \n",
    "            with tf.name_scope(\"maxpool-%s\" % filter_size):\n",
    "                # Maxpooling over the outputs\n",
    "                sentence_length = int(h.shape[1])\n",
    "                word2vec_size = int(h.shape[2])\n",
    "                \n",
    "                pool_size = [1, sentence_length, word2vec_size, 1]\n",
    "                print(\"pool size : \"+ str(pool_size))\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=pool_size,\n",
    "                    strides=[1,1,1,1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                print(\"pooled shape : \"+ str(pooled.shape))\n",
    "                pooled = input_weights[idx] * pooled\n",
    "                pooled_outputs.append(pooled)\n",
    "                num_filters_total += num_filter\n",
    "\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_pool = tf.concat(axis=3, values=pooled_outputs)\n",
    "            self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "            \n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate Mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses =tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.input_y, logits=self.scores)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter shape\n",
      "[4, 150, 1, 128]\n",
      "conv result shape\n",
      "(?, 147, 1, 128)\n",
      "pool size\n",
      "[1, 147, 1, 1]\n",
      "pooled shape\n",
      "(?, 1, 1, 128)\n",
      "WARNING:tensorflow:From c:\\users\\yejinlee\\anaconda3\\envs\\emotion\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Writing to D:\\emotionTest\\runs\\1524709946\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:32:48.123392: step 200, loss 5.85691, acc 0.7\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:33:09.631105: step 400, loss 4.28314, acc 0.73\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:33:29.892218: step 600, loss 3.16296, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:33:51.129925: step 800, loss 1.83106, acc 0.76\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:34:12.278036: step 1000, loss 1.67912, acc 0.73\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-1000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:34:32.665948: step 1200, loss 1.26571, acc 0.77\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-1200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:34:54.054060: step 1400, loss 1.15135, acc 0.63\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-1400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:35:15.194922: step 1600, loss 0.677119, acc 0.84\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-1600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:35:34.401616: step 1800, loss 0.667479, acc 0.83\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-1800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:35:54.411318: step 2000, loss 2.15871, acc 0.78\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-2000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:36:14.382823: step 2200, loss 0.630181, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-2200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:36:33.549317: step 2400, loss 0.587299, acc 0.76\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-2400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:36:53.618815: step 2600, loss 0.894812, acc 0.72\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-2600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:37:12.895914: step 2800, loss 0.614455, acc 0.77\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-2800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:37:32.983422: step 3000, loss 0.799049, acc 0.79\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-3000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:37:52.929516: step 3200, loss 0.94603, acc 0.68\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-3200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:38:12.087419: step 3400, loss 0.514961, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-3400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:38:32.148718: step 3600, loss 1.17548, acc 0.64\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-3600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:38:52.300626: step 3800, loss 0.511133, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-3800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:39:11.504730: step 4000, loss 1.05493, acc 0.69\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-4000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:39:31.545830: step 4200, loss 0.465884, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-4200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:39:51.496929: step 4400, loss 0.560005, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-4400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:40:10.643628: step 4600, loss 0.500146, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-4600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:40:30.804127: step 4800, loss 0.478673, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-4800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:40:50.047225: step 5000, loss 0.46632, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-5000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:41:10.078130: step 5200, loss 0.651794, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-5200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:41:30.438455: step 5400, loss 0.71827, acc 0.76\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-5400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:41:49.574753: step 5600, loss 0.476549, acc 0.83\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-5600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:42:09.626060: step 5800, loss 0.495698, acc 0.79\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-5800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:42:29.692765: step 6000, loss 0.490654, acc 0.77\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-6000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:42:48.902658: step 6200, loss 0.662533, acc 0.74\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-6200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:43:08.960763: step 6400, loss 0.412224, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-6400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:43:29.065862: step 6600, loss 0.745145, acc 0.71\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-6600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:43:48.264970: step 6800, loss 0.600883, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-6800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:44:08.304682: step 7000, loss 0.500933, acc 0.84\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-7000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:44:28.330983: step 7200, loss 0.644863, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-7200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:44:47.499891: step 7400, loss 0.490957, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-7400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:45:07.493587: step 7600, loss 0.494279, acc 0.81\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-7600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:45:26.781299: step 7800, loss 0.520519, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-7800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:45:46.890800: step 8000, loss 0.50803, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-8000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:46:06.894501: step 8200, loss 0.533433, acc 0.84\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-8200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:46:26.044597: step 8400, loss 0.782994, acc 0.71\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-8400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:46:46.134101: step 8600, loss 0.460905, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-8600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:47:06.120607: step 8800, loss 0.666747, acc 0.79\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-8800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:47:25.242309: step 9000, loss 0.678469, acc 0.76\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-9000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:47:45.329619: step 9200, loss 0.471834, acc 0.84\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-9200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:48:05.282726: step 9400, loss 1.10558, acc 0.71\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-9400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:48:24.457229: step 9600, loss 0.725213, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-9600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:48:44.488139: step 9800, loss 0.545222, acc 0.84\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-9800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:49:03.695640: step 10000, loss 0.608482, acc 0.78\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-10000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:49:23.816753: step 10200, loss 0.492592, acc 0.9\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-10200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:49:43.898458: step 10400, loss 0.631515, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-10400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:50:03.208370: step 10600, loss 0.630155, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-10600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:50:23.264078: step 10800, loss 0.651212, acc 0.81\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-10800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:50:43.415788: step 11000, loss 0.522845, acc 0.81\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-11000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:51:02.667101: step 11200, loss 0.542137, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-11200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:51:22.799404: step 11400, loss 1.3679, acc 0.66\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-11400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:51:43.098114: step 11600, loss 0.557928, acc 0.83\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-11600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:52:02.233209: step 11800, loss 0.50355, acc 0.84\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-11800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:52:22.211517: step 12000, loss 0.616758, acc 0.84\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-12000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:52:41.328818: step 12200, loss 0.571638, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-12200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:53:01.354120: step 12400, loss 0.563647, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-12400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:53:21.431424: step 12600, loss 0.936115, acc 0.77\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-12600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:53:42.064382: step 12800, loss 0.528845, acc 0.84\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-12800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:54:02.711871: step 13000, loss 0.584915, acc 0.83\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-13000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:54:22.710369: step 13200, loss 0.627184, acc 0.83\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-13200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:54:41.977274: step 13400, loss 0.545217, acc 0.81\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-13400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:55:02.040767: step 13600, loss 0.640344, acc 0.81\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-13600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:55:22.165079: step 13800, loss 0.582021, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-13800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:55:41.369182: step 14000, loss 0.543047, acc 0.83\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-14000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:56:01.447483: step 14200, loss 0.580449, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-14200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:56:21.484987: step 14400, loss 0.602339, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-14400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:56:40.752888: step 14600, loss 0.572176, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-14600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:57:00.777194: step 14800, loss 0.694327, acc 0.83\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-14800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:57:20.001497: step 15000, loss 0.617476, acc 0.84\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-15000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:57:40.203403: step 15200, loss 0.834145, acc 0.81\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-15200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:58:00.245314: step 15400, loss 0.598921, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-15400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:58:19.444413: step 15600, loss 0.689876, acc 0.78\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-15600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:58:39.495724: step 15800, loss 0.83638, acc 0.84\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-15800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:58:59.444431: step 16000, loss 0.623663, acc 0.84\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-16000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:59:18.654733: step 16200, loss 0.791118, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-16200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:59:38.748841: step 16400, loss 0.579192, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-16400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T11:59:58.815541: step 16600, loss 0.594436, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-16600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:00:18.173248: step 16800, loss 0.598944, acc 0.84\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-16800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:00:38.291762: step 17000, loss 0.696846, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-17000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:00:57.366660: step 17200, loss 0.704278, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-17200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:01:17.342972: step 17400, loss 0.809442, acc 0.81\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-17400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:01:37.594075: step 17600, loss 0.730172, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-17600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:01:56.750581: step 17800, loss 0.690182, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-17800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:02:16.737281: step 18000, loss 0.60364, acc 0.84\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-18000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:02:36.778985: step 18200, loss 0.672206, acc 0.87\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-18200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:02:55.907084: step 18400, loss 0.749548, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-18400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:03:15.939994: step 18600, loss 0.905408, acc 0.84\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-18600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:03:36.016303: step 18800, loss 0.689573, acc 0.81\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-18800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:03:55.181796: step 19000, loss 0.64687, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-19000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:04:15.236505: step 19200, loss 0.648382, acc 0.83\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-19200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:04:34.460799: step 19400, loss 0.62332, acc 0.83\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-19400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:04:54.461104: step 19600, loss 0.67328, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-19600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:05:14.534016: step 19800, loss 1.06682, acc 0.78\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-19800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:05:33.750112: step 20000, loss 0.690623, acc 0.79\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-20000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:05:53.767421: step 20200, loss 0.736566, acc 0.84\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-20200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:06:13.805517: step 20400, loss 0.833793, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-20400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:06:33.038019: step 20600, loss 0.649527, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-20600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:06:52.961314: step 20800, loss 0.730814, acc 0.81\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-20800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:07:13.011621: step 21000, loss 0.634561, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-21000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:07:32.160515: step 21200, loss 0.752536, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-21200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:07:52.270421: step 21400, loss 0.580133, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-21400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:08:12.270324: step 21600, loss 0.664272, acc 0.81\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-21600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:08:31.462022: step 21800, loss 0.786934, acc 0.86\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-21800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:08:51.437532: step 22000, loss 0.682931, acc 0.81\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-22000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:09:10.536039: step 22200, loss 0.798051, acc 0.81\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-22200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:09:30.599558: step 22400, loss 0.876079, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-22400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:09:50.697860: step 22600, loss 0.909482, acc 0.83\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-22600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:10:09.853159: step 22800, loss 0.83755, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-22800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:10:29.932270: step 23000, loss 0.73369, acc 0.83\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-23000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:10:50.027368: step 23200, loss 0.805306, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-23200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:11:09.479056: step 23400, loss 0.736078, acc 0.83\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-23400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:11:29.375949: step 23600, loss 0.804266, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-23600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:11:49.410842: step 23800, loss 0.8352, acc 0.79\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-23800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:12:08.446332: step 24000, loss 0.742507, acc 0.84\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-24000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:12:28.362817: step 24200, loss 0.902727, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-24200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:12:47.372716: step 24400, loss 0.821442, acc 0.83\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-24400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:13:07.138606: step 24600, loss 0.860777, acc 0.81\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-24600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:13:27.105503: step 24800, loss 0.833712, acc 0.83\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-24800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:13:46.309788: step 25000, loss 0.848955, acc 0.83\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-25000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:14:06.240886: step 25200, loss 0.855108, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-25200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:14:26.123770: step 25400, loss 0.828141, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-25400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:14:45.088463: step 25600, loss 0.864918, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-25600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:15:04.902559: step 25800, loss 1.02318, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-25800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:15:24.732651: step 26000, loss 1.42958, acc 0.79\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-26000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:15:43.805937: step 26200, loss 0.905285, acc 0.79\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-26200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:16:03.645025: step 26400, loss 1.08759, acc 0.77\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-26400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:16:22.656911: step 26600, loss 0.894166, acc 0.83\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-26600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:16:42.588401: step 26800, loss 0.827577, acc 0.81\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-26800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:17:02.511903: step 27000, loss 0.785091, acc 0.84\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-27000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:17:21.590996: step 27200, loss 0.788912, acc 0.81\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-27200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:17:41.450693: step 27400, loss 0.891994, acc 0.83\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-27400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:18:01.309989: step 27600, loss 0.825925, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-27600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:18:20.311284: step 27800, loss 0.730082, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-27800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:18:40.496983: step 28000, loss 0.874429, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-28000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:19:00.516874: step 28200, loss 0.905922, acc 0.81\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-28200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:19:19.532166: step 28400, loss 0.833304, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-28400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:19:39.711062: step 28600, loss 0.825205, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-28600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:19:59.758766: step 28800, loss 0.804274, acc 0.83\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-28800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:20:19.356853: step 29000, loss 0.897927, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-29000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:20:39.687155: step 29200, loss 0.901549, acc 0.81\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-29200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:20:58.695838: step 29400, loss 1.05616, acc 0.84\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-29400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:21:18.976538: step 29600, loss 1.02161, acc 0.86\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-29600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:21:39.172038: step 29800, loss 0.904432, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-29800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:21:58.285728: step 30000, loss 0.924571, acc 0.84\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-30000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:22:18.209424: step 30200, loss 0.802488, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-30200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:22:38.203909: step 30400, loss 0.753026, acc 0.81\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-30400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:22:57.223606: step 30600, loss 0.854805, acc 0.82\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-30600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:23:17.061888: step 30800, loss 0.817395, acc 0.78\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-30800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:23:36.935401: step 31000, loss 0.852971, acc 0.83\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-31000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2018-04-26T12:23:55.961292: step 31200, loss 0.855777, acc 0.81\n",
      "\n",
      "Saved model checkpoint to D:\\emotionTest\\runs\\1524709946\\checkpoints\\model-31200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. train the model and test\n",
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        with tf.device('/cpu:0'):\n",
    "            cnn = TextCNN(sequence_length=len(x_train1[0]),\n",
    "                          num_classes=y_train.shape[1],\n",
    "                          embedding_size=word2vector_size,\n",
    "                          filter_sizes=filter_sizes,\n",
    "                          num_filters=num_filters,\n",
    "                          l2_reg_lambda=l2_reg_lambda,\n",
    "                          input_weights=input_weights)\n",
    "\n",
    "            # Define Training procedure\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Keep track of gradient values and sparsity (optional)\n",
    "    #         grad_summaries = []\n",
    "    #         for g, v in grads_and_vars:\n",
    "    #             if g is not None:\n",
    "    #                 grad_hist_summary = tf.summary.histogram(\"{}\".format(v.name), g)\n",
    "    #                 sparsity_summary = tf.summary.scalar(\"{}\".format(v.name), tf.nn.zero_fraction(g))\n",
    "    #                 grad_summaries.append(grad_hist_summary)\n",
    "    #                 grad_summaries.append(sparsity_summary)\n",
    "    #         grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            # Summaries for loss and accuracy\n",
    "            loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "            acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "            # Train Summaries\n",
    "            train_summary_op = tf.summary.merge([loss_summary, acc_summary])#, grad_summaries_merged])\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # Dev summaries\n",
    "            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "            # Initialize all variables\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            def train_step(x_batch1, x_batch2, y_batch):\n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                    cnn.input_x0: x_batch1,\n",
    "                    cnn.input_x1: x_batch2,\n",
    "                    cnn.input_y: y_batch,\n",
    "                    cnn.dropout_keep_prob:dropout_keep_prob\n",
    "                }\n",
    "                try:\n",
    "                    _, step, summaries, loss, accuracy = sess.run(\n",
    "                        [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                        feed_dict)\n",
    "    #                 time_str = datetime.datetime.now().isoformat()\n",
    "    #                 print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                    train_summary_writer.add_summary(summaries, step)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "            def dev_step(x_batch1, x_batch2, y_batch, writer=None):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                    cnn.input_x0: x_batch1,\n",
    "                    cnn.input_x1: x_batch2,\n",
    "                    cnn.input_y: y_batch,\n",
    "                    cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run(\n",
    "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "\n",
    "            # Generate batches\n",
    "            batches = batch_iter(x_train1, x_train2, y_train,  batch_size, num_epochs)\n",
    "\n",
    "            testpoint = 0\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                gc.collect() #memory 문제가...\n",
    "                x_batch1, x_batch2, y_batch = zip(*batch)\n",
    "                train_step(x_batch1, x_batch2, y_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % evaluate_every == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_test1, x_test2, y_test, writer=dev_summary_writer)\n",
    "                    print(\"\")\n",
    "                if current_step % checkpoint_every == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(object):\n",
    "#     word_vec_size = 300\n",
    "#     window = 5\n",
    "#     min_count = 2\n",
    "#     flag = 'W'\n",
    "    \"\"\"\n",
    "    Word2Vec을 이용하여 사전을 만드는 여러가지 방식을 제공\n",
    "    <Parameters>\n",
    "        - word_vec_size : 단어 하나당 vector size\n",
    "        - window : 현재 word를 계산하는데 사용하는 maximum distance\n",
    "        - min_count : 단어가 해당 개수 이하로 나오면 무시\n",
    "        - flag : 5가지 종류 제공.\n",
    "          1) W : mecab 형태소분석 결과 word 단위 word2vec 사전\n",
    "          2) WT : mecab 형태소분석 결과 word 단위 + pos tagging word2vec 사전\n",
    "          3) JM : jamo 단위 word2vec 사전\n",
    "          4) LN : N = 1,2,.. n 글자수 단위 사전 제공\n",
    "    <Function>\n",
    "        - make_input(sentence_list) : 각 flag에 알맞은 input 형태를 만들어서 return\n",
    "        - save_w2v_dic(sentence_list, path) : 각 flag에 알맞은 사전은 원하는 path에 저장\n",
    "    \"\"\"\n",
    "    def __init__(self, word_vec_size=300, window=5, min_count=2, flag='W'):\n",
    "        self.word_vec_size = word_vec_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.flag = flag\n",
    "    \n",
    "\n",
    "    def parse_sentence(self, text, param):\n",
    "        if type(text) == str:\n",
    "            text = ''.join(text.split()) #space remove\n",
    "            words_array = []\n",
    "            \n",
    "            if self.flag[0] == 'W':\n",
    "                mecab = MeCab.Tagger()\n",
    "                \n",
    "                parse_result = mecab.parse(text) # pose parse\n",
    "                info_of_words = parse_result.split('\\n')\n",
    "                \n",
    "                for info in info_of_words:\n",
    "                    if not (info == 'EOS' or info == ''):\n",
    "                        info_elems = info.split(',')\n",
    "                        posed_word = info_elems[0].split('\\t')\n",
    "\n",
    "                        if param and len(posed_word)>1:\n",
    "                            words_array.append( posed_word[0]+'/'+posed_word[1])\n",
    "                        else:\n",
    "                            words_array.append( posed_word[0])\n",
    "            elif self.flag == 'JM':\n",
    "                words_array = list(j2hcj(h2j(text)))\n",
    "            elif self.flag[0] == 'L':\n",
    "                n = param\n",
    "                words_array = [text[i:i+n] for i in range(0, len(text), n)]\n",
    "                \n",
    "            return words_array\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "\n",
    "    def make_input(self, sentence_list):\n",
    "        input_x = []\n",
    "        \n",
    "        for text in sentence_list:\n",
    "            if self.flag[0] == 'W':\n",
    "                if len(self.flag) > 1 and self.flag[1] == 'T':\n",
    "                    param = True\n",
    "                else:\n",
    "                    param = False\n",
    "            elif self.flag[0] == 'L':\n",
    "                if len(self.flag) > 1 :\n",
    "                    param = int(self.flag[1])\n",
    "            else:\n",
    "                param = ''\n",
    "            result = self.parse_sentence(text=text, param=param)\n",
    "            if type(result)==list and len(result) > 0:\n",
    "                input_x.extend(result)\n",
    "        return input_x        \n",
    "    \n",
    "\n",
    "    def make_w2v_dic(self, sentence_list):\n",
    "        if type(sentence_list) == list:\n",
    "            input_x = self.make_input(sentence_list)\n",
    "            w2v_input = np.array(input_x)\n",
    "            model = gensim.models.Word2Vec(min_count=self.min_count, window=self.window, size=self.word_vec_size)\n",
    "            model.build_vocab(w2v_input)\n",
    "            model.train(w2v_input, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "            word_vectors = model.wv\n",
    "\n",
    "            return model, word_vectors\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def save_w2v_dic(self, sentence_list, path):\n",
    "\n",
    "        SAVE_PATH = path\n",
    "        SAVE_NAME = SAVE_PATH+datetime.datetime.now().strftime('%Y%m%d%H%M')+'_'+str(word_vec_size)+'_'+self.flag\n",
    "        \n",
    "        model, word_vectors = self.make_w2v_dic(sentence_list)\n",
    "        print(\"Dictionary is saved : \"+SAVE_NAME)\n",
    "        model.save(SAVE_NAME+'.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
